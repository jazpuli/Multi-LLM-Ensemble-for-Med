{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Project imports\n",
    "from src.evaluation.analysis import ResultsAnalyzer\n",
    "from src.ensemble.boosting_ensemble import BoostingEnsemble\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('Imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66543cb8",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d937a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ResultsAnalyzer(results_dir='../results')\n",
    "\n",
    "# Load different results\n",
    "try:\n",
    "    baseline = analyzer.load_results('baseline_results.json')\n",
    "    print('✓ Loaded baseline results')\n",
    "except FileNotFoundError:\n",
    "    baseline = {}\n",
    "    print('✗ Baseline results not found')\n",
    "\n",
    "try:\n",
    "    boosting = analyzer.load_results('boosting_ensemble_results.json')\n",
    "    print('✓ Loaded boosting ensemble results')\n",
    "except FileNotFoundError:\n",
    "    boosting = {}\n",
    "    print('✗ Boosting results not found')\n",
    "\n",
    "try:\n",
    "    dynamic = analyzer.load_results('dynamic_selection_results.json')\n",
    "    print('✓ Loaded dynamic selection results')\n",
    "except FileNotFoundError:\n",
    "    dynamic = {}\n",
    "    print('✗ Dynamic selection results not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29313e06",
   "metadata": {},
   "source": [
    "## 2. Compare Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f891e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comparison data\n",
    "comparison_data = []\n",
    "\n",
    "for dataset in baseline.keys():\n",
    "    # Individual models\n",
    "    for model, results in baseline[dataset].items():\n",
    "        comparison_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Method': model,\n",
    "            'Accuracy': results.get('accuracy', 0)\n",
    "        })\n",
    "    \n",
    "    # Boosting ensemble\n",
    "    if dataset in boosting:\n",
    "        comparison_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Method': 'Boosting Ensemble',\n",
    "            'Accuracy': boosting[dataset].get('ensemble_accuracy', 0)\n",
    "        })\n",
    "    \n",
    "    # Dynamic selection\n",
    "    if dataset in dynamic:\n",
    "        comparison_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Method': 'Dynamic Selection',\n",
    "            'Accuracy': dynamic[dataset].get('ensemble_accuracy', 0)\n",
    "        })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ceb682",
   "metadata": {},
   "source": [
    "## 3. Visualize Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_comparison.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # By dataset\n",
    "    sns.barplot(data=df_comparison, x='Dataset', y='Accuracy', hue='Method', ax=axes[0])\n",
    "    axes[0].set_title('Method Comparison by Dataset')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].legend(loc='lower left')\n",
    "    \n",
    "    # By method\n",
    "    sns.barplot(data=df_comparison, x='Method', y='Accuracy', hue='Dataset', ax=axes[1])\n",
    "    axes[1].set_title('Dataset Performance by Method')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/ensemble_comparison.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b56e1b",
   "metadata": {},
   "source": [
    "## 4. Improvement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline and boosting:\n",
    "    print('\\nImprovement Over Best Individual Model:')\n",
    "    print('=' * 70)\n",
    "    \n",
    "    for dataset in baseline.keys():\n",
    "        # Best individual accuracy\n",
    "        individual_accs = [r['accuracy'] for r in baseline[dataset].values()]\n",
    "        best_individual = max(individual_accs)\n",
    "        best_model = max(baseline[dataset].items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "        \n",
    "        # Boosting ensemble\n",
    "        if dataset in boosting:\n",
    "            boosting_acc = boosting[dataset].get('ensemble_accuracy', 0)\n",
    "            improvement = boosting_acc - best_individual\n",
    "            improvement_pct = (improvement / best_individual * 100) if best_individual > 0 else 0\n",
    "            \n",
    "            print(f'\\n{dataset}:')\n",
    "            print(f\"  Best Individual: {best_model} ({best_individual:.4f})\")\n",
    "            print(f\"  Boosting Ensemble: {boosting_acc:.4f}\")\n",
    "            print(f\"  Improvement: +{improvement:.4f} ({improvement_pct:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d190a8d",
   "metadata": {},
   "source": [
    "## 5. Disagreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if boosting:\n",
    "    print('\\nModel Disagreement Rates:')\n",
    "    print('=' * 50)\n",
    "    \n",
    "    disagreement_data = []\n",
    "    \n",
    "    for dataset, results in boosting.items():\n",
    "        disagreement_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Disagreement Rate': results.get('avg_disagreement_rate', 0)\n",
    "        })\n",
    "    \n",
    "    df_disagreement = pd.DataFrame(disagreement_data)\n",
    "    print(df_disagreement.to_string())\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(data=df_disagreement, x='Dataset', y='Disagreement Rate')\n",
    "    plt.title('Model Disagreement Rate by Dataset')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.savefig('../results/disagreement_rate.png', dpi=150)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
