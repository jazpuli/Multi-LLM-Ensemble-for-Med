{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Project imports\n",
    "from src.utils.config import ConfigLoader\n",
    "from src.utils.dataset_loader import DatasetLoader\n",
    "from src.evaluation.metrics import MetricsCalculator\n",
    "from src.evaluation.analysis import ResultsAnalyzer\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('Imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af05c7",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c677b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_loader = ConfigLoader(config_dir='../config')\n",
    "exp_config = config_loader.get_experiment_config()\n",
    "\n",
    "print('Experiment Configuration:')\n",
    "print(f\"Datasets enabled: {[k for k, v in exp_config['datasets'].items() if v.get('enabled')]}\")\n",
    "\n",
    "# Load datasets\n",
    "loader = DatasetLoader(cache_dir='../data')\n",
    "datasets = loader.load_all_datasets(exp_config)\n",
    "\n",
    "print(f'\\nLoaded {len(datasets)} datasets:')\n",
    "for name, examples in datasets.items():\n",
    "    print(f\"  {name}: {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ef9f9",
   "metadata": {},
   "source": [
    "## 2. Examine Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be61b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample from each dataset\n",
    "for dataset_name, examples in datasets.items():\n",
    "    print(f'\\n{dataset_name.upper()}:')\n",
    "    print('=' * 80)\n",
    "    \n",
    "    sample = examples[0]\n",
    "    print(f\"Question: {sample.get('question', '')[:200]}...\")\n",
    "    print(f\"Options: {sample.get('options', [])[:3]}\")\n",
    "    print(f\"Gold Label: {sample.get('gold_label', '')}\")\n",
    "    print(f\"Dataset: {sample.get('dataset', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531af698",
   "metadata": {},
   "source": [
    "## 3. Load Results and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results if available\n",
    "analyzer = ResultsAnalyzer(results_dir='../results')\n",
    "\n",
    "try:\n",
    "    baseline_results = analyzer.load_results('baseline_results.json')\n",
    "    print('Loaded baseline results')\n",
    "except FileNotFoundError:\n",
    "    print('No baseline results found. Run main.py --baseline-only to generate results.')\n",
    "    baseline_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90e351",
   "metadata": {},
   "source": [
    "## 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca346af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_results:\n",
    "    # Prepare data for visualization\n",
    "    results_list = []\n",
    "    \n",
    "    for dataset, model_results in baseline_results.items():\n",
    "        for model, results in model_results.items():\n",
    "            results_list.append({\n",
    "                'Dataset': dataset,\n",
    "                'Model': model,\n",
    "                'Accuracy': results.get('accuracy', 0)\n",
    "            })\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar plot by dataset\n",
    "    sns.barplot(data=df_results, x='Dataset', y='Accuracy', hue='Model', ax=axes[0])\n",
    "    axes[0].set_title('Model Accuracy by Dataset')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    # Bar plot by model\n",
    "    sns.barplot(data=df_results, x='Model', y='Accuracy', hue='Dataset', ax=axes[1])\n",
    "    axes[1].set_title('Model Accuracy Comparison')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/baseline_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(df_results.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc09d99",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d71dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_results:\n",
    "    print('\\nBaseline Summary Statistics:')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    for dataset in baseline_results.keys():\n",
    "        accuracies = [r['accuracy'] for r in baseline_results[dataset].values()]\n",
    "        print(f'\\n{dataset}:')\n",
    "        print(f\"  Mean Accuracy: {np.mean(accuracies):.4f}\")\n",
    "        print(f\"  Std Dev: {np.std(accuracies):.4f}\")\n",
    "        print(f\"  Min: {np.min(accuracies):.4f}\")\n",
    "        print(f\"  Max: {np.max(accuracies):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
