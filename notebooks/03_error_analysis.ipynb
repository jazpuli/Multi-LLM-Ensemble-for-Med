{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Project imports\n",
    "from src.evaluation.analysis import ResultsAnalyzer\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('Imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d787f31",
   "metadata": {},
   "source": [
    "## 1. Load Results and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ResultsAnalyzer(results_dir='../results')\n",
    "\n",
    "# Load results\n",
    "try:\n",
    "    baseline = analyzer.load_results('baseline_results.json')\n",
    "    print('✓ Loaded baseline results')\n",
    "except FileNotFoundError:\n",
    "    baseline = {}\n",
    "    print('✗ Baseline results not found. Run main.py first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b582938",
   "metadata": {},
   "source": [
    "## 2. Model Performance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline:\n",
    "    # Extract accuracy distribution\n",
    "    accuracy_data = []\n",
    "    \n",
    "    for dataset, model_results in baseline.items():\n",
    "        for model, results in model_results.items():\n",
    "            accuracy_data.append({\n",
    "                'Model': model,\n",
    "                'Dataset': dataset,\n",
    "                'Accuracy': results.get('accuracy', 0)\n",
    "            })\n",
    "    \n",
    "    df_accuracy = pd.DataFrame(accuracy_data)\n",
    "    \n",
    "    # Visualize distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Box plot by model\n",
    "    sns.boxplot(data=df_accuracy, x='Model', y='Accuracy', ax=axes[0])\n",
    "    axes[0].set_title('Accuracy Distribution by Model')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    # Box plot by dataset\n",
    "    sns.boxplot(data=df_accuracy, x='Dataset', y='Accuracy', ax=axes[1])\n",
    "    axes[1].set_title('Accuracy Distribution by Dataset')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/accuracy_distribution.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nAccuracy Summary:')\n",
    "    print(df_accuracy.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ed448",
   "metadata": {},
   "source": [
    "## 3. Model Specialization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline:\n",
    "    print('\\nModel Strengths by Dataset:')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    for dataset in baseline.keys():\n",
    "        print(f'\\n{dataset}:')\n",
    "        \n",
    "        accuracies = [(model, results.get('accuracy', 0)) \n",
    "                     for model, results in baseline[dataset].items()]\n",
    "        accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i, (model, acc) in enumerate(accuracies, 1):\n",
    "            print(f\"  {i}. {model:20s}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108ca79",
   "metadata": {},
   "source": [
    "## 4. Error Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f749588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error patterns\n",
    "if baseline:\n",
    "    print('\\nError Analysis by Model:')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    error_data = []\n",
    "    \n",
    "    for dataset, model_results in baseline.items():\n",
    "        for model, results in model_results.items():\n",
    "            accuracy = results.get('accuracy', 0)\n",
    "            n_samples = results.get('n_samples', 0)\n",
    "            n_correct = int(accuracy * n_samples)\n",
    "            n_errors = n_samples - n_correct\n",
    "            error_rate = 1 - accuracy\n",
    "            \n",
    "            error_data.append({\n",
    "                'Model': model,\n",
    "                'Dataset': dataset,\n",
    "                'Total': n_samples,\n",
    "                'Correct': n_correct,\n",
    "                'Errors': n_errors,\n",
    "                'Error Rate': error_rate\n",
    "            })\n",
    "    \n",
    "    df_errors = pd.DataFrame(error_data)\n",
    "    \n",
    "    # Visualize error rates\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df_errors.pivot(index='Model', columns='Dataset', values='Error Rate'),\n",
    "                cmap='RdYlGn_r', annot=True, fmt='.3f', cbar_kws={'label': 'Error Rate'})\n",
    "    plt.title('Error Rate Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/error_rate_heatmap.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(df_errors.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777647a",
   "metadata": {},
   "source": [
    "## 5. Complementarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which models complement each other\n",
    "if baseline:\n",
    "    print('\\nComplementarity Insights:')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    for dataset in baseline.keys():\n",
    "        accuracies = {model: results.get('accuracy', 0) \n",
    "                     for model, results in baseline[dataset].items()}\n",
    "        \n",
    "        # Find models with different strengths\n",
    "        models_sorted = sorted(accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f'\\n{dataset}:')\n",
    "        print(f\"  Ensemble potential: Combining {models_sorted[0][0]} (best)\"\n",
    "              f\" with {models_sorted[-1][0]} (weakest)\")\n",
    "        \n",
    "        max_acc = models_sorted[0][1]\n",
    "        min_acc = models_sorted[-1][1]\n",
    "        gap = max_acc - min_acc\n",
    "        \n",
    "        print(f\"  Accuracy gap: {gap:.4f}\")\n",
    "        print(f\"  Potential for ensemble improvement: {'HIGH' if gap > 0.1 else 'MEDIUM' if gap > 0.05 else 'LOW'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
