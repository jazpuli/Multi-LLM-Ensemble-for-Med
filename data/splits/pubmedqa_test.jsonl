{"id": "pubmedqa_20353735", "dataset": "pubmedqa", "question": "Context: Vitamin D deficiency/insufficiency (VDDI) is common in CKD patients and may be associated with abnormal mineral metabolism. It is not clear whether the K/DOQI recommended doses of ergocalciferol are adequate for correction of VDDI and hyperparathyroidism. Retrospective study of 88 patients with CKD Stages 1 - 5 and baseline 25-hydroxyvitamin D level<30 ng/ml (<75 nmol/l). Patients treated with ergocalciferol as recommended by K/DOQI guidelines. Only 53 patients had elevated baseline PTH level for the CKD stage. Patients were excluded if they received vitamin D preparations other than ergocalciferol or phosphate binders. 25-hydroxyvitamin D level, intact PTH level (iPTH), and other parameters of mineral metabolism were measured at baseline and after completion of ergocalciferol course. 88 patients with CKD were treated with ergocalciferol. Mean age 56.8 +/- 9.5 years and 41% were males. The mean (+/- SD) GFR was 28.3 +/- 16.6 ml/min. At the end of the 6-month period of ergocalciferol treatment, the mean 25-hydroxyvitamin D level increased from 15.1 +/- 5.8 to 23.3 +/- 11.8 ng/ml (37.75 +/- 14.5 to 58.25 +/- 29.5 nmol/l) (p<0.001). Treatment led to>or = 5 ng/ml (12.5 nmol/l) increases in 25-hydroxyvitamin D level in 54% of treated patients, and only 25% achieved levels>or = 30 ng/ml (75 nmol/l). Mean iPTH level decreased from 157.9 +/- 125.9 to 150.7 +/- 127.5 pg/ml (p = 0.5). Only 26% of patients had>or = 30% decrease in their iPTH level after treatment with ergocalciferol.\n\nQuestion: Treatment of vitamin D deficiency in CKD patients with ergocalciferol: are current K/DOQI treatment guidelines adequate?", "question_only": "Treatment of vitamin D deficiency in CKD patients with ergocalciferol: are current K/DOQI treatment guidelines adequate?", "context": "Vitamin D deficiency/insufficiency (VDDI) is common in CKD patients and may be associated with abnormal mineral metabolism. It is not clear whether the K/DOQI recommended doses of ergocalciferol are adequate for correction of VDDI and hyperparathyroidism. Retrospective study of 88 patients with CKD Stages 1 - 5 and baseline 25-hydroxyvitamin D level<30 ng/ml (<75 nmol/l). Patients treated with ergocalciferol as recommended by K/DOQI guidelines. Only 53 patients had elevated baseline PTH level for the CKD stage. Patients were excluded if they received vitamin D preparations other than ergocalciferol or phosphate binders. 25-hydroxyvitamin D level, intact PTH level (iPTH), and other parameters of mineral metabolism were measured at baseline and after completion of ergocalciferol course. 88 patients with CKD were treated with ergocalciferol. Mean age 56.8 +/- 9.5 years and 41% were males. The mean (+/- SD) GFR was 28.3 +/- 16.6 ml/min. At the end of the 6-month period of ergocalciferol treatment, the mean 25-hydroxyvitamin D level increased from 15.1 +/- 5.8 to 23.3 +/- 11.8 ng/ml (37.75 +/- 14.5 to 58.25 +/- 29.5 nmol/l) (p<0.001). Treatment led to>or = 5 ng/ml (12.5 nmol/l) increases in 25-hydroxyvitamin D level in 54% of treated patients, and only 25% achieved levels>or = 30 ng/ml (75 nmol/l). Mean iPTH level decreased from 157.9 +/- 125.9 to 150.7 +/- 127.5 pg/ml (p = 0.5). Only 26% of patients had>or = 30% decrease in their iPTH level after treatment with ergocalciferol.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Current K/DOQI guidelines are inadequate for correcting VDDI or secondary hyperparathyroidism in CKD patients. Future studies should examine the effects of higher or more frequent dosing of ergocalciferol on these clinical endpoints.", "meshes": ["Aged", "Chi-Square Distribution", "Dose-Response Relationship, Drug", "Drug Administration Schedule", "Ergocalciferols", "Female", "Humans", "Kidney Failure, Chronic", "Male", "Middle Aged", "Parathyroid Hormone", "Patient Selection", "Practice Guidelines as Topic", "Regression Analysis", "Retrospective Studies", "Treatment Outcome", "Vitamin D", "Vitamin D Deficiency", "Vitamins"], "year": "2010"}
{"id": "pubmedqa_25756710", "dataset": "pubmedqa", "question": "Context: To validate a clinical diagnostic tool, used by emergency physicians (EPs), to diagnose the central cause of patients presenting with vertigo, and to determine interrater reliability of this tool. A convenience sample of adult patients presenting to a single academic ED with isolated vertigo (i.e. vertigo without other neurological deficits) was prospectively evaluated with STANDING (SponTAneousNystagmus, Direction, head Impulse test, standiNG) by five trained EPs. The first step focused on the presence of spontaneous nystagmus, the second on the direction of nystagmus, the third on head impulse test and the fourth on gait. The local standard practice, senior audiologist evaluation corroborated by neuroimaging when deemed appropriate, was considered the reference standard. Sensitivity and specificity of STANDING were calculated. On the first 30 patients, inter-observer agreement among EPs was also assessed. Five EPs with limited experience in nystagmus assessment volunteered to participate in the present study enrolling 98 patients. Their average evaluation time was 9.9 ± 2.8 min (range 6-17). Central acute vertigo was suspected in 16 (16.3%) patients. There were 13 true positives, three false positives, 81 true negatives and one false negative, with a high sensitivity (92.9%, 95% CI 70-100%) and specificity (96.4%, 95% CI 93-38%) for central acute vertigo according to senior audiologist evaluation. The Cohen's kappas of the first, second, third and fourth steps of the STANDING were 0.86, 0.93, 0.73 and 0.78, respectively. The whole test showed a good inter-observer agreement (k = 0.76, 95% CI 0.45-1).\n\nQuestion: Can emergency physicians accurately and reliably assess acute vertigo in the emergency department?", "question_only": "Can emergency physicians accurately and reliably assess acute vertigo in the emergency department?", "context": "To validate a clinical diagnostic tool, used by emergency physicians (EPs), to diagnose the central cause of patients presenting with vertigo, and to determine interrater reliability of this tool. A convenience sample of adult patients presenting to a single academic ED with isolated vertigo (i.e. vertigo without other neurological deficits) was prospectively evaluated with STANDING (SponTAneousNystagmus, Direction, head Impulse test, standiNG) by five trained EPs. The first step focused on the presence of spontaneous nystagmus, the second on the direction of nystagmus, the third on head impulse test and the fourth on gait. The local standard practice, senior audiologist evaluation corroborated by neuroimaging when deemed appropriate, was considered the reference standard. Sensitivity and specificity of STANDING were calculated. On the first 30 patients, inter-observer agreement among EPs was also assessed. Five EPs with limited experience in nystagmus assessment volunteered to participate in the present study enrolling 98 patients. Their average evaluation time was 9.9 ± 2.8 min (range 6-17). Central acute vertigo was suspected in 16 (16.3%) patients. There were 13 true positives, three false positives, 81 true negatives and one false negative, with a high sensitivity (92.9%, 95% CI 70-100%) and specificity (96.4%, 95% CI 93-38%) for central acute vertigo according to senior audiologist evaluation. The Cohen's kappas of the first, second, third and fourth steps of the STANDING were 0.86, 0.93, 0.73 and 0.78, respectively. The whole test showed a good inter-observer agreement (k = 0.76, 95% CI 0.45-1).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "In the hands of EPs, STANDING showed a good inter-observer agreement and accuracy validated against the local standard of care.", "meshes": ["Acute Disease", "Algorithms", "Clinical Competence", "Decision Support Systems, Clinical", "Emergency Medicine", "Emergency Service, Hospital", "Humans", "Prospective Studies", "Vertigo"], "year": "2015"}
{"id": "pubmedqa_17715311", "dataset": "pubmedqa", "question": "Context: The purpose of this study was to evaluate the clinical usefulness of a fetal anatomic survey on follow-up antepartum sonograms. A retrospective follow-up study was conducted at a low-risk maternity clinic from July 1, 2005, to June 30, 2006. Eligible women had at least 1 prior sonographic examination beyond 18 weeks' gestation with a complete and normal fetal anatomic assessment and at least 1 follow-up sonogram. Full fetal anatomic surveys were performed on all follow-up sonograms regardless of the indication. Neonatal charts were reviewed for those patients whose follow-up sonograms revealed unanticipated fetal anomalies. Neonatal intervention was defined as surgical or medical therapy or arranged subspecialty follow-up specifically for the suspected fetal anomaly. Of a total of 4269 sonographic examinations performed, 437 (10.2%) were follow-up studies. Of these, 101 (23.1%) were excluded because the initial sonogram revealed a suspected fetal anomaly, and 42 (9.8%) were excluded for other reasons. Of the remaining 294 women, 21 (7.1%) had an unanticipated fetal anomaly, most often renal pyelectasis. Compared with follow-up sonography for other reasons, repeated sonography for fetal growth evaluation yielded a higher incidence of unexpected fetal anomalies: 15 (12.3%) of 122 versus 6 (3.5%) of 172 (P = .01). When compared with the neonates in the nongrowth indications group, those neonates whose mothers had sonographic examinations for fetal growth had a higher rate of neonatal interventions: 6 (40.0%) of 15 versus 0 (0%) of 6 (P = .04).\n\nQuestion: Is fetal anatomic assessment on follow-up antepartum sonograms clinically useful?", "question_only": "Is fetal anatomic assessment on follow-up antepartum sonograms clinically useful?", "context": "The purpose of this study was to evaluate the clinical usefulness of a fetal anatomic survey on follow-up antepartum sonograms. A retrospective follow-up study was conducted at a low-risk maternity clinic from July 1, 2005, to June 30, 2006. Eligible women had at least 1 prior sonographic examination beyond 18 weeks' gestation with a complete and normal fetal anatomic assessment and at least 1 follow-up sonogram. Full fetal anatomic surveys were performed on all follow-up sonograms regardless of the indication. Neonatal charts were reviewed for those patients whose follow-up sonograms revealed unanticipated fetal anomalies. Neonatal intervention was defined as surgical or medical therapy or arranged subspecialty follow-up specifically for the suspected fetal anomaly. Of a total of 4269 sonographic examinations performed, 437 (10.2%) were follow-up studies. Of these, 101 (23.1%) were excluded because the initial sonogram revealed a suspected fetal anomaly, and 42 (9.8%) were excluded for other reasons. Of the remaining 294 women, 21 (7.1%) had an unanticipated fetal anomaly, most often renal pyelectasis. Compared with follow-up sonography for other reasons, repeated sonography for fetal growth evaluation yielded a higher incidence of unexpected fetal anomalies: 15 (12.3%) of 122 versus 6 (3.5%) of 172 (P = .01). When compared with the neonates in the nongrowth indications group, those neonates whose mothers had sonographic examinations for fetal growth had a higher rate of neonatal interventions: 6 (40.0%) of 15 versus 0 (0%) of 6 (P = .04).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "A fetal anatomic survey on follow-up sonograms may identify unanticipated fetal anomalies, especially when the indication is for fetal growth.", "meshes": ["Adult", "Chi-Square Distribution", "Congenital Abnormalities", "Female", "Fetal Diseases", "Follow-Up Studies", "Gestational Age", "Humans", "Postpartum Period", "Pregnancy", "Retrospective Studies", "Statistics, Nonparametric", "Ultrasonography, Prenatal"], "year": "2007"}
{"id": "pubmedqa_15052394", "dataset": "pubmedqa", "question": "Context: The gender difference in prevalence and incidence rates of depression is one of the most consistent findings in psychiatric epidemiology. We sought to examine whether any gender differences in symptom profile might account for this difference in rates. This study was a population-based 13-year follow-up survey of community-dwelling adults living in East Baltimore in 1981. Subjects were the continuing participants of the Baltimore Epidemiologic Catchment Area Program. Participants interviewed between 1993 and 1996 with complete data on depressive symptoms and covariates were included (n = 1727). We applied structural equations with a measurement model for dichotomous data (the MIMIC-multiple indicators, multiple causes-model) to compare symptoms between women and men, in relation to the nine symptom groups comprising the diagnostic criteria for major depression, adjusting for several potentially influential characteristics (namely, age, self-reported ethnicity, educational attainment, marital status, and employment). There were no significant gender differences in the self-report of depression symptoms even taking into account the higher level of depressive symptoms of women and the influence of other covariates. For example, women were no more likely to endorse sadness than were men, as evidenced by a direct effect coefficient that was not significantly different from the null [adjusted estimated direct effect of gender on report of sadness = 0.105, 95% confidence interval (-0.113, 0.323)].\n\nQuestion: Are higher rates of depression in women accounted for by differential symptom reporting?", "question_only": "Are higher rates of depression in women accounted for by differential symptom reporting?", "context": "The gender difference in prevalence and incidence rates of depression is one of the most consistent findings in psychiatric epidemiology. We sought to examine whether any gender differences in symptom profile might account for this difference in rates. This study was a population-based 13-year follow-up survey of community-dwelling adults living in East Baltimore in 1981. Subjects were the continuing participants of the Baltimore Epidemiologic Catchment Area Program. Participants interviewed between 1993 and 1996 with complete data on depressive symptoms and covariates were included (n = 1727). We applied structural equations with a measurement model for dichotomous data (the MIMIC-multiple indicators, multiple causes-model) to compare symptoms between women and men, in relation to the nine symptom groups comprising the diagnostic criteria for major depression, adjusting for several potentially influential characteristics (namely, age, self-reported ethnicity, educational attainment, marital status, and employment). There were no significant gender differences in the self-report of depression symptoms even taking into account the higher level of depressive symptoms of women and the influence of other covariates. For example, women were no more likely to endorse sadness than were men, as evidenced by a direct effect coefficient that was not significantly different from the null [adjusted estimated direct effect of gender on report of sadness = 0.105, 95% confidence interval (-0.113, 0.323)].", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Men and women in this community sample reported similar patterns of depressive symptoms. No evidence that the presentation of depressive symptoms differs by gender was found.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Baltimore", "Catchment Area (Health)", "Depressive Disorder, Major", "Female", "Follow-Up Studies", "Humans", "Incidence", "Male", "Middle Aged", "Prevalence", "Psychometrics", "Risk Factors", "Self Disclosure", "Sex Factors", "Socioeconomic Factors", "Women's Health"], "year": "2004"}
{"id": "pubmedqa_23356465", "dataset": "pubmedqa", "question": "Context: Various factors contribute to the effective implementation of evidence-based treatments (EBTs). In this study, cognitive processing therapy (CPT) was administered in a Veterans Affairs (VA) posttraumatic stress disorder (PTSD) specialty clinic in which training and supervision were provided following VA implementation guidelines. The aim was to (a) estimate the proportion of variability in outcome attributable to therapists and (b) identify characteristics of those therapists who produced better outcomes. We used an archival database of veterans (n = 192) who completed 12 sessions of CPT by therapists (n = 25) who were trained by 2 nationally recognized trainers, 1 of whom also provided weekly group supervision. Multilevel modeling was used to estimate therapist effects, with therapists treated as a random factor. The supervisor was asked to retrospectively rate each therapist in terms of perceived effectiveness based on supervision interactions. Using single case study design, the supervisor was interviewed to determine what criteria she used to rate the therapists and emerging themes were coded. When initial level of severity on the PTSD Checklist (PCL; McDonald&Calhoun, 2010; Weathers, Litz, Herman, Huska,&Keane, 1993) was taken into account, approximately 12% of the variability in the PCL at the end of treatment was due to therapists. The trainer, blind to the results, identified the following characteristics and actions of effective therapists: effectively addressing patient avoidance, language used in supervision, flexible interpersonal style, and ability to develop a strong therapeutic alliance.\n\nQuestion: Uniformity of evidence-based treatments in practice?", "question_only": "Uniformity of evidence-based treatments in practice?", "context": "Various factors contribute to the effective implementation of evidence-based treatments (EBTs). In this study, cognitive processing therapy (CPT) was administered in a Veterans Affairs (VA) posttraumatic stress disorder (PTSD) specialty clinic in which training and supervision were provided following VA implementation guidelines. The aim was to (a) estimate the proportion of variability in outcome attributable to therapists and (b) identify characteristics of those therapists who produced better outcomes. We used an archival database of veterans (n = 192) who completed 12 sessions of CPT by therapists (n = 25) who were trained by 2 nationally recognized trainers, 1 of whom also provided weekly group supervision. Multilevel modeling was used to estimate therapist effects, with therapists treated as a random factor. The supervisor was asked to retrospectively rate each therapist in terms of perceived effectiveness based on supervision interactions. Using single case study design, the supervisor was interviewed to determine what criteria she used to rate the therapists and emerging themes were coded. When initial level of severity on the PTSD Checklist (PCL; McDonald&Calhoun, 2010; Weathers, Litz, Herman, Huska,&Keane, 1993) was taken into account, approximately 12% of the variability in the PCL at the end of treatment was due to therapists. The trainer, blind to the results, identified the following characteristics and actions of effective therapists: effectively addressing patient avoidance, language used in supervision, flexible interpersonal style, and ability to develop a strong therapeutic alliance.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "This study adds to the growing body of literature documenting the importance of the individual therapist as an important factor in the change process.", "meshes": ["Adult", "Aged", "Clinical Competence", "Cognitive Therapy", "Evidence-Based Practice", "Female", "Humans", "Interviews as Topic", "Male", "Middle Aged", "Severity of Illness Index", "Stress Disorders, Post-Traumatic", "Treatment Outcome", "United States", "United States Department of Veterans Affairs", "Veterans", "Wisconsin", "Young Adult"], "year": "2013"}
{"id": "pubmedqa_26452334", "dataset": "pubmedqa", "question": "Context: The aim of this study was to assess the reproducibility of different measurement methods and define the most workable technique for measuring head and neck paragangliomas, to determine the best method for evaluating tumour growth. The evaluation of tumour growth is vital for a 'wait-and-scan' policy, a management strategy that became increasingly important. Method comparison study. Thirty tumours, including carotid body, vagal body, jugulotympanic tumours and conglomerates of multiple tumours, were measured in duplicate, using linear dimensions, manual area tracing and an automated segmentation method. Reproducibility was assessed using the Bland-Altman method. The smallest detectable difference using the linear dimension method was 11% for carotid body and 27% for vagal body tumours, compared with 17% and 20% for the manual area tracing method. Due to the irregular shape of paragangliomas in the temporal bone and conglomerates, the manual area tracing method showed better results in these tumours (26% and 8% versus 54% and 47%). The linear dimension method was significantly faster (median 4.27 versus 18.46 minutes, P<0.001). The automatic segmentation method yielded smallest detectable differences between 39% and 75%, and although fast (2.19 ± 1.49 minutes), it failed technically.\n\nQuestion: Measurement of head and neck paragangliomas: is volumetric analysis worth the effort?", "question_only": "Measurement of head and neck paragangliomas: is volumetric analysis worth the effort?", "context": "The aim of this study was to assess the reproducibility of different measurement methods and define the most workable technique for measuring head and neck paragangliomas, to determine the best method for evaluating tumour growth. The evaluation of tumour growth is vital for a 'wait-and-scan' policy, a management strategy that became increasingly important. Method comparison study. Thirty tumours, including carotid body, vagal body, jugulotympanic tumours and conglomerates of multiple tumours, were measured in duplicate, using linear dimensions, manual area tracing and an automated segmentation method. Reproducibility was assessed using the Bland-Altman method. The smallest detectable difference using the linear dimension method was 11% for carotid body and 27% for vagal body tumours, compared with 17% and 20% for the manual area tracing method. Due to the irregular shape of paragangliomas in the temporal bone and conglomerates, the manual area tracing method showed better results in these tumours (26% and 8% versus 54% and 47%). The linear dimension method was significantly faster (median 4.27 versus 18.46 minutes, P<0.001). The automatic segmentation method yielded smallest detectable differences between 39% and 75%, and although fast (2.19 ± 1.49 minutes), it failed technically.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Due to a relatively good reproducibility, fast and easy application, we found the linear dimension method to be the most pragmatic approach for evaluation of growth of carotid and vagal body paragangliomas. For jugulotympanic tumours, the preferred method is manual area tracing. However, volumetric changes of these tumours may be of less clinical importance than changes in relation to surrounding anatomical structures.", "meshes": ["Female", "Head and Neck Neoplasms", "Humans", "Image Interpretation, Computer-Assisted", "Magnetic Resonance Angiography", "Male", "Paraganglioma", "Reproducibility of Results", "Tumor Burden"], "year": "2016"}
{"id": "pubmedqa_11034241", "dataset": "pubmedqa", "question": "Context: Routine intraoperative frozen section (FS) of sentinel lymph nodes (SLN) can detect metastatic disease, allowing immediate axillary dissection and avoiding the need for reoperation. Routine FS is also costly, increases operative time, and is subject to false-negative results. We examined the benefit of routine intraoperative FS among the first 1000 patients at Memorial Sloan Kettering Cancer Center who had SLN biopsy for breast cancer. We performed SLN biopsy with intraoperative FS in 890 consecutive breast cancer patients, none of whom had a back-up axillary dissection planned in advance. Serial sections and immunohistochemical staining for cytokeratins were performed on all SLN that proved negative on FS. The sensitivity of FS was determined as a function of (1) tumor size and (2) volume of metastatic disease in the SLN, and the benefit of FS was defined as the avoidance of a reoperative axillary dissection. The sensitivity of FS ranged from 40% for patients with Tla to 76% for patients with T2 cancers. The volume of SLN metastasis was highly correlated with tumor size, and FS was far more effective in detecting macrometastatic disease (sensitivity 92%) than micrometastases (sensitivity 17%). The benefit of FS in avoiding reoperative axillary dissection ranged from 4% for Tla (6 of 143) to 38% for T2 (45 of 119) cancers.\n\nQuestion: Is routine intraoperative frozen-section examination of sentinel lymph nodes in breast cancer worthwhile?", "question_only": "Is routine intraoperative frozen-section examination of sentinel lymph nodes in breast cancer worthwhile?", "context": "Routine intraoperative frozen section (FS) of sentinel lymph nodes (SLN) can detect metastatic disease, allowing immediate axillary dissection and avoiding the need for reoperation. Routine FS is also costly, increases operative time, and is subject to false-negative results. We examined the benefit of routine intraoperative FS among the first 1000 patients at Memorial Sloan Kettering Cancer Center who had SLN biopsy for breast cancer. We performed SLN biopsy with intraoperative FS in 890 consecutive breast cancer patients, none of whom had a back-up axillary dissection planned in advance. Serial sections and immunohistochemical staining for cytokeratins were performed on all SLN that proved negative on FS. The sensitivity of FS was determined as a function of (1) tumor size and (2) volume of metastatic disease in the SLN, and the benefit of FS was defined as the avoidance of a reoperative axillary dissection. The sensitivity of FS ranged from 40% for patients with Tla to 76% for patients with T2 cancers. The volume of SLN metastasis was highly correlated with tumor size, and FS was far more effective in detecting macrometastatic disease (sensitivity 92%) than micrometastases (sensitivity 17%). The benefit of FS in avoiding reoperative axillary dissection ranged from 4% for Tla (6 of 143) to 38% for T2 (45 of 119) cancers.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "In breast cancer patients having SLN biopsy, the failure of routine intraoperative FS is largely the failure to detect micrometastatic disease. The benefit of routine intraoperative FS increases with tumor size. Routine FS may not be indicated in patients with the smallest invasive cancers.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Breast Neoplasms", "Female", "Frozen Sections", "Humans", "Immunohistochemistry", "Intraoperative Period", "Lymphatic Metastasis", "Middle Aged", "Predictive Value of Tests", "Sensitivity and Specificity", "Sentinel Lymph Node Biopsy"], "year": "2000"}
{"id": "pubmedqa_23690198", "dataset": "pubmedqa", "question": "Context: Social and cultural factors combined with little information may prevent the diffusion of epidural analgesia for pain relief during childbirth. The present study was launched contemporarily to the implementation of analgesia for labor in our Department in order to perform a 2 years audit on its use. The goal is to evaluate the epidural acceptance and penetration into hospital practice by women and care givers and safety and efficacy during childbirth. This audit cycle measured epidural analgesia performance against 4 standards: (1) Implementation of epidural analgesia for labor to all patients; (2) Acceptance and good satisfaction level reported by patients and caregivers. (3) Effectiveness of labor analgesia; (4) No maternal or fetal side effects. During the audit period epidural analgesia increased from 15.5% of all labors in the first trimester of the study to 51% in the last trimester (p<0.005). Satisfaction levels reported by patients and care givers were good. A hierarchical clustering analysis identified two clusters based on VAS (Visual Analogue Scale) time course: in 226 patients (cluster 1) VAS decreased from 8.5±1.4 before to 4.1±1.3 after epidural analgesia; in 1002 patients (cluster 2) VAS decreased from 8.12±1.7 before (NS vs cluster 1), to 0.76±0.79 after (p<0.001 vs before and vs cluster 2 after). No other differences between clusters were observed.\n\nQuestion: Implementation of epidural analgesia for labor: is the standard of effective analgesia reachable in all women?", "question_only": "Implementation of epidural analgesia for labor: is the standard of effective analgesia reachable in all women?", "context": "Social and cultural factors combined with little information may prevent the diffusion of epidural analgesia for pain relief during childbirth. The present study was launched contemporarily to the implementation of analgesia for labor in our Department in order to perform a 2 years audit on its use. The goal is to evaluate the epidural acceptance and penetration into hospital practice by women and care givers and safety and efficacy during childbirth. This audit cycle measured epidural analgesia performance against 4 standards: (1) Implementation of epidural analgesia for labor to all patients; (2) Acceptance and good satisfaction level reported by patients and caregivers. (3) Effectiveness of labor analgesia; (4) No maternal or fetal side effects. During the audit period epidural analgesia increased from 15.5% of all labors in the first trimester of the study to 51% in the last trimester (p<0.005). Satisfaction levels reported by patients and care givers were good. A hierarchical clustering analysis identified two clusters based on VAS (Visual Analogue Scale) time course: in 226 patients (cluster 1) VAS decreased from 8.5±1.4 before to 4.1±1.3 after epidural analgesia; in 1002 patients (cluster 2) VAS decreased from 8.12±1.7 before (NS vs cluster 1), to 0.76±0.79 after (p<0.001 vs before and vs cluster 2 after). No other differences between clusters were observed.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Present audit shows that the process of implementation of labor analgesia was quick, successful and safe, notwithstanding the identification of one cluster of women with suboptimal response to epidural analgesia that need to be further studies, overall pregnant womens'adhesion to labor analgesia was satisfactory.", "meshes": ["Adult", "Analgesia, Epidural", "Analgesia, Obstetrical", "Apgar Score", "Cesarean Section", "Cluster Analysis", "Female", "Hemodynamics", "Humans", "Infant, Newborn", "Pain Measurement", "Parity", "Patient Safety", "Patient Satisfaction", "Pregnancy"], "year": "2013"}
{"id": "pubmedqa_12630042", "dataset": "pubmedqa", "question": "Context: The long-term survival of patients with gastric cancer is governed by various factors, such as the clinical stage of the cancer, the patient's nutritional state, and the treatment and may be governed by the volume of intraperitoneal adipose tissue. The aim of this study is to clarify the relationship between the degree of the patients' body mass index and their long-term survival. Gastric cancer patients who had undergone a gastrectomy with D2-lymphadenectomy and with resection A and B according to the criteria of the Japanese Research Society for Gastric Cancer Rules were subgrouped into those patients with a body mass index<0.185 (the lower body mass index group) and those patients with a body mass index>0.210 (the higher body mass index group). The patient's morbidity and long-term survival rate was retrospectively compared between the 2 groups. A significantly longer mean survival rate was observed for the lower body mass index group in stage 2 (1667 vs. 1322 days, P = 0.0240). Also, a significantly longer mean survival rate was observed for the higher BMI group in stage 3a (1431 vs. 943, P = 0.0071).\n\nQuestion: Does body mass index (BMI) influence morbidity and long-term survival in gastric cancer patients after gastrectomy?", "question_only": "Does body mass index (BMI) influence morbidity and long-term survival in gastric cancer patients after gastrectomy?", "context": "The long-term survival of patients with gastric cancer is governed by various factors, such as the clinical stage of the cancer, the patient's nutritional state, and the treatment and may be governed by the volume of intraperitoneal adipose tissue. The aim of this study is to clarify the relationship between the degree of the patients' body mass index and their long-term survival. Gastric cancer patients who had undergone a gastrectomy with D2-lymphadenectomy and with resection A and B according to the criteria of the Japanese Research Society for Gastric Cancer Rules were subgrouped into those patients with a body mass index<0.185 (the lower body mass index group) and those patients with a body mass index>0.210 (the higher body mass index group). The patient's morbidity and long-term survival rate was retrospectively compared between the 2 groups. A significantly longer mean survival rate was observed for the lower body mass index group in stage 2 (1667 vs. 1322 days, P = 0.0240). Also, a significantly longer mean survival rate was observed for the higher BMI group in stage 3a (1431 vs. 943, P = 0.0071).", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "The body mass index is one of the prognostic factors of stage 2 and stage 3a gastric cancer. However, it does not appear to be useful for determining the prognosis of stage 1a, 1b, 3b, and 4a gastric cancers.", "meshes": ["Adult", "Aged", "Body Mass Index", "Female", "Gastrectomy", "Humans", "Male", "Middle Aged", "Neoplasm Recurrence, Local", "Neoplasm Staging", "Postoperative Complications", "Prognosis", "Retrospective Studies", "Risk Factors", "Stomach Neoplasms", "Survival Rate", "Time Factors"], "year": null}
{"id": "pubmedqa_26037986", "dataset": "pubmedqa", "question": "Context: Emergency surgery is associated with poorer outcomes and higher mortality with recent studies suggesting the 30-day mortality to be 14-15%. The aim of this study was to analyse the 30-day mortality, age-related 30-day mortality and 1-year mortality following emergency laparotomy. We hope this will encourage prospective data collection, improvement of care and initiate strategies to establish best practice in this area. This was a retrospective study of patients who underwent emergency laparotomy from June 2010 to May 2012. The primary end point of the study was 30-day mortality, age-related 30-day mortality and 1-year all-cause mortality. 477 laparotomies were performed in 446 patients. 57% were aged<70 and 43% aged>70 years. 30-day mortality was 12, 4% in those aged<70 years and 22% in those>70 years (p<0.001). 1-year mortality was 25, 15% in those aged under 70 years and 38% in those aged>70 years (p<0.001).\n\nQuestion: 30-Day and 1-year mortality in emergency general surgery laparotomies: an area of concern and need for improvement?", "question_only": "30-Day and 1-year mortality in emergency general surgery laparotomies: an area of concern and need for improvement?", "context": "Emergency surgery is associated with poorer outcomes and higher mortality with recent studies suggesting the 30-day mortality to be 14-15%. The aim of this study was to analyse the 30-day mortality, age-related 30-day mortality and 1-year mortality following emergency laparotomy. We hope this will encourage prospective data collection, improvement of care and initiate strategies to establish best practice in this area. This was a retrospective study of patients who underwent emergency laparotomy from June 2010 to May 2012. The primary end point of the study was 30-day mortality, age-related 30-day mortality and 1-year all-cause mortality. 477 laparotomies were performed in 446 patients. 57% were aged<70 and 43% aged>70 years. 30-day mortality was 12, 4% in those aged<70 years and 22% in those>70 years (p<0.001). 1-year mortality was 25, 15% in those aged under 70 years and 38% in those aged>70 years (p<0.001).", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Emergency laparotomy carries a high rate of mortality, especially in those over the age of 70 years, and more needs to be done to improve outcomes, particularly in this group. This could involve increasing acute surgical care manpower, early recognition of patients requiring emergency surgery, development of clear management protocols for such patients or perhaps even considering centralisation of emergency surgical services to specialist centres with multidisciplinary teams involving emergency surgeons and care of the elderly physicians in hospital and related community outreach services for post-discharge care.", "meshes": ["Adult", "Age Factors", "Aged", "Aged, 80 and over", "Cause of Death", "Cohort Studies", "Emergency Treatment", "Female", "General Surgery", "Humans", "Incidence", "Laparotomy", "Male", "Middle Aged", "Needs Assessment", "Retrospective Studies", "Risk Assessment", "Time Factors", "United Kingdom"], "year": "2015"}
{"id": "pubmedqa_23899611", "dataset": "pubmedqa", "question": "Context: Twenty-eight female Sprague Dawley rats were allocated randomly to 4 groups. The sham group (group 1) was only subjected to catheter insertion, not to pneumoperitoneum. Group 2 received a 1 mg/kg dose of 0.9% sodium chloride by the intraperitoneal route for 10 min before pneumoperitoneum. Groups 3 and 4 received 6 and 12 mg/kg edaravone, respectively, by the intraperitoneal route for 10 min before pneumoperitoneum. After 60 min of pneumoperitoneum, the gas was deflated. Immediately after the reperfusion period, both ovaries were excised for histological scoring, caspase-3 immunohistochemistry and biochemical evaluation including glutathione (GSH) and malondialdehyde (MDA) levels. Also, total antioxidant capacity (TAC) was measured in plasma samples to evaluate the antioxidant effect of edaravone. Ovarian sections in the saline group revealed higher scores for follicular degeneration and edema (p<0.0001) when compared with the sham group. Administration of different doses of edaravone in rats significantly prevented degenerative changes in the ovary (p<0.0001). Caspase-3 expression was only detected in the ovarian surface epithelium in all groups, and there was a significant difference between the treatment groups and the saline group (p<0.0001). Treatment of rats with edaravone reduced caspase-3 expression in a dose-dependent manner. Moreover, biochemical measurements of oxidative stress markers (MDA, GSH and TAC) revealed that prophylactic edaravone treatment attenuated oxidative stress induced by I/R injury.\n\nQuestion: Attenuation of ischemia/reperfusion-induced ovarian damage in rats: does edaravone offer protection?", "question_only": "Attenuation of ischemia/reperfusion-induced ovarian damage in rats: does edaravone offer protection?", "context": "Twenty-eight female Sprague Dawley rats were allocated randomly to 4 groups. The sham group (group 1) was only subjected to catheter insertion, not to pneumoperitoneum. Group 2 received a 1 mg/kg dose of 0.9% sodium chloride by the intraperitoneal route for 10 min before pneumoperitoneum. Groups 3 and 4 received 6 and 12 mg/kg edaravone, respectively, by the intraperitoneal route for 10 min before pneumoperitoneum. After 60 min of pneumoperitoneum, the gas was deflated. Immediately after the reperfusion period, both ovaries were excised for histological scoring, caspase-3 immunohistochemistry and biochemical evaluation including glutathione (GSH) and malondialdehyde (MDA) levels. Also, total antioxidant capacity (TAC) was measured in plasma samples to evaluate the antioxidant effect of edaravone. Ovarian sections in the saline group revealed higher scores for follicular degeneration and edema (p<0.0001) when compared with the sham group. Administration of different doses of edaravone in rats significantly prevented degenerative changes in the ovary (p<0.0001). Caspase-3 expression was only detected in the ovarian surface epithelium in all groups, and there was a significant difference between the treatment groups and the saline group (p<0.0001). Treatment of rats with edaravone reduced caspase-3 expression in a dose-dependent manner. Moreover, biochemical measurements of oxidative stress markers (MDA, GSH and TAC) revealed that prophylactic edaravone treatment attenuated oxidative stress induced by I/R injury.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "These results indicate that prophylactic treatment with edaravone prevents I/R-induced ovarian damage during pneumoperitoneum in an experimental rat model.", "meshes": ["Animals", "Antipyrine", "Caspase 3", "Female", "Free Radical Scavengers", "Glutathione", "Immunohistochemistry", "Malondialdehyde", "Ovary", "Rats", "Rats, Sprague-Dawley", "Reperfusion Injury"], "year": "2013"}
{"id": "pubmedqa_11438275", "dataset": "pubmedqa", "question": "Context: It is generally believed that positioning of the patient in a head-down tilt (Trendelenberg position) decreases the likelihood of a venous air embolism during liver resection. The physiological effect of variation in horizontal attitude on central and hepatic venous pressure was measured in 10 patients during liver surgery. Hemodynamic indices were recorded with the operating table in the horizontal, 20 degrees head-up and 20 degrees head-down positions. There was no demonstrable pressure gradient between the hepatic and central venous levels in any of the positions. The absolute pressures did, however, vary in a predictable way, being highest in the head-down and lowest during head-up tilt. However, on no occasion was a negative intraluminal pressure recorded.\n\nQuestion: Does patient position during liver surgery influence the risk of venous air embolism?", "question_only": "Does patient position during liver surgery influence the risk of venous air embolism?", "context": "It is generally believed that positioning of the patient in a head-down tilt (Trendelenberg position) decreases the likelihood of a venous air embolism during liver resection. The physiological effect of variation in horizontal attitude on central and hepatic venous pressure was measured in 10 patients during liver surgery. Hemodynamic indices were recorded with the operating table in the horizontal, 20 degrees head-up and 20 degrees head-down positions. There was no demonstrable pressure gradient between the hepatic and central venous levels in any of the positions. The absolute pressures did, however, vary in a predictable way, being highest in the head-down and lowest during head-up tilt. However, on no occasion was a negative intraluminal pressure recorded.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The effect on venous pressures caused by the change in patient positioning alone during liver surgery does not affect the risk of venous air embolism.", "meshes": ["Adult", "Aged", "Central Venous Pressure", "Embolism, Air", "Female", "Head-Down Tilt", "Hepatectomy", "Hepatic Veins", "Humans", "Male", "Middle Aged", "Posture", "Risk Factors", "Vena Cava, Inferior", "Venous Pressure"], "year": "2001"}
{"id": "pubmedqa_26867834", "dataset": "pubmedqa", "question": "Context: Children with recurrent protracted bacterial bronchitis (PBB) and bronchiectasis share common features, and PBB is likely a forerunner to bronchiectasis. Both diseases are associated with neutrophilic inflammation and frequent isolation of potentially pathogenic microorganisms, including nontypeable Haemophilus influenzae (NTHi), from the lower airway. Defective alveolar macrophage phagocytosis of apoptotic bronchial epithelial cells (efferocytosis), as found in other chronic lung diseases, may also contribute to tissue damage and neutrophil persistence. Thus, in children with bronchiectasis or PBB and in control subjects, we quantified the phagocytosis of airway apoptotic cells and NTHi by alveolar macrophages and related the phagocytic capacity to clinical and airway inflammation. Children with bronchiectasis (n = 55) or PBB (n = 13) and control subjects (n = 13) were recruited. Alveolar macrophage phagocytosis, efferocytosis, and expression of phagocytic scavenger receptors were assessed by flow cytometry. Bronchoalveolar lavage fluid interleukin (IL) 1β was measured by enzyme-linked immunosorbent assay. For children with PBB or bronchiectasis, macrophage phagocytic capacity was significantly lower than for control subjects (P = .003 and P<.001 for efferocytosis and P = .041 and P = .004 for phagocytosis of NTHi; PBB and bronchiectasis, respectively); median phagocytosis of NTHi for the groups was as follows: bronchiectasis, 13.7% (interquartile range [IQR], 11%-16%); PBB, 16% (IQR, 11%-16%); control subjects, 19.0% (IQR, 13%-21%); and median efferocytosis for the groups was as follows: bronchiectasis, 14.1% (IQR, 10%-16%); PBB, 16.2% (IQR, 14%-17%); control subjects, 18.1% (IQR, 16%-21%). Mannose receptor expression was significantly reduced in the bronchiectasis group (P = .019), and IL-1β increased in both bronchiectasis and PBB groups vs control subjects.\n\nQuestion: Is Alveolar Macrophage Phagocytic Dysfunction in Children With Protracted Bacterial Bronchitis a Forerunner to Bronchiectasis?", "question_only": "Is Alveolar Macrophage Phagocytic Dysfunction in Children With Protracted Bacterial Bronchitis a Forerunner to Bronchiectasis?", "context": "Children with recurrent protracted bacterial bronchitis (PBB) and bronchiectasis share common features, and PBB is likely a forerunner to bronchiectasis. Both diseases are associated with neutrophilic inflammation and frequent isolation of potentially pathogenic microorganisms, including nontypeable Haemophilus influenzae (NTHi), from the lower airway. Defective alveolar macrophage phagocytosis of apoptotic bronchial epithelial cells (efferocytosis), as found in other chronic lung diseases, may also contribute to tissue damage and neutrophil persistence. Thus, in children with bronchiectasis or PBB and in control subjects, we quantified the phagocytosis of airway apoptotic cells and NTHi by alveolar macrophages and related the phagocytic capacity to clinical and airway inflammation. Children with bronchiectasis (n = 55) or PBB (n = 13) and control subjects (n = 13) were recruited. Alveolar macrophage phagocytosis, efferocytosis, and expression of phagocytic scavenger receptors were assessed by flow cytometry. Bronchoalveolar lavage fluid interleukin (IL) 1β was measured by enzyme-linked immunosorbent assay. For children with PBB or bronchiectasis, macrophage phagocytic capacity was significantly lower than for control subjects (P = .003 and P<.001 for efferocytosis and P = .041 and P = .004 for phagocytosis of NTHi; PBB and bronchiectasis, respectively); median phagocytosis of NTHi for the groups was as follows: bronchiectasis, 13.7% (interquartile range [IQR], 11%-16%); PBB, 16% (IQR, 11%-16%); control subjects, 19.0% (IQR, 13%-21%); and median efferocytosis for the groups was as follows: bronchiectasis, 14.1% (IQR, 10%-16%); PBB, 16.2% (IQR, 14%-17%); control subjects, 18.1% (IQR, 16%-21%). Mannose receptor expression was significantly reduced in the bronchiectasis group (P = .019), and IL-1β increased in both bronchiectasis and PBB groups vs control subjects.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "A reduced alveolar macrophage phagocytic host response to apoptotic cells or NTHi may contribute to neutrophilic inflammation and NTHi colonization in both PBB and bronchiectasis. Whether this mechanism also contributes to the progression of PBB to bronchiectasis remains unknown.", "meshes": ["Apoptosis", "Bacterial Infections", "Bronchiectasis", "Bronchitis", "Bronchoalveolar Lavage Fluid", "Cell Line", "Child, Preschool", "Enzyme-Linked Immunosorbent Assay", "Female", "Humans", "Infant", "Macrophages, Alveolar", "Male", "Phagocytosis"], "year": "2016"}
{"id": "pubmedqa_18269157", "dataset": "pubmedqa", "question": "Context: To describe the biomechanical and wound healing characteristics of corneas after excimer laser keratorefractive surgery. Histologic, ultrastructural, and cohesive tensile strength evaluations were performed on 25 normal human corneal specimens, 206 uncomplicated LASIK specimens, 17 uncomplicated sub-Bowman's keratomileusis (SBK) specimens, 4 uncomplicated photorefractive keratectomy (PRK) specimens, 2 uncomplicated advanced surface ablation (ASA) specimens, 5 keratoconus specimens, 12 postoperative LASIK ectasia specimens, and 1 postoperative PRK ectasia specimen and compared to previously published studies. Histologic and ultrastructural studies of normal corneas showed significant differences in the direction of collagen fibrils and/or the degree of lamellar interweaving in Bowman's layer, the anterior third of the corneal stroma, the posterior two-thirds of the corneal stroma, and Descemet's membrane. Cohesive tensile strength testing directly supported these morphologic findings as the stronger, more rigid regions of the cornea were located anteriorly and peripherally. This suggests that PRK and ASA, and secondarily SBK, should be biomechanically safer than conventional LASIK with regard to risk for causing keratectasia after surgery. Because adult human corneal stromal wounds heal slowly and incompletely, all excimer laser keratorefractive surgical techniques still have some distinct disadvantages due to inadequate reparative wound healing. Despite reducing some of the risk for corneal haze compared to conventional PRK, ASA cases still can develop corneal haze or breakthrough haze from the hypercellular fibrotic stromal scarring. In contrast, similar to conventional LASIK, SBK still has the short- and long-term potential for interface wound complications from the hypocellular primitive stromal scar.\n\nQuestion: Biomechanical and wound healing characteristics of corneas after excimer laser keratorefractive surgery: is there a difference between advanced surface ablation and sub-Bowman's keratomileusis?", "question_only": "Biomechanical and wound healing characteristics of corneas after excimer laser keratorefractive surgery: is there a difference between advanced surface ablation and sub-Bowman's keratomileusis?", "context": "To describe the biomechanical and wound healing characteristics of corneas after excimer laser keratorefractive surgery. Histologic, ultrastructural, and cohesive tensile strength evaluations were performed on 25 normal human corneal specimens, 206 uncomplicated LASIK specimens, 17 uncomplicated sub-Bowman's keratomileusis (SBK) specimens, 4 uncomplicated photorefractive keratectomy (PRK) specimens, 2 uncomplicated advanced surface ablation (ASA) specimens, 5 keratoconus specimens, 12 postoperative LASIK ectasia specimens, and 1 postoperative PRK ectasia specimen and compared to previously published studies. Histologic and ultrastructural studies of normal corneas showed significant differences in the direction of collagen fibrils and/or the degree of lamellar interweaving in Bowman's layer, the anterior third of the corneal stroma, the posterior two-thirds of the corneal stroma, and Descemet's membrane. Cohesive tensile strength testing directly supported these morphologic findings as the stronger, more rigid regions of the cornea were located anteriorly and peripherally. This suggests that PRK and ASA, and secondarily SBK, should be biomechanically safer than conventional LASIK with regard to risk for causing keratectasia after surgery. Because adult human corneal stromal wounds heal slowly and incompletely, all excimer laser keratorefractive surgical techniques still have some distinct disadvantages due to inadequate reparative wound healing. Despite reducing some of the risk for corneal haze compared to conventional PRK, ASA cases still can develop corneal haze or breakthrough haze from the hypercellular fibrotic stromal scarring. In contrast, similar to conventional LASIK, SBK still has the short- and long-term potential for interface wound complications from the hypocellular primitive stromal scar.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Ophthalmic pathology and basic science research show that SBK and ASA are improvements in excimer laser keratorefractive surgery compared to conventional LASIK or PRK, particularly with regard to maintaining corneal biomechanics and perhaps moderately reducing the risk of corneal haze. However, most of the disadvantages caused by wound healing issues remain.", "meshes": ["Biomechanical Phenomena", "Bowman Membrane", "Compressive Strength", "Cornea", "Corneal Surgery, Laser", "Humans", "Keratoconus", "Lasers, Excimer", "Tensile Strength", "Wound Healing"], "year": "2008"}
{"id": "pubmedqa_10927144", "dataset": "pubmedqa", "question": "Context: To examine whether p53 tumour suppressor gene alterations can be used to predict tumour response to pre-operative chemo-radiation in locally advanced rectal cancer in terms of reduction in tumour size and local failure. p53 alterations were studied in pre-treatment biopsy specimens of rectal carcinomas from 48 patients by immunohistochemistry (IHC) and polymerase chain reaction/single strand conformation polymorphism (PCR-SSCP) gene mutation analysis. Pre-operative pelvic radiotherapy was delivered with four fields, 45 Gy to the ICRU point in 25 fractions over 5 weeks. A radio-sensitising dose of 5-fluorouracil (500 mg/m(2)) was delivered concurrently for 6 days of the 5-week schedule (days 1, 2, 3 and days 22, 23 and 24). Total meso-rectal excision was planned 4 to 6 weeks from completion of pre-operative treatment. Response to therapy was assessed by macroscopic measurement of the surgical specimen by a pathologist who was unaware of the pre-treatment tumour size or of the p53 status. IHC evidence of p53 protein accumulation was found in 40% of tumours, p53 gene mutation in 35% and p53 alteration (either or both changes) in 46%. The average reduction in tumour size was 53% in the group with 'wild-type' p53 (IHC-/SSCP-) and 63% in the group with altered p53 (either IHC+ or SSCP+; P=0.18). No significant differences in tumour size reduction or local failure were observed in the groups with p53 overexpression or p53 mutation compared with normal.\n\nQuestion: Can p53 alterations be used to predict tumour response to pre-operative chemo-radiotherapy in locally advanced rectal cancer?", "question_only": "Can p53 alterations be used to predict tumour response to pre-operative chemo-radiotherapy in locally advanced rectal cancer?", "context": "To examine whether p53 tumour suppressor gene alterations can be used to predict tumour response to pre-operative chemo-radiation in locally advanced rectal cancer in terms of reduction in tumour size and local failure. p53 alterations were studied in pre-treatment biopsy specimens of rectal carcinomas from 48 patients by immunohistochemistry (IHC) and polymerase chain reaction/single strand conformation polymorphism (PCR-SSCP) gene mutation analysis. Pre-operative pelvic radiotherapy was delivered with four fields, 45 Gy to the ICRU point in 25 fractions over 5 weeks. A radio-sensitising dose of 5-fluorouracil (500 mg/m(2)) was delivered concurrently for 6 days of the 5-week schedule (days 1, 2, 3 and days 22, 23 and 24). Total meso-rectal excision was planned 4 to 6 weeks from completion of pre-operative treatment. Response to therapy was assessed by macroscopic measurement of the surgical specimen by a pathologist who was unaware of the pre-treatment tumour size or of the p53 status. IHC evidence of p53 protein accumulation was found in 40% of tumours, p53 gene mutation in 35% and p53 alteration (either or both changes) in 46%. The average reduction in tumour size was 53% in the group with 'wild-type' p53 (IHC-/SSCP-) and 63% in the group with altered p53 (either IHC+ or SSCP+; P=0.18). No significant differences in tumour size reduction or local failure were observed in the groups with p53 overexpression or p53 mutation compared with normal.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "p53 alteration detected by IHC or SSCP analysis is not a clinically useful predictor of local response to pre-operative adjuvant therapy in advanced rectal carcinoma.", "meshes": ["Adenocarcinoma", "Adult", "Aged", "Aged, 80 and over", "Antimetabolites, Antineoplastic", "Biomarkers, Tumor", "Chemotherapy, Adjuvant", "Female", "Fluorouracil", "Follow-Up Studies", "Humans", "Male", "Middle Aged", "Preoperative Care", "Probability", "Prognosis", "Prospective Studies", "Radiotherapy, Adjuvant", "Rectal Neoplasms", "Sensitivity and Specificity", "Survival Analysis", "Treatment Outcome", "Tumor Suppressor Protein p53"], "year": "2000"}
{"id": "pubmedqa_20537205", "dataset": "pubmedqa", "question": "Context: Halofantrine is a newly developed antimalarial drug used for the treatment of Plasmodium falciparum malaria. The introduction of this drug has been delayed because of its possible side effects, and due to insufficient studies on adverse reactions in humans. There have been no studies investigating its effect on hearing. Thirty guinea pigs were divided into three groups: a control group, a halofantrine therapeutic dose group and a halofantrine double therapeutic dose group. One cochlea specimen from each animal was stained with haematoxylin and eosin and the other with toluidine blue. No changes were detected in the control group. The halofantrine therapeutic dose group showed loss and distortion of inner hair cells and inner phalangeal cells, and loss of spiral ganglia cells. In the halofantrine double therapeutic dose group, the inner and outer hair cells were distorted and there was loss of spiral ganglia cells.\n\nQuestion: Is halofantrine ototoxic?", "question_only": "Is halofantrine ototoxic?", "context": "Halofantrine is a newly developed antimalarial drug used for the treatment of Plasmodium falciparum malaria. The introduction of this drug has been delayed because of its possible side effects, and due to insufficient studies on adverse reactions in humans. There have been no studies investigating its effect on hearing. Thirty guinea pigs were divided into three groups: a control group, a halofantrine therapeutic dose group and a halofantrine double therapeutic dose group. One cochlea specimen from each animal was stained with haematoxylin and eosin and the other with toluidine blue. No changes were detected in the control group. The halofantrine therapeutic dose group showed loss and distortion of inner hair cells and inner phalangeal cells, and loss of spiral ganglia cells. In the halofantrine double therapeutic dose group, the inner and outer hair cells were distorted and there was loss of spiral ganglia cells.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Halofantrine has mild to moderate pathological effects on cochlea histology, and can be considered an ototoxic drug.", "meshes": ["Animals", "Antimalarials", "Cochlea", "Dose-Response Relationship, Drug", "Guinea Pigs", "Hair Cells, Auditory, Outer", "Phenanthrenes", "Staining and Labeling"], "year": "2010"}
{"id": "pubmedqa_20121683", "dataset": "pubmedqa", "question": "Context: Community-based medical education is growing to meet the increased demand for quality clinical education in expanded settings, and its sustainability relies on patient participation. This study investigated patients' views on being used as an educational resource for teaching medical students. Questionnaire-based survey. Patients attending six rural and 11 regional general practices in New South Wales over 18 teaching sessions in November 2008, who consented to student involvement in their consultation. Patient perceptions, expectations and acceptance of medical student involvement in consultations, assessed by surveys before and after their consultations. 118 of 122 patients consented to medical student involvement; of these, 117 (99%) completed a survey before the consultation, and 100 (85%) after the consultation. Patients were overwhelmingly positive about their doctor and practice being involved in student teaching and felt they themselves played an important role. Pre-consultation, patients expressed reluctance to allow students to conduct some or all aspects of the consultation independently. However, after the consultation, they reported they would have accepted higher levels of involvement than actually occurred.\n\nQuestion: Are patients willing participants in the new wave of community-based medical education in regional and rural Australia?", "question_only": "Are patients willing participants in the new wave of community-based medical education in regional and rural Australia?", "context": "Community-based medical education is growing to meet the increased demand for quality clinical education in expanded settings, and its sustainability relies on patient participation. This study investigated patients' views on being used as an educational resource for teaching medical students. Questionnaire-based survey. Patients attending six rural and 11 regional general practices in New South Wales over 18 teaching sessions in November 2008, who consented to student involvement in their consultation. Patient perceptions, expectations and acceptance of medical student involvement in consultations, assessed by surveys before and after their consultations. 118 of 122 patients consented to medical student involvement; of these, 117 (99%) completed a survey before the consultation, and 100 (85%) after the consultation. Patients were overwhelmingly positive about their doctor and practice being involved in student teaching and felt they themselves played an important role. Pre-consultation, patients expressed reluctance to allow students to conduct some or all aspects of the consultation independently. However, after the consultation, they reported they would have accepted higher levels of involvement than actually occurred.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Patients in regional and rural settings were willing partners in developing skills of junior medical students, who had greater involvement in patient consultations than previously reported for urban students. Our study extends the findings from urban general practice that patients are underutilised partners in community-based medical training. The support of patients from regional and rural settings could facilitate the expansion of primary care-based medical education in these areas of workforce need.", "meshes": ["Community Health Services", "Education, Medical, Graduate", "Family Practice", "Health Care Surveys", "Humans", "New South Wales", "Patient Satisfaction", "Physician-Patient Relations", "Problem-Based Learning", "Rural Health Services"], "year": "2010"}
{"id": "pubmedqa_12855939", "dataset": "pubmedqa", "question": "Context: longitudinal descriptive study. 2 large nursing homes in Turin, Italy. 418 dependent elderly (83 males, 335 females, mean age 83.7+/-8.5 y, range 55-102) living in the nursing homes. the prevalence of peripheral arterial disease (PAD) was evaluated using a Doppler Ultrasound measurement of AAI (Ankle/Arm blood pressure Index). Death causes according to ICD-9-CM were ascertained on patient's clinical records. Diagnosis of PAD was made in 122 subjects (29.2%) with AAI<0.90. After a 3 year follow-up 203 patients (48.6%) died. The presence of PAD was not related to total mortality or to mortality for ischemic heart disease (IHD), cerebrovascular disease or other causes. IHD mortality was significantly and independently related to low haemoglobin values, previous cerebrovascular disease, polypharmacy and poor mobility conditions.\n\nQuestion: Is ankle/arm pressure predictive for cardiovascular mortality in older patients living in nursing homes?", "question_only": "Is ankle/arm pressure predictive for cardiovascular mortality in older patients living in nursing homes?", "context": "longitudinal descriptive study. 2 large nursing homes in Turin, Italy. 418 dependent elderly (83 males, 335 females, mean age 83.7+/-8.5 y, range 55-102) living in the nursing homes. the prevalence of peripheral arterial disease (PAD) was evaluated using a Doppler Ultrasound measurement of AAI (Ankle/Arm blood pressure Index). Death causes according to ICD-9-CM were ascertained on patient's clinical records. Diagnosis of PAD was made in 122 subjects (29.2%) with AAI<0.90. After a 3 year follow-up 203 patients (48.6%) died. The presence of PAD was not related to total mortality or to mortality for ischemic heart disease (IHD), cerebrovascular disease or other causes. IHD mortality was significantly and independently related to low haemoglobin values, previous cerebrovascular disease, polypharmacy and poor mobility conditions.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The prevalence of PAD is high in nursing home residents. AAI is not predictive for IHD mortality in this population. In very frail elderly traditional risk factors and PAD are less important predictors of death compared to poor functional status, nutritional factors and previous cardiovascular disease.", "meshes": ["Aged", "Aged, 80 and over", "Ankle", "Arm", "Blood Pressure", "Cardiovascular Diseases", "Female", "Humans", "Longitudinal Studies", "Male", "Nursing Homes", "Prognosis"], "year": "2003"}
{"id": "pubmedqa_12094116", "dataset": "pubmedqa", "question": "Context: The purpose of this study was to identify the relationships between leg muscle power and sprinting speed with changes of direction. the study was designed to describe relationships between physical qualities and a component of sports performance. testing was conducted in an indoor sports hall and a biomechanics laboratory. 15 male participants were required to be free of injury and have recent experience competing in sports involving sprints with changes of direction. subjects were timed in 8 m sprints in a straight line and with various changes of direction. They were also tested for bilateral and unilateral leg extensor muscle concentric power output by an isokinetic squat and reactive strength by a drop jump. The correlations between concentric power and straight sprinting speed were non-significant whereas the relationships between reactive strength and straight speed were statistically significant. Correlations between muscle power and speed while changing direction were generally low and non-significant for concentric leg power with some moderate and significant (p<0.05) coefficients found for reactive strength. The participants who turned faster to one side tended to have a reactive strength dominance in the leg responsible for the push-off action.\n\nQuestion: Is muscle power related to running speed with changes of direction?", "question_only": "Is muscle power related to running speed with changes of direction?", "context": "The purpose of this study was to identify the relationships between leg muscle power and sprinting speed with changes of direction. the study was designed to describe relationships between physical qualities and a component of sports performance. testing was conducted in an indoor sports hall and a biomechanics laboratory. 15 male participants were required to be free of injury and have recent experience competing in sports involving sprints with changes of direction. subjects were timed in 8 m sprints in a straight line and with various changes of direction. They were also tested for bilateral and unilateral leg extensor muscle concentric power output by an isokinetic squat and reactive strength by a drop jump. The correlations between concentric power and straight sprinting speed were non-significant whereas the relationships between reactive strength and straight speed were statistically significant. Correlations between muscle power and speed while changing direction were generally low and non-significant for concentric leg power with some moderate and significant (p<0.05) coefficients found for reactive strength. The participants who turned faster to one side tended to have a reactive strength dominance in the leg responsible for the push-off action.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The relationships between leg muscle power and change-of-direction speed were not consistent. Reactive strength as measured by the drop jump appears to have some importance for lateral change-of-direction speed, possibly because of similar push-off actions. It was concluded that reactive strength of the leg extensor muscles has some importance in change-of-direction performance but the other technical and perceptual factors than influence agility performance should also be considered.", "meshes": ["Adolescent", "Adult", "Humans", "Kinesis", "Leg", "Male", "Muscle Contraction", "Muscle, Skeletal", "Reaction Time", "Running"], "year": "2002"}
{"id": "pubmedqa_20130378", "dataset": "pubmedqa", "question": "Context: Congenital cytomegalovirus infection is currently the leading cause of congenital infection in 0.2-2.2% of live births worldwide leading to variable serious sequalae. The aim of the study was to determine if low birth weight is an indicator of CMV congenital infection evidenced by detecting CMV-DNA in umbilical cord blood at the time of delivery. CMV-IgG and IgM antibodies and CMV-DNAemia were assessed in umbilical cord blood of two hundreds newborns, one hundred of whom had birth weight<or = 2700 gram and/or head circumference<or = 32 cm. CMV-IgM was not detected, while CMV-IgG was positive in 80-90% of the two hundreds tested newborns. CMV-DNA was detected in four out of the 200 newborns. One of them was over the adopted weight limit (>2700 gram).\n\nQuestion: Is low birth weight a risk indicator for congenital cytomegalovirus infection?", "question_only": "Is low birth weight a risk indicator for congenital cytomegalovirus infection?", "context": "Congenital cytomegalovirus infection is currently the leading cause of congenital infection in 0.2-2.2% of live births worldwide leading to variable serious sequalae. The aim of the study was to determine if low birth weight is an indicator of CMV congenital infection evidenced by detecting CMV-DNA in umbilical cord blood at the time of delivery. CMV-IgG and IgM antibodies and CMV-DNAemia were assessed in umbilical cord blood of two hundreds newborns, one hundred of whom had birth weight<or = 2700 gram and/or head circumference<or = 32 cm. CMV-IgM was not detected, while CMV-IgG was positive in 80-90% of the two hundreds tested newborns. CMV-DNA was detected in four out of the 200 newborns. One of them was over the adopted weight limit (>2700 gram).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "CMV-IgM and IgG antibodies assessment was not a potential discriminative test to identify congenitally infected newborns. In addition, low birth weight and small head circumference at birth failed to predict congenital CMV infection. CMV-DNA detection in umbilical cord blood at the time of delivery using real-time PCR of all newborns is recommended as decisive, rapid and non-invasive test.", "meshes": ["Antibodies, Viral", "Body Weights and Measures", "Cytomegalovirus", "Cytomegalovirus Infections", "DNA, Viral", "Fetal Blood", "Humans", "Immunoglobulin G", "Immunoglobulin M", "Infant, Low Birth Weight", "Infant, Newborn", "Infant, Newborn, Diseases", "Polymerase Chain Reaction", "Risk Factors", "Viremia"], "year": "2009"}
{"id": "pubmedqa_22449464", "dataset": "pubmedqa", "question": "Context: Selection into general practice training is undertaken using a competency based approach. The clear advantage of this approach over traditional methods has been demonstrated through evaluation of its validity and reliability. However, the relationship between selection  and performance in the Royal College of General Practitioner examinations (MRCGP) has yet to be explored. The MRCGP comprises of an applied knowledge test (AKT), a clinical skills assessment (CSA) and workplace-based assessments (WPBA).AIM: To explore the predictive validity of general  practice selection scores using the AKT and CSA elements of the MRCGP as a final outcome measure. This study carried out a retrospective analysis of 101 trainees from the Wales Deanery who were successfully selected on to general practice training in 2007. Selection data consisted  of an overall selection score as well as scores from each individual stage of selection. Correlation was used to explore associations between selection scores and examination scores. The score for overall performance at selection achieved statistically significant correlation  with examination performance (r = 0.491 for the AKT and r = 0.526 for the CSA, P<0.01).\n\nQuestion: Do general practice selection scores predict success at MRCGP?", "question_only": "Do general practice selection scores predict success at MRCGP?", "context": "Selection into general practice training is undertaken using a competency based approach. The clear advantage of this approach over traditional methods has been demonstrated through evaluation of its validity and reliability. However, the relationship between selection  and performance in the Royal College of General Practitioner examinations (MRCGP) has yet to be explored. The MRCGP comprises of an applied knowledge test (AKT), a clinical skills assessment (CSA) and workplace-based assessments (WPBA).AIM: To explore the predictive validity of general  practice selection scores using the AKT and CSA elements of the MRCGP as a final outcome measure. This study carried out a retrospective analysis of 101 trainees from the Wales Deanery who were successfully selected on to general practice training in 2007. Selection data consisted  of an overall selection score as well as scores from each individual stage of selection. Correlation was used to explore associations between selection scores and examination scores. The score for overall performance at selection achieved statistically significant correlation  with examination performance (r = 0.491 for the AKT and r = 0.526 for the CSA, P<0.01).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The general practice selection process is predictive of future performance in the MRCGP.", "meshes": ["Achievement", "Clinical Competence", "Female", "General Practice", "Humans", "Internship and Residency", "Male", "Retrospective Studies", "School Admission Criteria", "United Kingdom"], "year": "2012"}
{"id": "pubmedqa_22534881", "dataset": "pubmedqa", "question": "Context: The correlation between radiographic transition zone on contrast enema in Hirschsprung's disease and the total length of aganglionosis is known to be inaccurate. The aim of our study was to analyse this correlation more precisely to improve preoperative planning of the corrective surgery. From 1998 to 2009, 79 patients were operated on for Hirschsprung's disease. All available preoperative contrast enemas (n = 61) had been single blind reviewed by the same radiologist who defined the radiographic transition zone when present in vertebral level. Four groups were determined (rectal, rectosigmoid, long segment, and absence of transition zone) and by Kappa coefficient of agreement correlated to the length of aganglionosis in the pathological report. Radiological findings were concordant with the specimen in pathology in 8 cases of 19 in rectal form (42 %), in 20 cases of 35 in rectosigmoid form (57 %), in all 6 cases of long-segment form (100 %), in the 2 cases of total colonic form (100 %) with a global agreement of 58.1 %, κ = 0.39 CI [0.24; 0.57].\n\nQuestion: Does the radiographic transition zone correlate with the level of aganglionosis on the specimen in Hirschsprung's disease?", "question_only": "Does the radiographic transition zone correlate with the level of aganglionosis on the specimen in Hirschsprung's disease?", "context": "The correlation between radiographic transition zone on contrast enema in Hirschsprung's disease and the total length of aganglionosis is known to be inaccurate. The aim of our study was to analyse this correlation more precisely to improve preoperative planning of the corrective surgery. From 1998 to 2009, 79 patients were operated on for Hirschsprung's disease. All available preoperative contrast enemas (n = 61) had been single blind reviewed by the same radiologist who defined the radiographic transition zone when present in vertebral level. Four groups were determined (rectal, rectosigmoid, long segment, and absence of transition zone) and by Kappa coefficient of agreement correlated to the length of aganglionosis in the pathological report. Radiological findings were concordant with the specimen in pathology in 8 cases of 19 in rectal form (42 %), in 20 cases of 35 in rectosigmoid form (57 %), in all 6 cases of long-segment form (100 %), in the 2 cases of total colonic form (100 %) with a global agreement of 58.1 %, κ = 0.39 CI [0.24; 0.57].", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Correlation between level of radiographic transition zone on contrast enema and length of aganglionosis remains low. Systematic preoperative biopsy by coelioscopy or ombilical incision is mandatory.", "meshes": ["Female", "Hirschsprung Disease", "Humans", "Infant", "Infant, Newborn", "Male", "Radiography", "Retrospective Studies"], "year": "2012"}
{"id": "pubmedqa_25186850", "dataset": "pubmedqa", "question": "Context: To compare the effect of student examiners (SE) to that of faculty examiners (FE) on examinee performance in an OSCE as well as on post-assessment evaluation in the area of emergency medicine management. An OSCE test-format (seven stations: Advanced Cardiac Life Support (ACLS), Basic Life Support (BLS), Trauma-Management (TM), Pediatric-Emergencies (PE), Acute-Coronary-Syndrome (ACS), Airway-Management (AM), and Obstetrical-Emergencies (OE)) was administered to 207 medical students in their third year of training after they had received didactics in emergency medicine management. Participants were randomly assigned to one of the two simultaneously run tracks: either with SE (n = 110) or with FE (n = 98). Students were asked to rate each OSCE station and to provide their overall OSCE perception by means of a standardized questionnaire. The independent samples t-test was used and effect sizes were calculated (Cohens d). Students achieved significantly higher scores for the OSCE stations \"TM\", \"AM\", and \"OE\" as well as \"overall OSCE score\" in the SE track, whereas the station score for \"PE\" was significantly higher for students in the FE track. Mostly small effect sizes were reported. In the post-assessment evaluation portion of the study, students gave significant higher ratings for the ACS station and \"overall OSCE evaluation\" in the FE track; also with small effect sizes.\n\nQuestion: May student examiners be reasonable substitute examiners for faculty in an undergraduate OSCE on medical emergencies?", "question_only": "May student examiners be reasonable substitute examiners for faculty in an undergraduate OSCE on medical emergencies?", "context": "To compare the effect of student examiners (SE) to that of faculty examiners (FE) on examinee performance in an OSCE as well as on post-assessment evaluation in the area of emergency medicine management. An OSCE test-format (seven stations: Advanced Cardiac Life Support (ACLS), Basic Life Support (BLS), Trauma-Management (TM), Pediatric-Emergencies (PE), Acute-Coronary-Syndrome (ACS), Airway-Management (AM), and Obstetrical-Emergencies (OE)) was administered to 207 medical students in their third year of training after they had received didactics in emergency medicine management. Participants were randomly assigned to one of the two simultaneously run tracks: either with SE (n = 110) or with FE (n = 98). Students were asked to rate each OSCE station and to provide their overall OSCE perception by means of a standardized questionnaire. The independent samples t-test was used and effect sizes were calculated (Cohens d). Students achieved significantly higher scores for the OSCE stations \"TM\", \"AM\", and \"OE\" as well as \"overall OSCE score\" in the SE track, whereas the station score for \"PE\" was significantly higher for students in the FE track. Mostly small effect sizes were reported. In the post-assessment evaluation portion of the study, students gave significant higher ratings for the ACS station and \"overall OSCE evaluation\" in the FE track; also with small effect sizes.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "It seems quite admissible and justified to encourage medical students to officiate as examiners in undergraduate emergency medicine OSCE formative testing, but not necessarily in summative assessment evaluations.", "meshes": ["Adult", "Clinical Competence", "Education, Medical, Undergraduate", "Educational Measurement", "Emergency Medicine", "Faculty, Medical", "Female", "Humans", "Male", "Medical History Taking", "Patient Simulation", "Physical Examination", "Students, Medical", "Young Adult"], "year": "2015"}
{"id": "pubmedqa_18403944", "dataset": "pubmedqa", "question": "Context: Celiac disease (CD) is believed to be a permanent intolerance to gluten. A number of patients, however, discontinue the gluten-free diet (GFD) without developing symptoms or signs. The aim of our study was to investigate whether CD patients are capable of developing tolerance to gluten. All 77 adult patients from our hospital known to have biopsy-proven CD for more than 10 years were invited to participate. We investigated symptoms, gluten consumption, antibodies for CD and other autoimmunity, human leukocyte antigen (HLA)-typing, bone mineral density, and performed small bowel biopsies. Tolerance was defined as no immunological or histological signs of CD while consuming gluten. Sixty-six patients accepted participation, but after review of the diagnostic biopsies 53 were found to have true CD. Twenty-three percent of patients had a gluten-containing diet, 15% admitted gluten transgression and 62% followed the GFD. Patients on a GFD had significantly more osteoporosis. Normal small bowel mucosa was found in four of eight on gluten-containing diet and in four of four with gluten transgression. Two patients were considered to have developed tolerance to gluten. One of them was HLA-DQ2/DQ8 negative.\n\nQuestion: Gluten tolerance in adult patients with celiac disease 20 years after diagnosis?", "question_only": "Gluten tolerance in adult patients with celiac disease 20 years after diagnosis?", "context": "Celiac disease (CD) is believed to be a permanent intolerance to gluten. A number of patients, however, discontinue the gluten-free diet (GFD) without developing symptoms or signs. The aim of our study was to investigate whether CD patients are capable of developing tolerance to gluten. All 77 adult patients from our hospital known to have biopsy-proven CD for more than 10 years were invited to participate. We investigated symptoms, gluten consumption, antibodies for CD and other autoimmunity, human leukocyte antigen (HLA)-typing, bone mineral density, and performed small bowel biopsies. Tolerance was defined as no immunological or histological signs of CD while consuming gluten. Sixty-six patients accepted participation, but after review of the diagnostic biopsies 53 were found to have true CD. Twenty-three percent of patients had a gluten-containing diet, 15% admitted gluten transgression and 62% followed the GFD. Patients on a GFD had significantly more osteoporosis. Normal small bowel mucosa was found in four of eight on gluten-containing diet and in four of four with gluten transgression. Two patients were considered to have developed tolerance to gluten. One of them was HLA-DQ2/DQ8 negative.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Development of tolerance to gluten seems possible in some patients with CD. Further follow-up will show whether this tolerance is permanent or only a long-term return to latency. This feature may be associated with genetic characteristics, especially with HLA genotypes that differ from DQ2 or DQ8. More insight into the mechanisms of the development of gluten tolerance may help to distinguish those CD patients that might not require life-long GFD.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Autoantibodies", "Bone Density", "Celiac Disease", "Female", "Follow-Up Studies", "Glutens", "HLA-DQ Antigens", "Histocompatibility Testing", "Humans", "Immune Tolerance", "Intestinal Mucosa", "Intestine, Small", "Male", "Middle Aged", "Patient Compliance", "Severity of Illness Index"], "year": "2008"}
{"id": "pubmedqa_15466981", "dataset": "pubmedqa", "question": "Context: The combined use of free and total prostate-specific antigen (PSA) in early detection of prostate cancer has been controversial. This article systematically evaluates the discriminating capacity of a large number of combination tests. Free and total PSA were analyzed in stored serum samples taken prior to diagnosis in 429 cases and 1,640 controls from the Physicians' Health Study. We used a classification algorithm called logic regression to search for clinically useful tests combining total and percent free PSA and receiver operating characteristic analysis and compared these tests with those based on total and complexed PSA. Data were divided into training and test subsets. For robustness, we considered 35 test-train splits of the original data and computed receiver operating characteristic curves for each test data set. The average area under the receiver operating characteristic curve across test data sets was 0.74 for total PSA and 0.76 for the combination tests. Combination tests with higher sensitivity and specificity than PSA>4.0 ng/mL were identified 29 out of 35 times. All these tests extended the PSA reflex range to below 4.0 ng/mL. Receiver operating characteristic curve analysis indicated that the overall diagnostic performance as expressed by the area under the curve did not differ significantly for the different tests.\n\nQuestion: Prostate-specific antigen and free prostate-specific antigen in the early detection of prostate cancer: do combination tests improve detection?", "question_only": "Prostate-specific antigen and free prostate-specific antigen in the early detection of prostate cancer: do combination tests improve detection?", "context": "The combined use of free and total prostate-specific antigen (PSA) in early detection of prostate cancer has been controversial. This article systematically evaluates the discriminating capacity of a large number of combination tests. Free and total PSA were analyzed in stored serum samples taken prior to diagnosis in 429 cases and 1,640 controls from the Physicians' Health Study. We used a classification algorithm called logic regression to search for clinically useful tests combining total and percent free PSA and receiver operating characteristic analysis and compared these tests with those based on total and complexed PSA. Data were divided into training and test subsets. For robustness, we considered 35 test-train splits of the original data and computed receiver operating characteristic curves for each test data set. The average area under the receiver operating characteristic curve across test data sets was 0.74 for total PSA and 0.76 for the combination tests. Combination tests with higher sensitivity and specificity than PSA>4.0 ng/mL were identified 29 out of 35 times. All these tests extended the PSA reflex range to below 4.0 ng/mL. Receiver operating characteristic curve analysis indicated that the overall diagnostic performance as expressed by the area under the curve did not differ significantly for the different tests.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Tests combining total and percent free PSA show modest overall improvements over total PSA. However, utilization of percent free PSA below a PSA threshold of 4 ng/mL could translate into a practically important reduction in unnecessary biopsies without sacrificing cancers detected.", "meshes": ["Aged", "Case-Control Studies", "Diagnosis, Differential", "Humans", "Male", "Mass Screening", "Middle Aged", "Prostate-Specific Antigen", "Prostatic Neoplasms", "Reference Values", "Sensitivity and Specificity"], "year": "2004"}
{"id": "pubmedqa_22849512", "dataset": "pubmedqa", "question": "Context: The aim of this study is to explore whether availability of sports facilities, parks, and neighbourhood social capital (NSC) and their interaction are associated with leisure time sports participation among Dutch adolescents. Cross-sectional analyses were conducted on complete data from the last wave of the YouRAction evaluation trial. Adolescents (n = 852) completed a questionnaire asking for sports participation, perceived NSC and demographics. Ecometric methods were used to aggregate perceived NSC to zip code level. Availability of sports facilities and parks was assessed by means of geographic information systems within the zip-code area and within a 1600 meter buffer. Multilevel logistic regression analyses, with neighborhood and individual as levels, were conducted to examine associations between physical and social environmental factors and leisure time sports participation. Simple slopes analysis was conducted to decompose interaction effects. NSC was significantly associated with sports participation (OR: 3.51 (95%CI: 1.18;10.41)) after adjustment for potential confounders. Availability of sports facilities and availability of parks were not associated with sports participation. A significant interaction between NSC and density of parks within the neighbourhood area (OR: 1.22 (90%CI: 1.01;1.34)) was found. Decomposition of the interaction term showed that adolescents were most likely to engage in leisure time sports when both availability of parks and NSC were highest.\n\nQuestion: Are neighbourhood social capital and availability of sports facilities related to sports participation among Dutch adolescents?", "question_only": "Are neighbourhood social capital and availability of sports facilities related to sports participation among Dutch adolescents?", "context": "The aim of this study is to explore whether availability of sports facilities, parks, and neighbourhood social capital (NSC) and their interaction are associated with leisure time sports participation among Dutch adolescents. Cross-sectional analyses were conducted on complete data from the last wave of the YouRAction evaluation trial. Adolescents (n = 852) completed a questionnaire asking for sports participation, perceived NSC and demographics. Ecometric methods were used to aggregate perceived NSC to zip code level. Availability of sports facilities and parks was assessed by means of geographic information systems within the zip-code area and within a 1600 meter buffer. Multilevel logistic regression analyses, with neighborhood and individual as levels, were conducted to examine associations between physical and social environmental factors and leisure time sports participation. Simple slopes analysis was conducted to decompose interaction effects. NSC was significantly associated with sports participation (OR: 3.51 (95%CI: 1.18;10.41)) after adjustment for potential confounders. Availability of sports facilities and availability of parks were not associated with sports participation. A significant interaction between NSC and density of parks within the neighbourhood area (OR: 1.22 (90%CI: 1.01;1.34)) was found. Decomposition of the interaction term showed that adolescents were most likely to engage in leisure time sports when both availability of parks and NSC were highest.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "The results of this study indicate that leisure time sports participation is associated with levels of NSC, but not with availability of parks or sports facilities. In addition, NSC and availability of parks in the zip code area interacted in such a way that leisure time sports participation is most likely among adolescents living in zip code areas with higher levels of NSC, and higher availability of parks. Hence, availability of parks appears only to be important for leisure time sports participation when NSC is high.", "meshes": ["Adolescent", "Cross-Sectional Studies", "Female", "Fitness Centers", "Humans", "Leisure Activities", "Male", "Netherlands", "Odds Ratio", "Public Facilities", "Social Environment", "Sports"], "year": "2012"}
{"id": "pubmedqa_27217036", "dataset": "pubmedqa", "question": "Context: Longer duration of neoadjuvant (NA) imatinib (IM) used for locally advanced (LA) gastrointestinal stromal tumours (GIST) is not based on biology of the tumour reflected by kit mutation analysis. LA or locally recurrent (LR) GIST treated with NA IM from May 2008 to March 2015 from a prospective database were included in the analysis. Archived formalin-fixed paraffin-embedded tissues (FFPE) were used for testing KIT exons 9, 11, 13 and 17 by PCR. One hundred twenty-five patients with LA or LR GIST were treated with NA IM. Forty-five patients (36 %) had undergone c-kit mutation testing. Exon 11 was seen in 25 patients (55.5 %), 3 with exon 9 (6.7 %) and 2 with exon 13 (4.4 %). Twelve were wild type (26.6 %) and  3 (6.7 %) were declared uninterpretable. Response rate (RR) for the exon 11 mutants was higher than the non-exon 11 mutant group (84 vs. 40 %, p = 0.01). Disease stabilization rate (DSR) rates were also higher in the exon 11 subgroup than non-exon 11 group (92 vs. 75 %). Eighty-four per cent exon 11 and 75 % non-exon 11 mutants were surgical candidates. Patients undergoing surgery had significantly improved event free survival (EFS) (p < 0.001) compared to patients not undergoing surgery, with the same trend seen in OS (p = 0.021). Patients with a SD on response to NA IM had a lower EFS (p = 0.076) and OS compared to patients achieving CR/PR. There were no differences between the various exon variants in terms of outcomes and responses\n\nQuestion: Neoadjuvant Imatinib in Locally Advanced Gastrointestinal stromal Tumours, Will Kit Mutation Analysis Be a Pathfinder?", "question_only": "Neoadjuvant Imatinib in Locally Advanced Gastrointestinal stromal Tumours, Will Kit Mutation Analysis Be a Pathfinder?", "context": "Longer duration of neoadjuvant (NA) imatinib (IM) used for locally advanced (LA) gastrointestinal stromal tumours (GIST) is not based on biology of the tumour reflected by kit mutation analysis. LA or locally recurrent (LR) GIST treated with NA IM from May 2008 to March 2015 from a prospective database were included in the analysis. Archived formalin-fixed paraffin-embedded tissues (FFPE) were used for testing KIT exons 9, 11, 13 and 17 by PCR. One hundred twenty-five patients with LA or LR GIST were treated with NA IM. Forty-five patients (36 %) had undergone c-kit mutation testing. Exon 11 was seen in 25 patients (55.5 %), 3 with exon 9 (6.7 %) and 2 with exon 13 (4.4 %). Twelve were wild type (26.6 %) and  3 (6.7 %) were declared uninterpretable. Response rate (RR) for the exon 11 mutants was higher than the non-exon 11 mutant group (84 vs. 40 %, p = 0.01). Disease stabilization rate (DSR) rates were also higher in the exon 11 subgroup than non-exon 11 group (92 vs. 75 %). Eighty-four per cent exon 11 and 75 % non-exon 11 mutants were surgical candidates. Patients undergoing surgery had significantly improved event free survival (EFS) (p < 0.001) compared to patients not undergoing surgery, with the same trend seen in OS (p = 0.021). Patients with a SD on response to NA IM had a lower EFS (p = 0.076) and OS compared to patients achieving CR/PR. There were no differences between the various exon variants in terms of outcomes and responses", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Upfront evaluation of kit mutation status may help us in delineating separate treatment strategies for potentially biologically different tumours and assessing the correct timing of surgery for this subset of GIST.", "meshes": ["Adult", "Aged", "Antineoplastic Agents", "Female", "Gastrointestinal Stromal Tumors", "Humans", "Imatinib Mesylate", "Male", "Middle Aged", "Mutation", "Neoadjuvant Therapy", "Young Adult"], "year": "2016"}
{"id": "pubmedqa_25752725", "dataset": "pubmedqa", "question": "Context: Schizophrenia patients are typically found to have low IQ both pre- and post-onset, in comparison to the general population. However, a subgroup of patients displays above average IQ pre-onset. The nature of these patients' illness and its relationship to typical schizophrenia is not well understood. The current study sought to investigate the symptom profile of high-IQ schizophrenia patients. We identified 29 schizophrenia patients of exceptionally high pre-morbid intelligence (mean estimated pre-morbid intelligence quotient (IQ) of 120), of whom around half also showed minimal decline (less than 10 IQ points) from their estimated pre-morbid IQ. We compared their symptom scores (SAPS, SANS, OPCRIT, MADRS, GAF, SAI-E) with a comparison group of schizophrenia patients of typical IQ using multinomial logistic regression. The patients with very high pre-morbid IQ had significantly lower scores on negative and disorganised symptoms than typical patients (RRR=0.019; 95% CI=0.001, 0.675, P=0.030), and showed better global functioning and insight (RRR=1.082; 95% CI=1.020, 1.148; P=0.009). Those with a minimal post-onset IQ decline also showed higher levels of manic symptoms (RRR=8.213; 95% CI=1.042, 64.750, P=0.046).\n\nQuestion: Schizophrenia patients with high intelligence: A clinically distinct sub-type of schizophrenia?", "question_only": "Schizophrenia patients with high intelligence: A clinically distinct sub-type of schizophrenia?", "context": "Schizophrenia patients are typically found to have low IQ both pre- and post-onset, in comparison to the general population. However, a subgroup of patients displays above average IQ pre-onset. The nature of these patients' illness and its relationship to typical schizophrenia is not well understood. The current study sought to investigate the symptom profile of high-IQ schizophrenia patients. We identified 29 schizophrenia patients of exceptionally high pre-morbid intelligence (mean estimated pre-morbid intelligence quotient (IQ) of 120), of whom around half also showed minimal decline (less than 10 IQ points) from their estimated pre-morbid IQ. We compared their symptom scores (SAPS, SANS, OPCRIT, MADRS, GAF, SAI-E) with a comparison group of schizophrenia patients of typical IQ using multinomial logistic regression. The patients with very high pre-morbid IQ had significantly lower scores on negative and disorganised symptoms than typical patients (RRR=0.019; 95% CI=0.001, 0.675, P=0.030), and showed better global functioning and insight (RRR=1.082; 95% CI=1.020, 1.148; P=0.009). Those with a minimal post-onset IQ decline also showed higher levels of manic symptoms (RRR=8.213; 95% CI=1.042, 64.750, P=0.046).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "These findings provide evidence for the existence of a high-IQ variant of schizophrenia that is associated with markedly fewer negative symptoms than typical schizophrenia, and lends support to the idea of a psychosis spectrum or continuum over boundaried diagnostic categories.", "meshes": ["Adult", "Cognition", "Female", "Humans", "Intelligence", "Intelligence Tests", "Male", "Middle Aged", "Neuropsychological Tests", "Schizophrenia", "Schizophrenic Psychology"], "year": "2015"}
{"id": "pubmedqa_18693227", "dataset": "pubmedqa", "question": "Context: This study was performed to describe the treatment plan modifications after a geriatric oncology clinic. Assessment of health and functional status and cancer assessment was performed in older cancer patients referred to a cancer center. Between June 2004 and May 2005, 105 patients 70 years old or older referred to a geriatric oncology consultation at the Institut Curie cancer center were included. Functional status, nutritional status, mood, mobility, comorbidity, medication, social support, and place of residence were assessed. Oncology data and treatment decisions were recorded before and after this consultation. Data were analyzed for a possible correlation between one domain of the assessment and modification of the treatment plan. Patient characteristics included a median age of 79 years and a predominance of women with breast cancer. About one half of patients had an independent functional status. Nearly 15% presented severe undernourishment. Depression was suspected in 53.1% of cases. One third of these patients had>2 chronic diseases, and 74% of patients took>or =3 medications. Of the 93 patients with an initial treatment decision, the treatment plan was modified for 38.7% of cases after this assessment. Only body mass index and the absence of depressive symptoms were associated with a modification of the treatment plan.\n\nQuestion: Does a geriatric oncology consultation modify the cancer treatment plan for elderly patients?", "question_only": "Does a geriatric oncology consultation modify the cancer treatment plan for elderly patients?", "context": "This study was performed to describe the treatment plan modifications after a geriatric oncology clinic. Assessment of health and functional status and cancer assessment was performed in older cancer patients referred to a cancer center. Between June 2004 and May 2005, 105 patients 70 years old or older referred to a geriatric oncology consultation at the Institut Curie cancer center were included. Functional status, nutritional status, mood, mobility, comorbidity, medication, social support, and place of residence were assessed. Oncology data and treatment decisions were recorded before and after this consultation. Data were analyzed for a possible correlation between one domain of the assessment and modification of the treatment plan. Patient characteristics included a median age of 79 years and a predominance of women with breast cancer. About one half of patients had an independent functional status. Nearly 15% presented severe undernourishment. Depression was suspected in 53.1% of cases. One third of these patients had>2 chronic diseases, and 74% of patients took>or =3 medications. Of the 93 patients with an initial treatment decision, the treatment plan was modified for 38.7% of cases after this assessment. Only body mass index and the absence of depressive symptoms were associated with a modification of the treatment plan.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The geriatric oncology consultation led to a modification of the cancer treatment plan in more than one third of cases. Further studies are needed to determine whether these modifications improve the outcome of these older patients.", "meshes": ["Activities of Daily Living", "Affect", "Aged", "Aged, 80 and over", "Cancer Care Facilities", "Female", "Geriatric Assessment", "Humans", "Male", "Medical Oncology", "Neoplasms", "Referral and Consultation"], "year": "2008"}
{"id": "pubmedqa_18222909", "dataset": "pubmedqa", "question": "Context: The hypothesis was tested that pectin content and methylation degree participate in regulation of cell wall mechanical properties and in this way may affect tissue growth and freezing resistance over the course of plant cold acclimation and de-acclimation. Experiments were carried on the leaves of two double-haploid lines of winter oil-seed rape (Brassica napus subsp. oleifera), differing in winter survival and resistance to blackleg fungus (Leptosphaeria maculans). Plant acclimation in the cold (2 degrees C) brought about retardation of leaf expansion, concomitant with development of freezing resistance. These effects were associated with the increases in leaf tensile stiffness, cell wall and pectin contents, pectin methylesterase (EC 3.1.1.11) activity and the low-methylated pectin content, independently of the genotype studied. However, the cold-induced modifications in the cell wall properties were more pronounced in the leaves of the more pathogen-resistant genotype. De-acclimation promoted leaf expansion and reversed most of the cold-induced effects, with the exception of pectin methylesterase activity.\n\nQuestion: Are pectins involved in cold acclimation and de-acclimation of winter oil-seed rape plants?", "question_only": "Are pectins involved in cold acclimation and de-acclimation of winter oil-seed rape plants?", "context": "The hypothesis was tested that pectin content and methylation degree participate in regulation of cell wall mechanical properties and in this way may affect tissue growth and freezing resistance over the course of plant cold acclimation and de-acclimation. Experiments were carried on the leaves of two double-haploid lines of winter oil-seed rape (Brassica napus subsp. oleifera), differing in winter survival and resistance to blackleg fungus (Leptosphaeria maculans). Plant acclimation in the cold (2 degrees C) brought about retardation of leaf expansion, concomitant with development of freezing resistance. These effects were associated with the increases in leaf tensile stiffness, cell wall and pectin contents, pectin methylesterase (EC 3.1.1.11) activity and the low-methylated pectin content, independently of the genotype studied. However, the cold-induced modifications in the cell wall properties were more pronounced in the leaves of the more pathogen-resistant genotype. De-acclimation promoted leaf expansion and reversed most of the cold-induced effects, with the exception of pectin methylesterase activity.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The results show that the temperature-dependent modifications in pectin content and their methyl esterification degree correlate with changes in tensile strength of a leaf tissue, and in this way affect leaf expansion ability and its resistance to freezing and to fungus pathogens.", "meshes": ["Acclimatization", "Ascomycota", "Biomechanical Phenomena", "Brassica napus", "Carboxylic Ester Hydrolases", "Cell Enlargement", "Cell Wall", "Esterification", "Freezing", "Genotype", "Pectins", "Plant Diseases", "Plant Leaves"], "year": "2008"}
{"id": "pubmedqa_23735520", "dataset": "pubmedqa", "question": "Context: To determine the potential prognostic value of using functional magnetic resonance imaging (fMRI) to identify patients with disorders of consciousness, who show potential for recovery. Observational study. Unit for acute rehabilitation care. Patients (N=22) in a vegetative state (VS; n=10) and minimally conscious state (MCS; n=12) during the first 200 days after the initial incident. Not applicable. Further course on the Coma Recovery Scale-Revised. Participants performed a mental imagery fMRI paradigm. They were asked to alternately imagine playing tennis and navigating through their home. In 14 of the 22 examined patients (VS, n=5; MCS, n=9), a significant activation of the regions of interest (ROIs) of the mental imagery paradigm could be found. All 5 patients with activation of a significant blood oxygen level dependent signal, who were in a VS at the time of the fMRI examination, reached at least an MCS at the end of the observation period. In contrast, 5 participants in a VS who failed to show activation in ROIs, did not (sensitivity 100%, specificity 100%). Six of 9 patients in an MCS with activation in ROIs emerged from an MCS. Of 3 patients in an MCS who did not show activation, 2 patients stayed in an MCS and 1 patient emerged from the MCS (sensitivity 85%, specificity 40%).\n\nQuestion: Can mental imagery functional magnetic resonance imaging predict recovery in patients with disorders of consciousness?", "question_only": "Can mental imagery functional magnetic resonance imaging predict recovery in patients with disorders of consciousness?", "context": "To determine the potential prognostic value of using functional magnetic resonance imaging (fMRI) to identify patients with disorders of consciousness, who show potential for recovery. Observational study. Unit for acute rehabilitation care. Patients (N=22) in a vegetative state (VS; n=10) and minimally conscious state (MCS; n=12) during the first 200 days after the initial incident. Not applicable. Further course on the Coma Recovery Scale-Revised. Participants performed a mental imagery fMRI paradigm. They were asked to alternately imagine playing tennis and navigating through their home. In 14 of the 22 examined patients (VS, n=5; MCS, n=9), a significant activation of the regions of interest (ROIs) of the mental imagery paradigm could be found. All 5 patients with activation of a significant blood oxygen level dependent signal, who were in a VS at the time of the fMRI examination, reached at least an MCS at the end of the observation period. In contrast, 5 participants in a VS who failed to show activation in ROIs, did not (sensitivity 100%, specificity 100%). Six of 9 patients in an MCS with activation in ROIs emerged from an MCS. Of 3 patients in an MCS who did not show activation, 2 patients stayed in an MCS and 1 patient emerged from the MCS (sensitivity 85%, specificity 40%).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The fMRI paradigm mental imagery displays a high concordance with the further clinical course of patients in a VS. All 5 patients in a VS who showed significant activation of ROIs had a favorable further course until the end of the observation period. We therefore propose the term \"functional minimally conscious state\" for these patients. They may benefit from rehabilitation treatment. In cases where no significant activation was seen, the method has no prognostic value. Prediction of the clinical course of patients in an MCS by fMRI was considerably less accurate than in patients in a VS.", "meshes": ["Adolescent", "Adult", "Aged", "Consciousness", "Female", "Humans", "Magnetic Resonance Imaging", "Male", "Middle Aged", "Persistent Vegetative State", "Prognosis", "Recovery of Function", "Rehabilitation Centers", "Young Adult"], "year": "2013"}
{"id": "pubmedqa_27281318", "dataset": "pubmedqa", "question": "Context: This study aims to study femoral tunnel lengths drilled with a flexible reamer and the distance to important lateral structures obtained by flexing the knee at various angles and by drilling the guide pins arthroscopically to resemble clinical practice. The purpose of this cadaveric study was twofold: 1. to determine whether femoral tunnel lengths of greater than 20 mm can be created with a flexible reamer system at 90 ° of knee flexion and 2. to determine whether the lateral structures of the knee are safe with this technique. Ten fresh cadaveric knees were utilized. The intra-osseous length can be measured with a specially de - signed flexible guide pin. Flexible pins were inserted with the knee at 70°, 90°, and 120° of flexion. The intra-osseous length was measured with the measuring device. Each speci - men was dissected around the lateral aspect of the knee to identify the critical structures, the common peroneal nerve, and the LCL. The distance from the guide pins to the com - mon peroneal nerve and femoral attachment of the LCL were measured with a standard flexible paper ruler to the nearest millimeter. There is a trend for progressively increasing mean intra-osseous length associated with increased flexion of the knee. The mean intra-osseous length for 70° flexion was 25.2 mm (20 mm to 32 mm), which was statistically significant when compared to mean intra-osseous lengths of 32.1 mm (22 mm to 45 mm) and 38.0 mm (34 mm to 45 mm) in the 90° and 120° flexion groups, respectively (p<0.05). There were no significant differences among the groups with respect to distance to the LCL. There is a trend toward longer distances to the common peroneal nerve with increased flexion. There was a statistically significant dif - ference when comparing 120° versus 70° (p<0.05).\n\nQuestion: Can Flexible Instruments Create Adequate Femoral Tunnel Lengths at 90° of Knee Flexion in Anterior Cruciate Ligament Reconstruction?", "question_only": "Can Flexible Instruments Create Adequate Femoral Tunnel Lengths at 90° of Knee Flexion in Anterior Cruciate Ligament Reconstruction?", "context": "This study aims to study femoral tunnel lengths drilled with a flexible reamer and the distance to important lateral structures obtained by flexing the knee at various angles and by drilling the guide pins arthroscopically to resemble clinical practice. The purpose of this cadaveric study was twofold: 1. to determine whether femoral tunnel lengths of greater than 20 mm can be created with a flexible reamer system at 90 ° of knee flexion and 2. to determine whether the lateral structures of the knee are safe with this technique. Ten fresh cadaveric knees were utilized. The intra-osseous length can be measured with a specially de - signed flexible guide pin. Flexible pins were inserted with the knee at 70°, 90°, and 120° of flexion. The intra-osseous length was measured with the measuring device. Each speci - men was dissected around the lateral aspect of the knee to identify the critical structures, the common peroneal nerve, and the LCL. The distance from the guide pins to the com - mon peroneal nerve and femoral attachment of the LCL were measured with a standard flexible paper ruler to the nearest millimeter. There is a trend for progressively increasing mean intra-osseous length associated with increased flexion of the knee. The mean intra-osseous length for 70° flexion was 25.2 mm (20 mm to 32 mm), which was statistically significant when compared to mean intra-osseous lengths of 32.1 mm (22 mm to 45 mm) and 38.0 mm (34 mm to 45 mm) in the 90° and 120° flexion groups, respectively (p<0.05). There were no significant differences among the groups with respect to distance to the LCL. There is a trend toward longer distances to the common peroneal nerve with increased flexion. There was a statistically significant dif - ference when comparing 120° versus 70° (p<0.05).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "This study that shows that adequate femoral tunnel lengths can be safely created without knee hyperflex - ion using flexible instruments via an anteromedial portal.", "meshes": ["Aged", "Aged, 80 and over", "Anatomic Landmarks", "Anterior Cruciate Ligament", "Anterior Cruciate Ligament Reconstruction", "Biomechanical Phenomena", "Cadaver", "Equipment Design", "Femur", "Humans", "Knee Joint", "Middle Aged", "Pliability", "Range of Motion, Articular", "Surgical Instruments"], "year": "2016"}
{"id": "pubmedqa_26556589", "dataset": "pubmedqa", "question": "Context: Achilles tendon structure deteriorates 2-days after maximal loading in elite athletes. The load-response behaviour of tendons may be altered in type 1 diabetes mellitus (T1DM) as hyperglycaemia accelerates collagen cross-linking. This study compared Achilles tendon load-response in participants with T1DM and controls. Achilles tendon structure was quantified at day-0, day-2 and day-4 after a 10 km run. Ultrasound tissue characterisation (UTC) measures tendon structural integrity by classifying pixels as echo-type I, II, III or IV. Echo-type I has the most aligned collagen fibrils and IV has the least. Participants were 7 individuals with T1DM and 10 controls. All regularly ran distances greater than 5 km and VISA-A scores indicated good tendon function (T1DM = 94 ± 11, control = 94 ± 10). There were no diabetic complications and HbA1c was 8.7 ± 2.6 mmol/mol for T1DM and 5.3 ± 0.4 mmol/mol for control groups. Baseline tendon structure was similar in T1DM and control groups - UTC echo-types (I-IV) and anterior-posterior thickness were all p > 0.05. No response to load was seen in either T1DM or control group over the 4-days post exercise.\n\nQuestion: Does type 1 diabetes mellitus affect Achilles tendon response to a 10 km run?", "question_only": "Does type 1 diabetes mellitus affect Achilles tendon response to a 10 km run?", "context": "Achilles tendon structure deteriorates 2-days after maximal loading in elite athletes. The load-response behaviour of tendons may be altered in type 1 diabetes mellitus (T1DM) as hyperglycaemia accelerates collagen cross-linking. This study compared Achilles tendon load-response in participants with T1DM and controls. Achilles tendon structure was quantified at day-0, day-2 and day-4 after a 10 km run. Ultrasound tissue characterisation (UTC) measures tendon structural integrity by classifying pixels as echo-type I, II, III or IV. Echo-type I has the most aligned collagen fibrils and IV has the least. Participants were 7 individuals with T1DM and 10 controls. All regularly ran distances greater than 5 km and VISA-A scores indicated good tendon function (T1DM = 94 ± 11, control = 94 ± 10). There were no diabetic complications and HbA1c was 8.7 ± 2.6 mmol/mol for T1DM and 5.3 ± 0.4 mmol/mol for control groups. Baseline tendon structure was similar in T1DM and control groups - UTC echo-types (I-IV) and anterior-posterior thickness were all p > 0.05. No response to load was seen in either T1DM or control group over the 4-days post exercise.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Active individuals with T1DM do not have a heightened Achilles tendon response to load, which suggests no increased risk of tendon injury. We cannot extrapolate these findings to sedentary individuals with T1DM.", "meshes": ["Achilles Tendon", "Adult", "Case-Control Studies", "Diabetes Mellitus, Type 1", "Female", "Humans", "Male", "Middle Aged", "Running", "Weight-Bearing"], "year": "2015"}
{"id": "pubmedqa_17606778", "dataset": "pubmedqa", "question": "Context: Complex regional pain syndrome type I is treated symptomatically. A protective effect of vitamin C (ascorbic acid) has been reported previously. A dose-response study was designed to evaluate its effect in patients with wrist fractures. In a double-blind, prospective, multicenter trial, 416 patients with 427 wrist fractures were randomly allocated to treatment with placebo or treatment with 200, 500, or 1500 mg of vitamin C daily for fifty days. The effect of gender, age, fracture type, and cast-related complaints on the occurrence of complex regional pain syndrome was analyzed. Three hundred and seventeen patients with 328 fractures were randomized to receive vitamin C, and ninety-nine patients with ninety-nine fractures were randomized to receive a placebo. The prevalence of complex regional pain syndrome was 2.4% (eight of 328) in the vitamin C group and 10.1% (ten of ninety-nine) in the placebo group (p=0.002); all of the affected patients were elderly women. Analysis of the different doses of vitamin C showed that the prevalence of complex regional pain syndrome was 4.2% (four of ninety-six) in the 200-mg group (relative risk, 0.41; 95% confidence interval, 0.13 to 1.27), 1.8% (two of 114) in the 500-mg group (relative risk, 0.17; 95% confidence interval, 0.04 to 0.77), and 1.7% (two of 118) in the 1500-mg group (relative risk, 0.17; 95% confidence interval, 0.04 to 0.75). Early cast-related complaints predicted the development of complex regional pain syndrome (relative risk, 5.35; 95% confidence interval, 2.13 to 13.42).\n\nQuestion: Can vitamin C prevent complex regional pain syndrome in patients with wrist fractures?", "question_only": "Can vitamin C prevent complex regional pain syndrome in patients with wrist fractures?", "context": "Complex regional pain syndrome type I is treated symptomatically. A protective effect of vitamin C (ascorbic acid) has been reported previously. A dose-response study was designed to evaluate its effect in patients with wrist fractures. In a double-blind, prospective, multicenter trial, 416 patients with 427 wrist fractures were randomly allocated to treatment with placebo or treatment with 200, 500, or 1500 mg of vitamin C daily for fifty days. The effect of gender, age, fracture type, and cast-related complaints on the occurrence of complex regional pain syndrome was analyzed. Three hundred and seventeen patients with 328 fractures were randomized to receive vitamin C, and ninety-nine patients with ninety-nine fractures were randomized to receive a placebo. The prevalence of complex regional pain syndrome was 2.4% (eight of 328) in the vitamin C group and 10.1% (ten of ninety-nine) in the placebo group (p=0.002); all of the affected patients were elderly women. Analysis of the different doses of vitamin C showed that the prevalence of complex regional pain syndrome was 4.2% (four of ninety-six) in the 200-mg group (relative risk, 0.41; 95% confidence interval, 0.13 to 1.27), 1.8% (two of 114) in the 500-mg group (relative risk, 0.17; 95% confidence interval, 0.04 to 0.77), and 1.7% (two of 118) in the 1500-mg group (relative risk, 0.17; 95% confidence interval, 0.04 to 0.75). Early cast-related complaints predicted the development of complex regional pain syndrome (relative risk, 5.35; 95% confidence interval, 2.13 to 13.42).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Vitamin C reduces the prevalence of complex regional pain syndrome after wrist fractures. A daily dose of 500 mg for fifty days is recommended.", "meshes": ["Analysis of Variance", "Antioxidants", "Ascorbic Acid", "Chi-Square Distribution", "Dose-Response Relationship, Drug", "Double-Blind Method", "Female", "Fractures, Bone", "Humans", "Male", "Middle Aged", "Reflex Sympathetic Dystrophy", "Treatment Outcome", "Wrist Injuries"], "year": "2007"}
{"id": "pubmedqa_8200238", "dataset": "pubmedqa", "question": "Context: This prospective, randomized study was designed to evaluate whether or not early postoperative feeding (claimed as a unique benefit of laparoscopic surgery) is possible after laparotomy and colorectal resection. The trial was performed between July 1, 1992 and October 31, 1992 and included all 64 consecutive patients who underwent laparotomy with either a colonic or an ileal resection. In all cases the nasogastric tube was removed immediately after the operation. Group 1 consisted of 32 patients (age range, 15-81 years; mean, 52 years) who received a regular diet on the first postoperative morning. Group 2 consisted of 32 patients (age range, 15-87 years; mean, 52 years) who were fed in a traditional manner. Regular food was permitted after resolution of ileus as defined by resumption of bowel movements in the absence of abdominal distention, nausea, or vomiting. The rate of nasogastric tube reinsertion for distention with persistent vomiting was 18.7 percent (six patients) in Group 1 and 12.5 percent (four patients) in Group 2. Although vomiting was experienced more frequently by patients in Group 1 (44 percent vs. 25 percent, respectively), there was no difference between the two groups with regard to the duration of postoperative ileus (3.6 vs. 3.4 days, respectively). In the 26 patients from Group 1 who did not require nasogastric tube reinsertion, there was a trend toward shorter hospitalization (6.7 vs. 8.0 days, respectively).\n\nQuestion: Must early postoperative oral intake be limited to laparoscopy?", "question_only": "Must early postoperative oral intake be limited to laparoscopy?", "context": "This prospective, randomized study was designed to evaluate whether or not early postoperative feeding (claimed as a unique benefit of laparoscopic surgery) is possible after laparotomy and colorectal resection. The trial was performed between July 1, 1992 and October 31, 1992 and included all 64 consecutive patients who underwent laparotomy with either a colonic or an ileal resection. In all cases the nasogastric tube was removed immediately after the operation. Group 1 consisted of 32 patients (age range, 15-81 years; mean, 52 years) who received a regular diet on the first postoperative morning. Group 2 consisted of 32 patients (age range, 15-87 years; mean, 52 years) who were fed in a traditional manner. Regular food was permitted after resolution of ileus as defined by resumption of bowel movements in the absence of abdominal distention, nausea, or vomiting. The rate of nasogastric tube reinsertion for distention with persistent vomiting was 18.7 percent (six patients) in Group 1 and 12.5 percent (four patients) in Group 2. Although vomiting was experienced more frequently by patients in Group 1 (44 percent vs. 25 percent, respectively), there was no difference between the two groups with regard to the duration of postoperative ileus (3.6 vs. 3.4 days, respectively). In the 26 patients from Group 1 who did not require nasogastric tube reinsertion, there was a trend toward shorter hospitalization (6.7 vs. 8.0 days, respectively).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Early oral intake is possible after laparotomy and colorectal resection. Thus, the laparoscopic surgeon's claim of early tolerated oral intake may not be unique to laparoscopy.", "meshes": ["Adolescent", "Adult", "Aged", "Aged, 80 and over", "Eating", "Female", "Humans", "Intestinal Obstruction", "Intestines", "Intubation, Gastrointestinal", "Laparoscopy", "Length of Stay", "Male", "Middle Aged", "Postoperative Care", "Postoperative Complications", "Prospective Studies", "Vomiting"], "year": "1994"}
{"id": "pubmedqa_24019262", "dataset": "pubmedqa", "question": "Context: Epidemiological studies have suggested inverse relationships between blood pressure and prevalence of conditions such as migraine and headache. It is not yet clear whether similar relationships can be established for back pain in particular in prospective studies. Associations between blood pressure and chronic low back pain were explored in the cross-sectional HUNT 2 survey of a Norwegian county in 1995-1997, including 39,872 individuals who never used antihypertensive medication. A prospective study, comprising 17,209 initially back pain-free individuals and 5740 individuals reporting low back pain, was established by re-examinations in the HUNT 3 survey in 2006-2008. Associations were assessed by logistic regression with respect to systolic, diastolic and pulse pressure, with adjustment for education, work status, physical activity, smoking, body mass and lipid levels. In the cross-sectional study, all three blood pressure measures showed inverse relationships with prevalence of low back pain in both sexes. In the prospective study of disease-free women, baseline pulse pressure and systolic pressure were inversely associated with risk of low back pain [odds ratio (OR) 0.93 per 10 mm Hg increase in pulse pressure, 95% confidence interval (CI) 0.89-0.98, p = 0.007; OR 0.95 per 10 mm Hg increase in systolic pressure, 95% CI 0.92-0.99, p = 0.005]. Results among men were equivocal. No associations were indicated with the occurrence of pain in individuals with low back pain at baseline.\n\nQuestion: Does high blood pressure reduce the risk of chronic low back pain?", "question_only": "Does high blood pressure reduce the risk of chronic low back pain?", "context": "Epidemiological studies have suggested inverse relationships between blood pressure and prevalence of conditions such as migraine and headache. It is not yet clear whether similar relationships can be established for back pain in particular in prospective studies. Associations between blood pressure and chronic low back pain were explored in the cross-sectional HUNT 2 survey of a Norwegian county in 1995-1997, including 39,872 individuals who never used antihypertensive medication. A prospective study, comprising 17,209 initially back pain-free individuals and 5740 individuals reporting low back pain, was established by re-examinations in the HUNT 3 survey in 2006-2008. Associations were assessed by logistic regression with respect to systolic, diastolic and pulse pressure, with adjustment for education, work status, physical activity, smoking, body mass and lipid levels. In the cross-sectional study, all three blood pressure measures showed inverse relationships with prevalence of low back pain in both sexes. In the prospective study of disease-free women, baseline pulse pressure and systolic pressure were inversely associated with risk of low back pain [odds ratio (OR) 0.93 per 10 mm Hg increase in pulse pressure, 95% confidence interval (CI) 0.89-0.98, p = 0.007; OR 0.95 per 10 mm Hg increase in systolic pressure, 95% CI 0.92-0.99, p = 0.005]. Results among men were equivocal. No associations were indicated with the occurrence of pain in individuals with low back pain at baseline.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Results for low back pain are consistent with the theory of hypertension-associated hypalgesia, predicting diminished pain sensitivity with increasing blood pressure, possibly with modified reactions in people suffering from long-lasting pain.", "meshes": ["Adult", "Aged", "Angiotensin Amide", "Body Mass Index", "Cross-Sectional Studies", "Female", "Genetic Testing", "Humans", "Logistic Models", "Low Back Pain", "Male", "Middle Aged", "Prevalence", "Risk Factors"], "year": "2014"}
{"id": "pubmedqa_20297950", "dataset": "pubmedqa", "question": "Context: To investigate the effect of fenofibrate on sleep apnoea indices. Proof-of-concept study comprising a placebo run-in period (1 week, 5 weeks if fibrate washout was required) and a 4-week randomized, double-blind treatment period. Thirty-four subjects (mean age 55 years, body mass index 34 kg/m 2 , fasting triglycerides 3.5 mmol/L) with diagnosed sleep apnoea syndrome not treated with continuous positive airways pressure were enrolled and randomized to once daily treatment with fenofibrate (145 mg NanoCrystal(R) tablet) or placebo. Overnight polysomnography, computerized attention/vigilance tests and blood sampling for measurement of lipids, insulin, fasting plasma glucose and fibrinogen were performed at the end of each study period. NCT00816829. As this was an exploratory study, a range of sleep variables were evaluated. The apnoea/hypopnoea index (AHI) and percentage of time spent with arterial oxygen saturation (SpO(2))<90% were relevant as they have been evaluated in other clinical trials. Other variables included total apnoeas, hypopnoeas and oxygen desaturations, and non-cortical micro-awakenings related to respiratory events per hour. Fenofibrate treatment significantly reduced the percentage of time with SpO(2)<90% (from 9.0% to 3.5% vs. 10.0% to 11.5% with placebo, p = 0.007), although there was no significant change in the AHI (reduction vs. control 14% (95%CI -47 to 40%, p = 0.533). Treatment reduced obstructive apnoeas (by 44%, from 18.5 at baseline to 15.0 at end of treatment vs. 29.0 to 30.5 on placebo, p = 0.048), and non-cortical micro-awakenings per hour (from 23.5 to 18.0 vs. 24.0 to 25.0 with placebo, p = 0.004). Other sleep variables were not significantly influenced by fenofibrate. Exploratory study in patients with mild to moderate sleep apnoea, limited treatment duration; concomitant hypnotic treatment (35%); lack of correction for multiplicity of testing.\n\nQuestion: Proof of concept study: does fenofibrate have a role in sleep apnoea syndrome?", "question_only": "Proof of concept study: does fenofibrate have a role in sleep apnoea syndrome?", "context": "To investigate the effect of fenofibrate on sleep apnoea indices. Proof-of-concept study comprising a placebo run-in period (1 week, 5 weeks if fibrate washout was required) and a 4-week randomized, double-blind treatment period. Thirty-four subjects (mean age 55 years, body mass index 34 kg/m 2 , fasting triglycerides 3.5 mmol/L) with diagnosed sleep apnoea syndrome not treated with continuous positive airways pressure were enrolled and randomized to once daily treatment with fenofibrate (145 mg NanoCrystal(R) tablet) or placebo. Overnight polysomnography, computerized attention/vigilance tests and blood sampling for measurement of lipids, insulin, fasting plasma glucose and fibrinogen were performed at the end of each study period. NCT00816829. As this was an exploratory study, a range of sleep variables were evaluated. The apnoea/hypopnoea index (AHI) and percentage of time spent with arterial oxygen saturation (SpO(2))<90% were relevant as they have been evaluated in other clinical trials. Other variables included total apnoeas, hypopnoeas and oxygen desaturations, and non-cortical micro-awakenings related to respiratory events per hour. Fenofibrate treatment significantly reduced the percentage of time with SpO(2)<90% (from 9.0% to 3.5% vs. 10.0% to 11.5% with placebo, p = 0.007), although there was no significant change in the AHI (reduction vs. control 14% (95%CI -47 to 40%, p = 0.533). Treatment reduced obstructive apnoeas (by 44%, from 18.5 at baseline to 15.0 at end of treatment vs. 29.0 to 30.5 on placebo, p = 0.048), and non-cortical micro-awakenings per hour (from 23.5 to 18.0 vs. 24.0 to 25.0 with placebo, p = 0.004). Other sleep variables were not significantly influenced by fenofibrate. Exploratory study in patients with mild to moderate sleep apnoea, limited treatment duration; concomitant hypnotic treatment (35%); lack of correction for multiplicity of testing.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The consistent direction of change in sleep indices in this proof-of-concept study may support further investigation of fenofibrate in moderate to severe sleep apnoea syndrome.", "meshes": ["Attention", "Double-Blind Method", "Female", "Fenofibrate", "Humans", "Hypolipidemic Agents", "Lipids", "Male", "Middle Aged", "Placebos", "Sleep", "Sleep Apnea Syndromes"], "year": "2010"}
{"id": "pubmedqa_25735444", "dataset": "pubmedqa", "question": "Context: A multicentre, retrospective study was conducted of patients with rectal cancer threatening or affecting the prostatic plane, but not the bladder, judged by magnetic resonance imaging (MRI). The use of preoperative chemoradiotherapy and the type of urologic resection were correlated with the status of the pathological circumferential resection margin (CRM) and local recurrence. A consecutive series of 126 men with rectal cancer threatening (44) or affecting (82) the prostatic plane on preoperative staging and operated with local curative intent between 1998 and 2010 was analysed. In patients who did not have chemoradiotherapy but had a preoperative threatened anterior margin the CRM-positive rate was 25.0%. In patients who did not have preoperative chemoradiotherapy but did have an affected margin, the CRM-positive rate was 41.7%. When preoperative radiotherapy was given, the respective CRM infiltration rates were 7.1 and 20.7%. In patients having preoperative chemoradiotherapy followed by prostatic resection the rate of CRM positivity was 2.4%. Partial prostatectomy after preoperative chemoradiotherapy resulted in a free anterior CRM in all cases, but intra-operative urethral damage occurred in 36.4% of patients who underwent partial prostatectomy, resulting in a postoperative urinary fistula in 18.2% of patients.\n\nQuestion: Rectal cancer threatening or affecting the prostatic plane: is partial prostatectomy oncologically adequate?", "question_only": "Rectal cancer threatening or affecting the prostatic plane: is partial prostatectomy oncologically adequate?", "context": "A multicentre, retrospective study was conducted of patients with rectal cancer threatening or affecting the prostatic plane, but not the bladder, judged by magnetic resonance imaging (MRI). The use of preoperative chemoradiotherapy and the type of urologic resection were correlated with the status of the pathological circumferential resection margin (CRM) and local recurrence. A consecutive series of 126 men with rectal cancer threatening (44) or affecting (82) the prostatic plane on preoperative staging and operated with local curative intent between 1998 and 2010 was analysed. In patients who did not have chemoradiotherapy but had a preoperative threatened anterior margin the CRM-positive rate was 25.0%. In patients who did not have preoperative chemoradiotherapy but did have an affected margin, the CRM-positive rate was 41.7%. When preoperative radiotherapy was given, the respective CRM infiltration rates were 7.1 and 20.7%. In patients having preoperative chemoradiotherapy followed by prostatic resection the rate of CRM positivity was 2.4%. Partial prostatectomy after preoperative chemoradiotherapy resulted in a free anterior CRM in all cases, but intra-operative urethral damage occurred in 36.4% of patients who underwent partial prostatectomy, resulting in a postoperative urinary fistula in 18.2% of patients.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Preoperative chemoradiation is mandatory in male patients with a threatened or affected anterior circumferential margin on preoperative MRI. In patients with preoperative prostatic infiltration, prostatic resection is necessary. In this group of patients partial prostatectomy seems to be oncologically safe.", "meshes": ["Aged", "Chemoradiotherapy, Adjuvant", "Humans", "Magnetic Resonance Imaging", "Male", "Middle Aged", "Neoadjuvant Therapy", "Neoplasm Invasiveness", "Neoplasm Recurrence, Local", "Neoplasm, Residual", "Prostate", "Prostatectomy", "Radiotherapy, Adjuvant", "Rectal Neoplasms", "Retrospective Studies", "Urethra", "Urinary Fistula"], "year": "2015"}
{"id": "pubmedqa_1571683", "dataset": "pubmedqa", "question": "Context: To assess quality of storage of vaccines in the community. Questionnaire survey of general practices and child health clinics, and monitoring of storage temperatures of selected refrigerators. Central Manchester and Bradford health districts. 45 general practices and five child health clinics, of which 40 (80%) responded. Eight practices were selected for refrigeration monitoring. Adherence to Department of Health guidelines for vaccine storage, temperature range to which vaccines were exposed over two weeks. Of the 40 respondents, only 16 were aware of the appropriate storage conditions for the vaccines; eight had minimum and maximum thermometers but only one of these was monitored daily. In six of the eight practices selected for monitoring of refrigeration temperatures the vaccines were exposed to either subzero temperatures (three fridges) or temperatures up to 16 degrees C (three). Two of these were specialised drug storage refrigerators with an incorporated thermostat and external temperature gauges.\n\nQuestion: Storage of vaccines in the community: weak link in the cold chain?", "question_only": "Storage of vaccines in the community: weak link in the cold chain?", "context": "To assess quality of storage of vaccines in the community. Questionnaire survey of general practices and child health clinics, and monitoring of storage temperatures of selected refrigerators. Central Manchester and Bradford health districts. 45 general practices and five child health clinics, of which 40 (80%) responded. Eight practices were selected for refrigeration monitoring. Adherence to Department of Health guidelines for vaccine storage, temperature range to which vaccines were exposed over two weeks. Of the 40 respondents, only 16 were aware of the appropriate storage conditions for the vaccines; eight had minimum and maximum thermometers but only one of these was monitored daily. In six of the eight practices selected for monitoring of refrigeration temperatures the vaccines were exposed to either subzero temperatures (three fridges) or temperatures up to 16 degrees C (three). Two of these were specialised drug storage refrigerators with an incorporated thermostat and external temperature gauges.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Vaccines were exposed to temperatures that may reduce their potency. Safe storage of vaccines in the clinics cannot be ensured without adhering to the recommended guidelines. Provision of adequate equipment and training for staff in maintaining the \"cold chain\" and the use and care of equipment are important components of a successful immunisation programme.", "meshes": ["Child", "Child Health Services", "Drug Storage", "Family Practice", "Humans", "Refrigeration", "Time Factors", "Vaccines"], "year": "1992"}
{"id": "pubmedqa_26778755", "dataset": "pubmedqa", "question": "Context: Although dose-volume parameters in image-guided brachytherapy have become a standard, the use of posterior-inferior border of the pubic symphysis (PIBS) points has been recently proposed in the reporting of vaginal doses. The aim was to evaluate their pertinence. Nineteen patients who received image-guided brachytherapy after concurrent radiochemotherapy were included. Per treatment, CT scans were performed at Days 2 and 3, with reporting of the initial dwell positions and times. Doses delivered to the PIBS points were evaluated on each plan, considering that they were representative of one-third of the treatment. The movements of the applicator according to the PIBS point were analysed. Mean prescribed doses at PIBS -2, PIBS, PIBS +2 were, respectively, 2.23 ± 1.4, 6.39 ± 6.6, and 31.85 ± 36.06 Gy. Significant differences were observed between the 5 patients with vaginal involvement and the remaining 14 at the level of PIBS +2 and PIBS: +47.60 Gy and +7.46 Gy, respectively (p = 0.023 and 0.03). The variations between delivered and prescribed doses at PIBS points were not significant. However, at International commission on radiation units and measurements rectovaginal point, the delivered dose was decreased by 1.43 ± 2.49 Gy from the planned dose (p = 0.019). The delivered doses at the four points were strongly correlated with the prescribed doses with R(2) ranging from 0.93 to 0.95. The movements of the applicator in regard of the PIBS point assessed with the Digital Imaging and Communications in Medicine coordinates were insignificant.\n\nQuestion: Vaginal dose assessment in image-guided brachytherapy for cervical cancer: Can we really rely on dose-point evaluation?", "question_only": "Vaginal dose assessment in image-guided brachytherapy for cervical cancer: Can we really rely on dose-point evaluation?", "context": "Although dose-volume parameters in image-guided brachytherapy have become a standard, the use of posterior-inferior border of the pubic symphysis (PIBS) points has been recently proposed in the reporting of vaginal doses. The aim was to evaluate their pertinence. Nineteen patients who received image-guided brachytherapy after concurrent radiochemotherapy were included. Per treatment, CT scans were performed at Days 2 and 3, with reporting of the initial dwell positions and times. Doses delivered to the PIBS points were evaluated on each plan, considering that they were representative of one-third of the treatment. The movements of the applicator according to the PIBS point were analysed. Mean prescribed doses at PIBS -2, PIBS, PIBS +2 were, respectively, 2.23 ± 1.4, 6.39 ± 6.6, and 31.85 ± 36.06 Gy. Significant differences were observed between the 5 patients with vaginal involvement and the remaining 14 at the level of PIBS +2 and PIBS: +47.60 Gy and +7.46 Gy, respectively (p = 0.023 and 0.03). The variations between delivered and prescribed doses at PIBS points were not significant. However, at International commission on radiation units and measurements rectovaginal point, the delivered dose was decreased by 1.43 ± 2.49 Gy from the planned dose (p = 0.019). The delivered doses at the four points were strongly correlated with the prescribed doses with R(2) ranging from 0.93 to 0.95. The movements of the applicator in regard of the PIBS point assessed with the Digital Imaging and Communications in Medicine coordinates were insignificant.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "The doses evaluated at PIBS points are not impacted by intrafractional movements. PIBS and PIBS +2 dose points allow distinguishing the plans of patients with vaginal infiltration. Further studies are needed to correlate these parameters with vaginal morbidity.", "meshes": ["Brachytherapy", "Chemoradiotherapy", "Female", "Humans", "Pubic Symphysis", "Radiation Dosage", "Radiotherapy, Image-Guided", "Tomography, X-Ray Computed", "Uterine Cervical Neoplasms", "Vagina"], "year": null}
{"id": "pubmedqa_25371231", "dataset": "pubmedqa", "question": "Context: The aetiology of osteochondritis dissecans is still unclear. The aim of this prospective pilot study was to analyse whether vitamin D insufficiency, or deficiency, might be a contributing etiological factor in the development of an OCD lesion. The serum level of vitamin D3 in 23 consecutive patients (12 male and 11 female) suffering from a stage III, or stages III and IV, OCD lesion (mostly stage III) admitted for surgery was measured. The patients' mean age was 31.3 years and most of them already exhibited closed epiphyseal plates. In the majority of patients (18/23), a distinct vitamin D3 deficiency was found, two patients were vitamin D3-insufficient and, in three patients, the vitamin D3 level reached the lowest normal value.\n\nQuestion: Is vitamin D insufficiency or deficiency related to the development of osteochondritis dissecans?", "question_only": "Is vitamin D insufficiency or deficiency related to the development of osteochondritis dissecans?", "context": "The aetiology of osteochondritis dissecans is still unclear. The aim of this prospective pilot study was to analyse whether vitamin D insufficiency, or deficiency, might be a contributing etiological factor in the development of an OCD lesion. The serum level of vitamin D3 in 23 consecutive patients (12 male and 11 female) suffering from a stage III, or stages III and IV, OCD lesion (mostly stage III) admitted for surgery was measured. The patients' mean age was 31.3 years and most of them already exhibited closed epiphyseal plates. In the majority of patients (18/23), a distinct vitamin D3 deficiency was found, two patients were vitamin D3-insufficient and, in three patients, the vitamin D3 level reached the lowest normal value.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "These first data show that a vitamin D3 deficiency rather than an insufficiency may be involved in the development of OCD lesions. Probably, with a vitamin D3 substitution, the development of an advanced OCD stage could be avoided. Further analyses, including morphological analyses regarding a possible osteomalacia, and examination of the PTH and other determinants of the bone metabolism, should be undertaken to either confirm or refute these data.", "meshes": ["Adolescent", "Adult", "Aged", "Child", "Female", "Humans", "Male", "Middle Aged", "Osteochondritis Dissecans", "Pilot Projects", "Prospective Studies", "Vitamin D Deficiency", "Young Adult"], "year": "2016"}
{"id": "pubmedqa_27456836", "dataset": "pubmedqa", "question": "Context: To explore whether electrochemiluminescence (ECL) assays can help improve prediction of time to type 1 diabetes in the TrialNet autoantibody-positive population. TrialNet subjects who were positive for one or more autoantibodies (microinsulin autoantibody, GAD65 autoantibody [GADA], IA-2A, and ZnT8A) with available ECL-insulin autoantibody (IAA) and ECL-GADA data at their initial visit were analyzed; after a median follow-up of 24 months, 177 of these 1,287 subjects developed diabetes. Univariate analyses showed that autoantibodies by radioimmunoassays (RIAs), ECL-IAA, ECL-GADA, age, sex, number of positive autoantibodies, presence of HLA DR3/4-DQ8 genotype, HbA1c, and oral glucose tolerance test (OGTT) measurements were all significantly associated with progression to diabetes. Subjects who were ECL positive had a risk of progression to diabetes within 6 years of 58% compared with 5% for the ECL-negative subjects (P<0.0001). Multivariate Cox proportional hazards models were compared, with the base model including age, sex, OGTT measurements, and number of positive autoantibodies by RIAs. The model with positivity for ECL-GADA and/or ECL-IAA was the best, and factors that remained significantly associated with time to diabetes were area under the curve (AUC) C-peptide, fasting C-peptide, AUC glucose, number of positive autoantibodies by RIAs, and ECL positivity. Adding ECL to the Diabetes Prevention Trial risk score (DPTRS) improved the receiver operating characteristic curves with AUC of 0.83 (P<0.0001).\n\nQuestion: Do Electrochemiluminescence Assays Improve Prediction of Time to Type 1 Diabetes in Autoantibody-Positive TrialNet Subjects?", "question_only": "Do Electrochemiluminescence Assays Improve Prediction of Time to Type 1 Diabetes in Autoantibody-Positive TrialNet Subjects?", "context": "To explore whether electrochemiluminescence (ECL) assays can help improve prediction of time to type 1 diabetes in the TrialNet autoantibody-positive population. TrialNet subjects who were positive for one or more autoantibodies (microinsulin autoantibody, GAD65 autoantibody [GADA], IA-2A, and ZnT8A) with available ECL-insulin autoantibody (IAA) and ECL-GADA data at their initial visit were analyzed; after a median follow-up of 24 months, 177 of these 1,287 subjects developed diabetes. Univariate analyses showed that autoantibodies by radioimmunoassays (RIAs), ECL-IAA, ECL-GADA, age, sex, number of positive autoantibodies, presence of HLA DR3/4-DQ8 genotype, HbA1c, and oral glucose tolerance test (OGTT) measurements were all significantly associated with progression to diabetes. Subjects who were ECL positive had a risk of progression to diabetes within 6 years of 58% compared with 5% for the ECL-negative subjects (P<0.0001). Multivariate Cox proportional hazards models were compared, with the base model including age, sex, OGTT measurements, and number of positive autoantibodies by RIAs. The model with positivity for ECL-GADA and/or ECL-IAA was the best, and factors that remained significantly associated with time to diabetes were area under the curve (AUC) C-peptide, fasting C-peptide, AUC glucose, number of positive autoantibodies by RIAs, and ECL positivity. Adding ECL to the Diabetes Prevention Trial risk score (DPTRS) improved the receiver operating characteristic curves with AUC of 0.83 (P<0.0001).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "ECL assays improved the ability to predict time to diabetes in these autoantibody-positive relatives at risk for developing diabetes. These findings might be helpful in the design and eligibility criteria for prevention trials in the future.", "meshes": ["Adolescent", "Adult", "Autoantibodies", "Blood Glucose", "C-Peptide", "Child", "Diabetes Mellitus, Type 1", "Disease Progression", "Female", "Glycated Hemoglobin A", "Humans", "Insulin Antibodies", "Longitudinal Studies", "Luminescence", "Male", "Proportional Hazards Models", "Prospective Studies", "Risk Factors", "Time Factors", "Young Adult"], "year": "2016"}
{"id": "pubmedqa_23506394", "dataset": "pubmedqa", "question": "Context: Arterial calcification is a significant cardiovascular risk factor in hemodialysis patients. A series of factors are involved in the process of arterial calcification; however, the relationship between malnutrition and arterial calcification is still unclear. 68 hemodialysis patients were enrolled in this study. Nutrition status was evaluated using modified quantitative subjective global assessment (MQSGA). Related serum biochemical parameters were measured. And the radial artery samples were collected during the arteriovenous fistula surgeries. Hematoxylin/eosin stain was used to observe the arterial structures while Alizarin red stain to observe calcified depositions and classify calcified degree. The expressions of bone morphogenetic protein 2 (BMP2) and matrix Gla protein (MGP) were detected by immunohistochemistry and western blot methods. 66.18% hemodialysis patients were malnutrition. In hemodialysis patients, the calcified depositions were mainly located in the medial layer of the radial arteries and the expressions of BMP2 and MGP were both increased in the calcified areas. The levels of serum albumin were negatively associated with calcification score and the expressions of BMP2 and MGP. While MQSGA score, serum phosphorus and calcium × phosphorus product showed positive relationships with calcification score and the expressions of BMP2 and MGP.\n\nQuestion: Malnutrition, a new inducer for arterial calcification in hemodialysis patients?", "question_only": "Malnutrition, a new inducer for arterial calcification in hemodialysis patients?", "context": "Arterial calcification is a significant cardiovascular risk factor in hemodialysis patients. A series of factors are involved in the process of arterial calcification; however, the relationship between malnutrition and arterial calcification is still unclear. 68 hemodialysis patients were enrolled in this study. Nutrition status was evaluated using modified quantitative subjective global assessment (MQSGA). Related serum biochemical parameters were measured. And the radial artery samples were collected during the arteriovenous fistula surgeries. Hematoxylin/eosin stain was used to observe the arterial structures while Alizarin red stain to observe calcified depositions and classify calcified degree. The expressions of bone morphogenetic protein 2 (BMP2) and matrix Gla protein (MGP) were detected by immunohistochemistry and western blot methods. 66.18% hemodialysis patients were malnutrition. In hemodialysis patients, the calcified depositions were mainly located in the medial layer of the radial arteries and the expressions of BMP2 and MGP were both increased in the calcified areas. The levels of serum albumin were negatively associated with calcification score and the expressions of BMP2 and MGP. While MQSGA score, serum phosphorus and calcium × phosphorus product showed positive relationships with calcification score and the expressions of BMP2 and MGP.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Malnutrition is prevalent in hemodialysis patients and is associated with arterial calcification and the expressions of BMP2 and MGP in calcified radial arteries. Malnutrition may be a new inducer candidate for arterial calcification in hemodialysis patients.", "meshes": ["Arteries", "Blotting, Western", "Bone Morphogenetic Protein 2", "Calcinosis", "Calcium", "Calcium-Binding Proteins", "Extracellular Matrix Proteins", "Humans", "Immunohistochemistry", "Malnutrition", "Phosphorus", "Renal Dialysis", "Serum Albumin"], "year": "2013"}
{"id": "pubmedqa_19108857", "dataset": "pubmedqa", "question": "Context: Tuberculosis continues to be a public health problem in emerging countries with a recent evidence of increased incidence of extrapulmonary localization in developed countries probably linked to HIV. To our knowledge the occurrence of cerebro-mediastinal tuberculosis in an immuno-competent child has not been previously described; moreover the child we describe has a probable Say-Barber-Miller syndrome. We discuss a putative causative link between this syndrome and the occurrence of tuberculosis. A seven-year-old girl presented to our department with a history of infantile encephalopathy since birth characterized by a facial dysmorphy (evocative of a bird face), microcephaly, and mental retardation, and with recurrent infections. The child had complained of back pain for several months; the parents reported anorexia, loss of weight. Spinal and cerebral MRI showed a mediastinal mass involving the spine and cerebral lesions evocative of tuberculomas. The tuberculin interdermal reaction was positive. Culture of a vertebral biopsy was positive for Koch bacillus. Anti-tuberculosis treatment improved general and local status. An extensive immunological work-up was normal.\n\nQuestion: Cerebromediastinal tuberculosis in a child with a probable Say-Barber-Miller syndrome: a causative link?", "question_only": "Cerebromediastinal tuberculosis in a child with a probable Say-Barber-Miller syndrome: a causative link?", "context": "Tuberculosis continues to be a public health problem in emerging countries with a recent evidence of increased incidence of extrapulmonary localization in developed countries probably linked to HIV. To our knowledge the occurrence of cerebro-mediastinal tuberculosis in an immuno-competent child has not been previously described; moreover the child we describe has a probable Say-Barber-Miller syndrome. We discuss a putative causative link between this syndrome and the occurrence of tuberculosis. A seven-year-old girl presented to our department with a history of infantile encephalopathy since birth characterized by a facial dysmorphy (evocative of a bird face), microcephaly, and mental retardation, and with recurrent infections. The child had complained of back pain for several months; the parents reported anorexia, loss of weight. Spinal and cerebral MRI showed a mediastinal mass involving the spine and cerebral lesions evocative of tuberculomas. The tuberculin interdermal reaction was positive. Culture of a vertebral biopsy was positive for Koch bacillus. Anti-tuberculosis treatment improved general and local status. An extensive immunological work-up was normal.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "[corrected] This observation is exceptional in many aspects: very early age of onset of extrapulmonary tuberculosis, no immune deficit, association with a rare congenital neurological syndrome. We discuss the possible link between this entity and the occurrence of tuberculosis.", "meshes": ["Anorexia", "Body Dysmorphic Disorders", "Child", "Consanguinity", "Diagnosis, Differential", "Face", "Female", "Humans", "Intellectual Disability", "Male", "Pedigree", "Syndrome", "Tuberculoma"], "year": "2009"}
{"id": "pubmedqa_18096128", "dataset": "pubmedqa", "question": "Context: Studies have identified clinical predictors to guide radiologic evaluation of the cervical spine in geriatric patients. We hypothesized that clinical predictors are not adequate in the identification of cervical spine fractures in geriatric blunt trauma patients with low-energy mechanism. A retrospective case-control study was performed on geriatric blunt trauma patients sustaining low-energy trauma from January 2000 to January 2006. A data form including 8 clinical predictors was completed for each group. There were 35 study and 64 control patients identified. Both groups were similar in age (study 83.6 vs control 81.2) and injury severity score (study 9.06 vs control 9.61). Only neck tenderness exceeded the expected occurrence in the presence of a cervical spine injury (chi(2) = 18.1, P = .001) in just 45.5% of the study group.\n\nQuestion: Cervical spine fractures in geriatric blunt trauma patients with low-energy mechanism: are clinical predictors adequate?", "question_only": "Cervical spine fractures in geriatric blunt trauma patients with low-energy mechanism: are clinical predictors adequate?", "context": "Studies have identified clinical predictors to guide radiologic evaluation of the cervical spine in geriatric patients. We hypothesized that clinical predictors are not adequate in the identification of cervical spine fractures in geriatric blunt trauma patients with low-energy mechanism. A retrospective case-control study was performed on geriatric blunt trauma patients sustaining low-energy trauma from January 2000 to January 2006. A data form including 8 clinical predictors was completed for each group. There were 35 study and 64 control patients identified. Both groups were similar in age (study 83.6 vs control 81.2) and injury severity score (study 9.06 vs control 9.61). Only neck tenderness exceeded the expected occurrence in the presence of a cervical spine injury (chi(2) = 18.1, P = .001) in just 45.5% of the study group.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Clinical predictors appear inadequate for the evaluation of the cervical spine in geriatric trauma patients with low-energy mechanism.", "meshes": ["Accidental Falls", "Age Factors", "Aged", "Aged, 80 and over", "Case-Control Studies", "Cervical Vertebrae", "Female", "Geriatric Assessment", "Humans", "Incidence", "Injury Severity Score", "Male", "Physical Examination", "Predictive Value of Tests", "Probability", "Retrospective Studies", "Sensitivity and Specificity", "Spinal Fractures", "Tomography, X-Ray Computed", "Wounds, Nonpenetrating"], "year": "2008"}
{"id": "pubmedqa_28177278", "dataset": "pubmedqa", "question": "Context: Polyarteritis nodosa (PAN) is a systemic vasculitis involving mainly medium-sized arteries and, rarely, small-sized arteries. The diagnosis is principally based on clinical exams, biopsy of an affected organ, and/or arteriography of renal or mesenteric arteries. Once diagnosed, immunosuppressive agents, such as glucocorticoids and cyclophosphamide, are generally introduced as soon as possible. Whether spontaneous remission of PAN occurs is therefore largely unknown. We describe the case of a 51-year-old woman who presented with a 4-day-history of intense pain in her left flank, hypertension, fever, microscopic hematuria, and acute renal failure. Contrast-enhanced renal ultrasound strongly suggested bilateral renal infarction. Medical history and an extensive workup allowed to exclude systemic embolism, recreational drug abuse, cardiac arrhythmias, and thrombophilia. A possible diagnosis of PAN was considered; however, within 2 weeks of admission, spontaneous remission of her clinical and biological symptoms occurred without the use of any immunosuppressive treatment. Finally, 3 months later, renal arteriography confirmed the diagnosis of PAN. The patient remains free of symptoms 1 year after initial presentation.\n\nQuestion: Does spontaneous remission occur in polyarteritis nodosa?", "question_only": "Does spontaneous remission occur in polyarteritis nodosa?", "context": "Polyarteritis nodosa (PAN) is a systemic vasculitis involving mainly medium-sized arteries and, rarely, small-sized arteries. The diagnosis is principally based on clinical exams, biopsy of an affected organ, and/or arteriography of renal or mesenteric arteries. Once diagnosed, immunosuppressive agents, such as glucocorticoids and cyclophosphamide, are generally introduced as soon as possible. Whether spontaneous remission of PAN occurs is therefore largely unknown. We describe the case of a 51-year-old woman who presented with a 4-day-history of intense pain in her left flank, hypertension, fever, microscopic hematuria, and acute renal failure. Contrast-enhanced renal ultrasound strongly suggested bilateral renal infarction. Medical history and an extensive workup allowed to exclude systemic embolism, recreational drug abuse, cardiac arrhythmias, and thrombophilia. A possible diagnosis of PAN was considered; however, within 2 weeks of admission, spontaneous remission of her clinical and biological symptoms occurred without the use of any immunosuppressive treatment. Finally, 3 months later, renal arteriography confirmed the diagnosis of PAN. The patient remains free of symptoms 1 year after initial presentation.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "This case illustrates the importance of considering PAN in the differential diagnosis of renal infarction with inflammatory syndrome and shows that spontaneous remission of renal PAN can occur. .", "meshes": ["Cyclophosphamide", "Diagnosis, Differential", "Female", "Glucocorticoids", "Humans", "Immunosuppressive Agents", "Infarction", "Kidney", "Middle Aged", "Polyarteritis Nodosa", "Remission, Spontaneous"], "year": "2017"}
{"id": "pubmedqa_18575014", "dataset": "pubmedqa", "question": "Context: Nasal Polyposis (NP) is defined as a chronic inflammatory disease of sinonasal mucosa leading to diffuse formation of benign polyps. Although family histories are frequently suggested in medical literature, no specific study focused on this point has been reported. The purpose of this study is to determine whether a hereditary factor could be implied for NP in a family where several members were affected. We included 99 members of this family. All patients were assessed for conditions known to be associated with the development or presence of NP. Concerning NP, patients were screened with a validated questionnaire and selected patients had a medical examination by an Ear, Nose and Throat practitioner. Thirteen patients had a personal history of NP without asthma, aspirin intolerance, Churg Strauss syndrome, cystic fibrosis, Young's syndrome, bare lymphocyte syndrome, or primary ciliary dyskinesia. Within this family, 19.7% of those older than 17 years were affected by NP, as compared with the national French prevalence of 2.1%.\n\nQuestion: Nasal polyposis: is there an inheritance pattern?", "question_only": "Nasal polyposis: is there an inheritance pattern?", "context": "Nasal Polyposis (NP) is defined as a chronic inflammatory disease of sinonasal mucosa leading to diffuse formation of benign polyps. Although family histories are frequently suggested in medical literature, no specific study focused on this point has been reported. The purpose of this study is to determine whether a hereditary factor could be implied for NP in a family where several members were affected. We included 99 members of this family. All patients were assessed for conditions known to be associated with the development or presence of NP. Concerning NP, patients were screened with a validated questionnaire and selected patients had a medical examination by an Ear, Nose and Throat practitioner. Thirteen patients had a personal history of NP without asthma, aspirin intolerance, Churg Strauss syndrome, cystic fibrosis, Young's syndrome, bare lymphocyte syndrome, or primary ciliary dyskinesia. Within this family, 19.7% of those older than 17 years were affected by NP, as compared with the national French prevalence of 2.1%.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Regarding the pedigree, we discuss different modes of inheritance. The presence of consanguineous unions in this family suggests the possibility of a common ancestor and thus a recessive autosomal mode of inheritance.", "meshes": ["Adolescent", "Adult", "Child", "Consanguinity", "Female", "France", "Genetic Linkage", "Humans", "Inheritance Patterns", "Male", "Middle Aged", "Nasal Polyps", "Paranasal Sinus Diseases", "Pedigree"], "year": "2008"}
{"id": "pubmedqa_25787073", "dataset": "pubmedqa", "question": "Context: The serum C-reactive protein (CRP) level correlates with the clinical prognosis in patients with kidney, penile and metastatic castration-resistant prostate cancer (PC). We prospectively evaluated the preoperative CRP level as a predictive marker for an advanced tumor stage or high-grade cancer in patients with clinically localized PC. The study evaluated 629 patients with clinically localized PC who underwent radical prostatectomy between 2010 and 2013. Exclusion criteria were signs of systemic infection, symptoms of an autoimmune disease or neoadjuvant androgen deprivation. Poorly differentiated PC tends to be more common in patients with elevated CRP levels (15.5 vs. 9.5%, p = 0.08). Analogously, patients with a Gleason score ≥8 PC had significantly higher median CRP levels than those with a Gleason score ≤7 PC (1.9 vs. 1.2 mg/l, p = 0.03). However, neither uni- nor multivariate analysis showed an association between the preoperative CRP level and the presence of a locally advanced tumor stage, lymph node metastases or a positive surgical margin. CRP also failed to correlate with the initial PSA level and the clinical tumor-associated findings. Moreover, multivariate analysis relativized the association between an elevated CRP level and poor tumor differentiation.\n\nQuestion: Do preoperative serum C-reactive protein levels predict the definitive pathological stage in patients with clinically localized prostate cancer?", "question_only": "Do preoperative serum C-reactive protein levels predict the definitive pathological stage in patients with clinically localized prostate cancer?", "context": "The serum C-reactive protein (CRP) level correlates with the clinical prognosis in patients with kidney, penile and metastatic castration-resistant prostate cancer (PC). We prospectively evaluated the preoperative CRP level as a predictive marker for an advanced tumor stage or high-grade cancer in patients with clinically localized PC. The study evaluated 629 patients with clinically localized PC who underwent radical prostatectomy between 2010 and 2013. Exclusion criteria were signs of systemic infection, symptoms of an autoimmune disease or neoadjuvant androgen deprivation. Poorly differentiated PC tends to be more common in patients with elevated CRP levels (15.5 vs. 9.5%, p = 0.08). Analogously, patients with a Gleason score ≥8 PC had significantly higher median CRP levels than those with a Gleason score ≤7 PC (1.9 vs. 1.2 mg/l, p = 0.03). However, neither uni- nor multivariate analysis showed an association between the preoperative CRP level and the presence of a locally advanced tumor stage, lymph node metastases or a positive surgical margin. CRP also failed to correlate with the initial PSA level and the clinical tumor-associated findings. Moreover, multivariate analysis relativized the association between an elevated CRP level and poor tumor differentiation.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "In patients with clinically localized PC, CRP does not appear to possess the predictive value and it was shown to have patients with other tumor entities or advanced PC.", "meshes": ["Adenocarcinoma", "Adult", "Aged", "Aged, 80 and over", "C-Reactive Protein", "Digital Rectal Examination", "Humans", "Lymphatic Metastasis", "Male", "Middle Aged", "Neoplasm Grading", "Neoplasm Staging", "Neoplasm, Residual", "Predictive Value of Tests", "Preoperative Period", "Prospective Studies", "Prostate-Specific Antigen", "Prostatectomy", "Prostatic Neoplasms"], "year": "2015"}
{"id": "pubmedqa_21214884", "dataset": "pubmedqa", "question": "Context: Using polymerase chain reaction techniques, we evaluated the presence of HPV infection in human breast milk collected from 21 HPV-positive and 11 HPV-negative mothers. Of the 32 studied human milk specimens, no 'high-risk' HPV 16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58 or 58 DNA was detected.\n\nQuestion: Can 'high-risk' human papillomaviruses (HPVs) be detected in human breast milk?", "question_only": "Can 'high-risk' human papillomaviruses (HPVs) be detected in human breast milk?", "context": "Using polymerase chain reaction techniques, we evaluated the presence of HPV infection in human breast milk collected from 21 HPV-positive and 11 HPV-negative mothers. Of the 32 studied human milk specimens, no 'high-risk' HPV 16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58 or 58 DNA was detected.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "This preliminary case-control study indicates the absence of mucosal 'high-risk' HPV types in human breast milk.", "meshes": ["Adult", "Alphapapillomavirus", "Case-Control Studies", "DNA, Viral", "Female", "Humans", "Infant", "Infectious Disease Transmission, Vertical", "Milk, Human", "Papillomavirus Infections", "Polymerase Chain Reaction", "Risk Assessment", "Young Adult"], "year": "2011"}
{"id": "pubmedqa_16361634", "dataset": "pubmedqa", "question": "Context: Lynch syndrome (hereditary nonpolyposis colorectal cancer; HNPCC) is an autosomal-dominant cancer predisposition syndrome that increases risk for multiple cancers, including colon, endometrial, and ovarian cancer. Revised Bethesda Criteria recommend that patients with two HNPCC-associated cancers undergo molecular evaluation to determine whether they have a mismatch repair (MMR) defect associated with HNPCC. The purpose of our study was to determine the likelihood of MMR defects (MSH2, MSH6, MLH1) in women with synchronous endometrial and ovarian cancer. Between 1989 and 2004, 102 women with synchronous endometrial and ovarian cancers were identified; 59 patients had tumor blocks available for analysis. Patients were divided into risk groups based on family history: high (met Amsterdam criteria), medium (personal history or first-degree relative with an HNPCC-associated cancer), and low (all others). Protein expression for MSH2, MSH6, and MLH1 was evaluated by immunohistochemistry. Microsatellite instability and MLH1 promoter methylation analyses were performed on a subset of cases. Median age was 50 years. Two patients met Amsterdam criteria for HNPCC. Five additional patients, all medium-risk, had molecular findings consistent with a germline mutation of either MSH2 or MLH1. None of the low-risk patients had molecular results consistent with a germline mutation.\n\nQuestion: Women with synchronous primary cancers of the endometrium and ovary: do they have Lynch syndrome?", "question_only": "Women with synchronous primary cancers of the endometrium and ovary: do they have Lynch syndrome?", "context": "Lynch syndrome (hereditary nonpolyposis colorectal cancer; HNPCC) is an autosomal-dominant cancer predisposition syndrome that increases risk for multiple cancers, including colon, endometrial, and ovarian cancer. Revised Bethesda Criteria recommend that patients with two HNPCC-associated cancers undergo molecular evaluation to determine whether they have a mismatch repair (MMR) defect associated with HNPCC. The purpose of our study was to determine the likelihood of MMR defects (MSH2, MSH6, MLH1) in women with synchronous endometrial and ovarian cancer. Between 1989 and 2004, 102 women with synchronous endometrial and ovarian cancers were identified; 59 patients had tumor blocks available for analysis. Patients were divided into risk groups based on family history: high (met Amsterdam criteria), medium (personal history or first-degree relative with an HNPCC-associated cancer), and low (all others). Protein expression for MSH2, MSH6, and MLH1 was evaluated by immunohistochemistry. Microsatellite instability and MLH1 promoter methylation analyses were performed on a subset of cases. Median age was 50 years. Two patients met Amsterdam criteria for HNPCC. Five additional patients, all medium-risk, had molecular findings consistent with a germline mutation of either MSH2 or MLH1. None of the low-risk patients had molecular results consistent with a germline mutation.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Overall, 7% of women in our cohort met either clinical or molecular criteria for Lynch syndrome. All of these women had a prior history or a first-degree relative with an HNPCC-associated cancer. Limiting genetic evaluation to women with synchronous endometrial and ovarian cancer who have a family history suggestive of HNPCC may appropriately identify women with Lynch syndrome.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Base Pair Mismatch", "Cohort Studies", "Colorectal Neoplasms, Hereditary Nonpolyposis", "DNA Methylation", "DNA Repair", "Endometrial Neoplasms", "Female", "Gene Expression Profiling", "Humans", "Immunohistochemistry", "Microsatellite Repeats", "Middle Aged", "Neoplasms, Multiple Primary", "Ovarian Neoplasms", "Pedigree", "Risk Factors"], "year": "2005"}
{"id": "pubmedqa_23449952", "dataset": "pubmedqa", "question": "Context: To investigate the diagnostic value of a half dose compared with a full dose of gadobenate dimeglumine in the assessment of synovitis or tenosynovitis in the wrist and finger joints in patients with early rheumatoid arthritis (RA) and a disease activity score greater than 3.2. With institutional review board approval and informed consent, 57 patients with early RA underwent 3-T magnetic resonance (MR) imaging with two different doses of contrast media. The contrast enhancement was measured in inflamed synovial tissue at half dose (0.05 mmol per kilogram of body weight) and at full dose (0.1 mmol/kg) by using T1-weighted sequences with fat saturation. The differences and the correlation of signal intensities (SIs) at half- and full-dose sequences were compared by using the paired t test and Pearson correlations. Image quality, Rheumatoid Arthritis MRI Score (RAMRIS), and tenosynovitis score on half- and full-dose images were compared by two observers using the Wilcoxon test. Interrater agreement was assessed by using κ statistics. A significant difference in SI was found between half-dose and full-dose gadobenate dimeglumine-enhanced synovial tissue (mean: 914.35 ± 251.1 vs 1022 ± 244.5, P<.001). Because the SI showed high correlation between the ratio at half dose and full dose (r = 0.875), the formula, ratio of synovial enhancement to saline syringe at full dose = 0.337 + 1.070 × ratio of synovial enhancement to saline syringe at half dose, can be used to convert the normalized value of half dose to full dose. However, no difference in RAMRIS (score 0 in 490 of 1026 joints; score 1 in 344; score 2 in 158; and score 3 in 34) or tenosynovitis scores in grading synovitis or tenosynovitis in image quality and in assessment of synovial enhancement was detected between half-dose and full-dose images (P = 1).\n\nQuestion: Contrast-enhanced MR imaging of hand and finger joints in patients with early rheumatoid arthritis: do we really need a full dose of gadobenate dimeglumine for assessing synovial enhancement at 3 T?", "question_only": "Contrast-enhanced MR imaging of hand and finger joints in patients with early rheumatoid arthritis: do we really need a full dose of gadobenate dimeglumine for assessing synovial enhancement at 3 T?", "context": "To investigate the diagnostic value of a half dose compared with a full dose of gadobenate dimeglumine in the assessment of synovitis or tenosynovitis in the wrist and finger joints in patients with early rheumatoid arthritis (RA) and a disease activity score greater than 3.2. With institutional review board approval and informed consent, 57 patients with early RA underwent 3-T magnetic resonance (MR) imaging with two different doses of contrast media. The contrast enhancement was measured in inflamed synovial tissue at half dose (0.05 mmol per kilogram of body weight) and at full dose (0.1 mmol/kg) by using T1-weighted sequences with fat saturation. The differences and the correlation of signal intensities (SIs) at half- and full-dose sequences were compared by using the paired t test and Pearson correlations. Image quality, Rheumatoid Arthritis MRI Score (RAMRIS), and tenosynovitis score on half- and full-dose images were compared by two observers using the Wilcoxon test. Interrater agreement was assessed by using κ statistics. A significant difference in SI was found between half-dose and full-dose gadobenate dimeglumine-enhanced synovial tissue (mean: 914.35 ± 251.1 vs 1022 ± 244.5, P<.001). Because the SI showed high correlation between the ratio at half dose and full dose (r = 0.875), the formula, ratio of synovial enhancement to saline syringe at full dose = 0.337 + 1.070 × ratio of synovial enhancement to saline syringe at half dose, can be used to convert the normalized value of half dose to full dose. However, no difference in RAMRIS (score 0 in 490 of 1026 joints; score 1 in 344; score 2 in 158; and score 3 in 34) or tenosynovitis scores in grading synovitis or tenosynovitis in image quality and in assessment of synovial enhancement was detected between half-dose and full-dose images (P = 1).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Postcontrast synovial SIs showed high correlation between half dose and full dose, and image quality was rated identically. Therefore, half-dose gadobenate dimeglumine at 3-T MR imaging may be sufficient for assessing synovitis or tenosynovitis in early RA.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Arthritis, Rheumatoid", "Contrast Media", "Female", "Finger Joint", "Hand", "Humans", "Image Interpretation, Computer-Assisted", "Linear Models", "Magnetic Resonance Imaging", "Male", "Meglumine", "Middle Aged", "Organometallic Compounds", "Prospective Studies", "Synovitis", "Tenosynovitis", "Wrist Joint"], "year": "2013"}
{"id": "pubmedqa_22813804", "dataset": "pubmedqa", "question": "Context: Childhood obesity is pandemic condition. The effect of obesity on trauma outcomes in children has been relatively understudied. We conducted this study to ascertain the effects of obesity on the hospital outcome of injured children. A retrospective cohort study of patients aged 2 to 18 years admitted to the King Abdul Aziz Medical City between May 2001 and May 2009 was conducted. Patients were categorized as lean (body mass index<95th percentile) and obese (body mass index ≥ 95th percentile). Groups were compared regarding admission demographics, mechanism of injury, pattern of injury, length of stay, intensive care unit admission, ventilation duration, types of procedures performed, injury severity score, and mortality. Nine hundred thirty-three patients were included, of those 55 (5.89%) children were obese. The obese children were older than nonobese (P = .001) and had a higher injury severity score (P = .001) and a lower pediatric trauma score (P = .00), heart rate (P = .0081), and respiratory rate (P = .000). There were no differences between groups with regard to sex, mechanism of injury, and surgical procedures. Obese children were more likely to have rib fractures (P = .02) and pelvic injuries (P = .033). There was no significant association between mortality and obesity (P = .42).\n\nQuestion: Does obesity impact the pattern and outcome of trauma in children?", "question_only": "Does obesity impact the pattern and outcome of trauma in children?", "context": "Childhood obesity is pandemic condition. The effect of obesity on trauma outcomes in children has been relatively understudied. We conducted this study to ascertain the effects of obesity on the hospital outcome of injured children. A retrospective cohort study of patients aged 2 to 18 years admitted to the King Abdul Aziz Medical City between May 2001 and May 2009 was conducted. Patients were categorized as lean (body mass index<95th percentile) and obese (body mass index ≥ 95th percentile). Groups were compared regarding admission demographics, mechanism of injury, pattern of injury, length of stay, intensive care unit admission, ventilation duration, types of procedures performed, injury severity score, and mortality. Nine hundred thirty-three patients were included, of those 55 (5.89%) children were obese. The obese children were older than nonobese (P = .001) and had a higher injury severity score (P = .001) and a lower pediatric trauma score (P = .00), heart rate (P = .0081), and respiratory rate (P = .000). There were no differences between groups with regard to sex, mechanism of injury, and surgical procedures. Obese children were more likely to have rib fractures (P = .02) and pelvic injuries (P = .033). There was no significant association between mortality and obesity (P = .42).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Obesity does not seem to impact the severity of injury, mortality rate, types of injury, and procedure outcomes in children. Obese patients are more likely to have rib and pelvic injuries.", "meshes": ["Adolescent", "Child", "Child, Preschool", "Cohort Studies", "Female", "Hospitalization", "Humans", "Injury Severity Score", "Male", "Obesity", "Prognosis", "Proportional Hazards Models", "Registries", "Retrospective Studies", "Saudi Arabia", "Wounds and Injuries"], "year": "2012"}
{"id": "pubmedqa_14599616", "dataset": "pubmedqa", "question": "Context: Lymphedema may be identified by simpler circumference changes as compared with changes in limb volume. Ninety breast cancer patients were prospectively enrolled in an academic trial, and seven upper extremity circumferences were measured quarterly for 3 years. A 10% volume increase or greater than 1 cm increase in arm circumference identified lymphedema with verification by a lymphedema specialist. Sensitivity and specificity of several different criteria for detecting lymphedema were compared using the academic trial as the standard. Thirty-nine cases of lymphedema were identified by the academic trial. Using a 10% increase in circumference at two sites as the criterion, half the lymphedema cases were detected (sensitivity 37%). When using a 10% increase in circumference at any site, 74.4% of cases were detected (sensitivity 49%). Detection by a 5% increase in circumference at any site was 91% sensitive.\n\nQuestion: Can a practicing surgeon detect early lymphedema reliably?", "question_only": "Can a practicing surgeon detect early lymphedema reliably?", "context": "Lymphedema may be identified by simpler circumference changes as compared with changes in limb volume. Ninety breast cancer patients were prospectively enrolled in an academic trial, and seven upper extremity circumferences were measured quarterly for 3 years. A 10% volume increase or greater than 1 cm increase in arm circumference identified lymphedema with verification by a lymphedema specialist. Sensitivity and specificity of several different criteria for detecting lymphedema were compared using the academic trial as the standard. Thirty-nine cases of lymphedema were identified by the academic trial. Using a 10% increase in circumference at two sites as the criterion, half the lymphedema cases were detected (sensitivity 37%). When using a 10% increase in circumference at any site, 74.4% of cases were detected (sensitivity 49%). Detection by a 5% increase in circumference at any site was 91% sensitive.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "An increase of 5% in circumference measurements identified the most potential lymphedema cases compared with an academic trial.", "meshes": ["Arm", "Breast Neoplasms", "Early Diagnosis", "Female", "Follow-Up Studies", "Humans", "Lymphedema", "Middle Aged", "Postoperative Complications", "Prospective Studies", "Radiotherapy, Adjuvant", "Sensitivity and Specificity", "Time Factors"], "year": "2003"}
{"id": "pubmedqa_28359277", "dataset": "pubmedqa", "question": "Context: Governments are urged to determine methods to control the use of medical resources and curb the rise of healthcare costs. The question is, do health behaviors have an impact on the use of medical resources? This study aims to identify and understand the difference in the number of outpatient visits and health examinations based on various health behaviors and to determine whether patients seek medical care for illness from the same physicians. This study used the dataset derived from the Department of Budget, Accounting and Statistics of Kaohsiung, Taiwan in 2005. Persons older than 15 years were surveyed using an on-site questionnaire. A total of 2911 persons were enrolled in this study. Independent t-tests, chi-square tests, one-way ANOVA, multiple linear regression and binominal logistic regression were used in the data analysis. The regression model for the frequency of doctor visits, health examinations, and whether the same physician is sought for medical care has demonstrated significant correlations with gender, age and education-level variables. Four health behaviors (i.e., exercise habits, dietary habits, regular blood pressure measurement, drinking habits) exhibited a significant correlation with healthcare utilization (P<0.05).\n\nQuestion: Do healthier lifestyles lead to less utilization of healthcare resources?", "question_only": "Do healthier lifestyles lead to less utilization of healthcare resources?", "context": "Governments are urged to determine methods to control the use of medical resources and curb the rise of healthcare costs. The question is, do health behaviors have an impact on the use of medical resources? This study aims to identify and understand the difference in the number of outpatient visits and health examinations based on various health behaviors and to determine whether patients seek medical care for illness from the same physicians. This study used the dataset derived from the Department of Budget, Accounting and Statistics of Kaohsiung, Taiwan in 2005. Persons older than 15 years were surveyed using an on-site questionnaire. A total of 2911 persons were enrolled in this study. Independent t-tests, chi-square tests, one-way ANOVA, multiple linear regression and binominal logistic regression were used in the data analysis. The regression model for the frequency of doctor visits, health examinations, and whether the same physician is sought for medical care has demonstrated significant correlations with gender, age and education-level variables. Four health behaviors (i.e., exercise habits, dietary habits, regular blood pressure measurement, drinking habits) exhibited a significant correlation with healthcare utilization (P<0.05).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Healthy lifestyles lead to an increase in the utilization of preventive health services. However, there is not much significantly reducing the number of outpatient visits in people with health behaviors. Specifically, people with regular exercise habits and who take their blood pressure measurement regularly have an increased number of outpatient visits. It is suggested that more available and accessible health consultation services be provided to inculcate in the general public the importance of maintaining a healthy lifestyle.", "meshes": ["Adult", "Aged", "Alcohol Drinking", "Exercise", "Female", "Health Behavior", "Health Care Costs", "Health Services Accessibility", "Healthy Lifestyle", "Humans", "Logistic Models", "Male", "Middle Aged", "Patient Acceptance of Health Care", "Preventive Health Services", "Surveys and Questionnaires", "Taiwan"], "year": "2017"}
{"id": "pubmedqa_9483814", "dataset": "pubmedqa", "question": "Context: Uterus-specific synthetic Prostaglandin analogues (gemeprost, sulproston etc.) have been widely employed for termination of pregnancy in the second trimester. Since paracervical anaesthesia may be useful during this procedure, we investigated in this prospective randomised study its impact on the clinical course of abortion and pain especially in the late first and second stage of labour. 20 women scheduled for elective abortion (fetal reasons) between the 16th and 23rd week of gestation were to be given 1 mg gemeprost vaginally every 6 hours. They were allocated at random: 10 women received only Pethidin intravenously and Butylscopolamine rectally, another 10 women were additionally treated by paracervical anaesthesia (2 x 10 ml 0.5% Bupivacain solution) at a cervical dilatation of 2-3 cm. A median of 3 gemeprost applications were administered in both groups. In the group without paracervical anaesthesia the median induction to abortion interval was 20 hours (range: 8-44 hours), 13 hours (range: 8-36 hours, NS) resulting for the paracervical anaesthesia group. The intervals from the last application of prostaglandin until abortion and from 3 cm cervical dilatation to abortion were slightly, but not significantly shorter in the paracervical anaesthesia group. The requirement of Butylscopolamine was higher in the latter group (p<0.05). The requirement of Pethidin and the intensity of pain (measured by pain scale according to Huskisson) especially in the late first stage of labour were not statistically different between both groups. Side effects of paracervical anaesthesia did not occur.\n\nQuestion: Does para-cervical block offer additional advantages in abortion induction with gemeprost in the 2nd trimester?", "question_only": "Does para-cervical block offer additional advantages in abortion induction with gemeprost in the 2nd trimester?", "context": "Uterus-specific synthetic Prostaglandin analogues (gemeprost, sulproston etc.) have been widely employed for termination of pregnancy in the second trimester. Since paracervical anaesthesia may be useful during this procedure, we investigated in this prospective randomised study its impact on the clinical course of abortion and pain especially in the late first and second stage of labour. 20 women scheduled for elective abortion (fetal reasons) between the 16th and 23rd week of gestation were to be given 1 mg gemeprost vaginally every 6 hours. They were allocated at random: 10 women received only Pethidin intravenously and Butylscopolamine rectally, another 10 women were additionally treated by paracervical anaesthesia (2 x 10 ml 0.5% Bupivacain solution) at a cervical dilatation of 2-3 cm. A median of 3 gemeprost applications were administered in both groups. In the group without paracervical anaesthesia the median induction to abortion interval was 20 hours (range: 8-44 hours), 13 hours (range: 8-36 hours, NS) resulting for the paracervical anaesthesia group. The intervals from the last application of prostaglandin until abortion and from 3 cm cervical dilatation to abortion were slightly, but not significantly shorter in the paracervical anaesthesia group. The requirement of Butylscopolamine was higher in the latter group (p<0.05). The requirement of Pethidin and the intensity of pain (measured by pain scale according to Huskisson) especially in the late first stage of labour were not statistically different between both groups. Side effects of paracervical anaesthesia did not occur.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Paracervical anaesthesia is a method for analgesia during second trimester abortion with a low rate of side effects. It can shorten the duration of last period of second trimester abortion in some cases but has no impact on the perception of pain nor requirement of analgesics and so with only limited benefit in second trimester abortion with vaginal gemeprost.", "meshes": ["Abortifacient Agents, Nonsteroidal", "Abortion, Eugenic", "Adolescent", "Adult", "Alprostadil", "Anesthesia, Local", "Anesthesia, Obstetrical", "Bupivacaine", "Cervix Uteri", "Female", "Humans", "Pain Measurement", "Pregnancy", "Pregnancy Trimester, Second", "Prospective Studies"], "year": "1997"}
{"id": "pubmedqa_22668712", "dataset": "pubmedqa", "question": "Context: The aim of this study was to assess the diagnostic value of articular sounds, standardized clinical examination, and standardized articular ultrasound in the detection of internal derangements of the temporomandibular joint. Forty patients and 20 asymptomatic volunteers underwent a standardized interview, physical examination, and static and dynamic articular ultrasound. Sensitivity, specificity, and predictive values were calculated using magnetic resonance as the reference test. A total of 120 temporomandibular joints were examined. Based on our findings, the presence of articular sounds and physical signs are often insufficient to detect disk displacement. Imaging by static and dynamic high-resolution ultrasound demonstrates considerably lower sensitivity when compared with magnetic resonance. Some of the technical difficulties resulted from a limited access because of the presence of surrounding bone structures.\n\nQuestion: Internal derangement of the temporomandibular joint: is there still a place for ultrasound?", "question_only": "Internal derangement of the temporomandibular joint: is there still a place for ultrasound?", "context": "The aim of this study was to assess the diagnostic value of articular sounds, standardized clinical examination, and standardized articular ultrasound in the detection of internal derangements of the temporomandibular joint. Forty patients and 20 asymptomatic volunteers underwent a standardized interview, physical examination, and static and dynamic articular ultrasound. Sensitivity, specificity, and predictive values were calculated using magnetic resonance as the reference test. A total of 120 temporomandibular joints were examined. Based on our findings, the presence of articular sounds and physical signs are often insufficient to detect disk displacement. Imaging by static and dynamic high-resolution ultrasound demonstrates considerably lower sensitivity when compared with magnetic resonance. Some of the technical difficulties resulted from a limited access because of the presence of surrounding bone structures.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The present study does not support the recommendation of ultrasound as a conclusive diagnostic tool for internal derangements of the temporomandibular joint.", "meshes": ["Adult", "Case-Control Studies", "Female", "Humans", "Joint Dislocations", "Magnetic Resonance Imaging", "Male", "Middle Aged", "Prospective Studies", "Reference Standards", "Sensitivity and Specificity", "Statistics, Nonparametric", "Surveys and Questionnaires", "Temporomandibular Joint Disc", "Temporomandibular Joint Disorders", "Ultrasonography", "Young Adult"], "year": "2012"}
{"id": "pubmedqa_26879871", "dataset": "pubmedqa", "question": "Context: Studies have linked ethnic differences in depression rates with neighbourhood ethnic density although results have not been conclusive. We looked at this using a novel approach analysing whole population data covering just over one million GP patients in four London boroughs. Using a dataset of GP records for all patients registered in Lambeth, Hackney, Tower Hamlets and Newham in 2013 we investigated new diagnoses of depression and antidepressant use for: Indian, Pakistani, Bangladeshi, black Caribbean and black African patients. Neighbourhood effects were assessed independently of GP practice using a cross-classified multilevel model. Black and minority ethnic groups are up to four times less likely to be newly diagnosed with depression or prescribed antidepressants compared to white British patients. We found an inverse relationship between neighbourhood ethnic density and new depression diagnosis for some groups, where an increase of 10% own-ethnic density was associated with a statistically significant (p<0.05) reduced odds of depression for Pakistani [odds ratio (OR) 0.81, 95% confidence interval (CI) 0.70-0.93], Indian (OR 0.88, CI 0.81-0.95), African (OR 0.88, CI 0.78-0.99) and Bangladeshi (OR 0.94, CI 0.90-0.99) patients. Black Caribbean patients, however, showed the opposite effect (OR 1.26, CI 1.09-1.46). The results for antidepressant use were very similar although the corresponding effect for black Caribbeans was no longer statistically significant (p = 0.07).\n\nQuestion: Does depression diagnosis and antidepressant prescribing vary by location?", "question_only": "Does depression diagnosis and antidepressant prescribing vary by location?", "context": "Studies have linked ethnic differences in depression rates with neighbourhood ethnic density although results have not been conclusive. We looked at this using a novel approach analysing whole population data covering just over one million GP patients in four London boroughs. Using a dataset of GP records for all patients registered in Lambeth, Hackney, Tower Hamlets and Newham in 2013 we investigated new diagnoses of depression and antidepressant use for: Indian, Pakistani, Bangladeshi, black Caribbean and black African patients. Neighbourhood effects were assessed independently of GP practice using a cross-classified multilevel model. Black and minority ethnic groups are up to four times less likely to be newly diagnosed with depression or prescribed antidepressants compared to white British patients. We found an inverse relationship between neighbourhood ethnic density and new depression diagnosis for some groups, where an increase of 10% own-ethnic density was associated with a statistically significant (p<0.05) reduced odds of depression for Pakistani [odds ratio (OR) 0.81, 95% confidence interval (CI) 0.70-0.93], Indian (OR 0.88, CI 0.81-0.95), African (OR 0.88, CI 0.78-0.99) and Bangladeshi (OR 0.94, CI 0.90-0.99) patients. Black Caribbean patients, however, showed the opposite effect (OR 1.26, CI 1.09-1.46). The results for antidepressant use were very similar although the corresponding effect for black Caribbeans was no longer statistically significant (p = 0.07).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "New depression diagnosis and antidepressant use was shown to be less likely in areas of higher own-ethnic density for some, but not all, ethnic groups.", "meshes": ["Adult", "African Continental Ancestry Group", "Antidepressive Agents", "Datasets as Topic", "Depressive Disorder", "Drug Prescriptions", "Ethnic Groups", "European Continental Ancestry Group", "Female", "Humans", "London", "Male", "Middle Aged", "Primary Health Care", "Regression Analysis", "Residence Characteristics", "Young Adult"], "year": "2016"}
{"id": "pubmedqa_20674150", "dataset": "pubmedqa", "question": "Context: A new edition of the TNM was recently released that includes modifications for the staging system of kidney cancers. Specifically, T2 cancers were subclassified into T2a and T2b (<or =10 cm vs>10 cm), tumors with renal vein involvement or perinephric fat involvement were classified as T3a cancers, and those with adrenal involvement were classified as T4 cancers. Our aim was to validate the recently released edition of the TNM staging system for primary tumor classification in kidney cancer. Our multicenter retrospective study consisted of 5339 patients treated in 16 academic Italian centers. Patients underwent either radical or partial nephrectomy. Univariable and multivariable Cox regression models addressed cancer-specific survival (CSS) after surgery. In the study, 1897 patients (35.5%) were classified as pT1a, 1453 (27%) as pT1b, 437 (8%) as pT2a, 153 (3%) as pT2b, 1059 (20%) as pT3a, 117 (2%) as pT3b, 26 (0.5%) as pT3c, and 197 (4%) as pT4. At a median follow-up of 42 mo, 786 (15%) had died of disease. In univariable analysis, patients with pT2b and pT3a tumors had similar CSS, as did patients with pT3c and pT4 tumors. Moreover, both pT3a and pT3b stages included patients with heterogeneous outcomes. In multivariable analysis, the novel classification of the primary tumor was a powerful independent predictor of CSS (p for trend<0.0001). However, the substratification of pT1 tumors did not retain an independent predictive role. The major limitations of the study are retrospective design, lack of central pathologic review, and the small number of patients included in some substages.\n\nQuestion: Validation of the 2009 TNM version in a large multi-institutional cohort of patients treated for renal cell carcinoma: are further improvements needed?", "question_only": "Validation of the 2009 TNM version in a large multi-institutional cohort of patients treated for renal cell carcinoma: are further improvements needed?", "context": "A new edition of the TNM was recently released that includes modifications for the staging system of kidney cancers. Specifically, T2 cancers were subclassified into T2a and T2b (<or =10 cm vs>10 cm), tumors with renal vein involvement or perinephric fat involvement were classified as T3a cancers, and those with adrenal involvement were classified as T4 cancers. Our aim was to validate the recently released edition of the TNM staging system for primary tumor classification in kidney cancer. Our multicenter retrospective study consisted of 5339 patients treated in 16 academic Italian centers. Patients underwent either radical or partial nephrectomy. Univariable and multivariable Cox regression models addressed cancer-specific survival (CSS) after surgery. In the study, 1897 patients (35.5%) were classified as pT1a, 1453 (27%) as pT1b, 437 (8%) as pT2a, 153 (3%) as pT2b, 1059 (20%) as pT3a, 117 (2%) as pT3b, 26 (0.5%) as pT3c, and 197 (4%) as pT4. At a median follow-up of 42 mo, 786 (15%) had died of disease. In univariable analysis, patients with pT2b and pT3a tumors had similar CSS, as did patients with pT3c and pT4 tumors. Moreover, both pT3a and pT3b stages included patients with heterogeneous outcomes. In multivariable analysis, the novel classification of the primary tumor was a powerful independent predictor of CSS (p for trend<0.0001). However, the substratification of pT1 tumors did not retain an independent predictive role. The major limitations of the study are retrospective design, lack of central pathologic review, and the small number of patients included in some substages.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The recently released seventh edition of the primary tumor staging system for kidney tumors is a powerful predictor of CSS. However, some of the substages identified by the classification have overlapping prognoses, and other substages include patients with heterogeneous outcomes. The few modifications included in this edition may have not resolved the most critical issues in the previous version.", "meshes": ["Aged", "Carcinoma, Renal Cell", "Cohort Studies", "Female", "Humans", "Kidney Neoplasms", "Male", "Middle Aged", "Neoplasm Staging", "Retrospective Studies"], "year": "2010"}
{"id": "pubmedqa_18307476", "dataset": "pubmedqa", "question": "Context: The robust relationship between socioeconomic factors and health suggests that social and economic policies might substantially affect health, while other evidence suggests that medical care, the main focus of current health policy, may not be the primary determinant of population health. Income support policies are one promising avenue to improve population health. This study examines whether the federal cash transfer program to poor elderly, the Supplemental Security Income (SSI) program, affects old-age disability. This study uses the 1990 and 2000 censuses, employing state and year fixed-effect models, to test whether within-state changes in maximum SSI benefits over time lead to changes in disability among people aged sixty-five and older. Higher benefits are linked to lower disability rates. Among all single elderly individuals, 30 percent have mobility limitations, and an increase of $100 per month in the maximum SSI benefit caused the rate of mobility limitations to fall by 0.46 percentage points. The findings were robust to sensitivity analyses. First, analyses limited to those most likely to receive SSI produced larger effects, but analyses limited to those least likely to receive SSI produced no measurable effect. Second, varying the disability measure did not meaningfully alter the findings. Third, excluding the institutionalized, immigrants, individuals living in states with exceptionally large benefit changes, and individuals living in states with no SSI supplements did not change the substantive conclusions. Fourth, Medicaid did not confound the effects. Finally, these results were robust for married individuals.\n\nQuestion: Upstream solutions: does the supplemental security income program reduce disability in the elderly?", "question_only": "Upstream solutions: does the supplemental security income program reduce disability in the elderly?", "context": "The robust relationship between socioeconomic factors and health suggests that social and economic policies might substantially affect health, while other evidence suggests that medical care, the main focus of current health policy, may not be the primary determinant of population health. Income support policies are one promising avenue to improve population health. This study examines whether the federal cash transfer program to poor elderly, the Supplemental Security Income (SSI) program, affects old-age disability. This study uses the 1990 and 2000 censuses, employing state and year fixed-effect models, to test whether within-state changes in maximum SSI benefits over time lead to changes in disability among people aged sixty-five and older. Higher benefits are linked to lower disability rates. Among all single elderly individuals, 30 percent have mobility limitations, and an increase of $100 per month in the maximum SSI benefit caused the rate of mobility limitations to fall by 0.46 percentage points. The findings were robust to sensitivity analyses. First, analyses limited to those most likely to receive SSI produced larger effects, but analyses limited to those least likely to receive SSI produced no measurable effect. Second, varying the disability measure did not meaningfully alter the findings. Third, excluding the institutionalized, immigrants, individuals living in states with exceptionally large benefit changes, and individuals living in states with no SSI supplements did not change the substantive conclusions. Fourth, Medicaid did not confound the effects. Finally, these results were robust for married individuals.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Income support policy may be a significant new lever for improving population health, especially that of lower-income persons. Even though the findings are robust, further analyses are needed to confirm their reliability. Future research should examine a variety of different income support policies, as well as whether a broader range of social and economic policies affect health.", "meshes": ["Activities of Daily Living", "Aged", "Aged, 80 and over", "Censuses", "Disabled Persons", "Female", "Humans", "Income", "Male", "Medicaid", "Models, Econometric", "Policy Making", "Social Security", "United States"], "year": "2008"}
{"id": "pubmedqa_20187289", "dataset": "pubmedqa", "question": "Context: Stock et al. (Eur Respir J 25:47-53, 2005) recently estimated asthma prevalence in Germany using claims data on prescriptions and hospital diagnoses and found high prevalence peaks in infants. Our objective was to critically assess and discuss various aspects of identifying children with asthma using prescription data. We replicated the selection procedure of Stock et al. using data on 290,919 children aged 0-17 years insured in the Gmünder ErsatzKasse (GEK) in 2005. Asthma prevalence was also estimated in a sample of 17,641 children aged 0-17 years participating in the German Health Interview and Examination Survey for Children and Adolescents (KiGGS) from 2003 to 2006. In children aged 0-4 years insured in the GEK, prevalences were found to range from 11.7 to 17.7% for boys and from 7.2 to 11.1% for girls when the criteria of Stock et al. were applied. A steady decline in prevalences was observed in older age groups. Asthma prevalence estimated in the KiGGS data showed a quite different distribution. In the age group 0-4 years, prevalences were found to range from 0 to 2.6% in boys and from 0 to 1.0% in girls; in children>4 years, prevalences were found to increase with increasing age.\n\nQuestion: Prescriptions as a proxy for asthma in children: a good choice?", "question_only": "Prescriptions as a proxy for asthma in children: a good choice?", "context": "Stock et al. (Eur Respir J 25:47-53, 2005) recently estimated asthma prevalence in Germany using claims data on prescriptions and hospital diagnoses and found high prevalence peaks in infants. Our objective was to critically assess and discuss various aspects of identifying children with asthma using prescription data. We replicated the selection procedure of Stock et al. using data on 290,919 children aged 0-17 years insured in the Gmünder ErsatzKasse (GEK) in 2005. Asthma prevalence was also estimated in a sample of 17,641 children aged 0-17 years participating in the German Health Interview and Examination Survey for Children and Adolescents (KiGGS) from 2003 to 2006. In children aged 0-4 years insured in the GEK, prevalences were found to range from 11.7 to 17.7% for boys and from 7.2 to 11.1% for girls when the criteria of Stock et al. were applied. A steady decline in prevalences was observed in older age groups. Asthma prevalence estimated in the KiGGS data showed a quite different distribution. In the age group 0-4 years, prevalences were found to range from 0 to 2.6% in boys and from 0 to 1.0% in girls; in children>4 years, prevalences were found to increase with increasing age.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "When additional validation studies were taken into account, asthma medications were found to be prescribed not only for asthma but also for other respiratory diseases. In addition, not all children with current asthma had prescriptions. We therefore conclude that asthma medications are therefore not a good proxy for the disease.", "meshes": ["Adolescent", "Age Distribution", "Age Factors", "Anti-Asthmatic Agents", "Asthma", "Child", "Child, Preschool", "Drug Prescriptions", "Drug Utilization", "Female", "Germany", "Health Care Surveys", "Humans", "Infant", "Infant, Newborn", "Insurance, Pharmaceutical Services", "Male", "Prevalence", "Reproducibility of Results", "Time Factors"], "year": "2010"}
{"id": "pubmedqa_18388848", "dataset": "pubmedqa", "question": "Context: This study examines whether having a regular clinician for preventive care is associated with quality of care for young children, as measured by interpersonal quality ratings and content of anticipatory guidance. The National Survey of Early Childhood Health (NSECH), a nationally representative parent survey of health care quality for 2068 young US children fielded by the National Center for Health Statistics (NCHS). Bivariate and multivariate analyses evaluate associations between having a regular clinician for well child care and interpersonal quality, the content of anticipatory guidance, and timely access to care. In bivariate analysis, parents of children with a regular clinician for preventive care reported slightly higher interpersonal quality (69 vs. 65 on a 0-100 scale, P = 0.01). Content of anticipatory guidance received was slightly greater for children with a regular clinician (82 vs. 80 on a 0-100 scale, P = 0.03). In bivariate analysis, a regular clinician was associated with interpersonal quality only among African American and Hispanic children. In multivariate analyses, controlling for factors that could independently influence self-reports of experiences with care, interpersonal quality but not anticipatory guidance content was higher for children with a regular clinician.\n\nQuestion: Does having a regular primary care clinician improve quality of preventive care for young children?", "question_only": "Does having a regular primary care clinician improve quality of preventive care for young children?", "context": "This study examines whether having a regular clinician for preventive care is associated with quality of care for young children, as measured by interpersonal quality ratings and content of anticipatory guidance. The National Survey of Early Childhood Health (NSECH), a nationally representative parent survey of health care quality for 2068 young US children fielded by the National Center for Health Statistics (NCHS). Bivariate and multivariate analyses evaluate associations between having a regular clinician for well child care and interpersonal quality, the content of anticipatory guidance, and timely access to care. In bivariate analysis, parents of children with a regular clinician for preventive care reported slightly higher interpersonal quality (69 vs. 65 on a 0-100 scale, P = 0.01). Content of anticipatory guidance received was slightly greater for children with a regular clinician (82 vs. 80 on a 0-100 scale, P = 0.03). In bivariate analysis, a regular clinician was associated with interpersonal quality only among African American and Hispanic children. In multivariate analyses, controlling for factors that could independently influence self-reports of experiences with care, interpersonal quality but not anticipatory guidance content was higher for children with a regular clinician.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Having a regular primary care clinician is embraced in pediatrics, although team care among physicians is also widely practiced. For young children, having a regular clinician is associated with modest gains in interpersonal quality and no differences in content of anticipatory guidance. The benefit of having a regular clinician may primarily occur in interpersonal quality for subgroups of young children.", "meshes": ["Child, Preschool", "Continental Population Groups", "Educational Status", "Female", "Health Services", "Health Services Accessibility", "Health Status", "Humans", "Infant", "Insurance Coverage", "Insurance, Health", "Male", "Primary Health Care", "Primary Prevention", "Quality of Health Care"], "year": "2008"}
{"id": "pubmedqa_19351635", "dataset": "pubmedqa", "question": "Context: National guidelines and government directives have adopted policies for urgent assessment of patients with a transient ischaemic attack or minor stroke not admitted to hospital. The risk of recurrent stroke increases substantially with age, as does the potential benefit of secondary prevention. In order to develop effective strategies for older patients, it is important to identify how stroke care is currently provided for this patient group. Between 2004 and 2006, older patients (>75 years) referred to a neurovascular clinic were compared with younger patients (<or =75 years). Sociodemographic details, clinical features, resource use and secondary prevention in a neurovascular clinic were collected. Of 379 patients referred to the clinic, 129 (34%) were given a non-stroke diagnosis. Of the remaining 250 patients, 149 (60%) were<or =75 years. Median time from symptom onset to clinic appointment was similar for the two groups (24 (IQR 15-42) vs 24 (IQR 14-43) days; p = 0.58). Older patients were more likely to be in atrial fibrillation (10.1% vs 22.8%, p<0.001) and have lacunar stroke (34.7% vs 22.1%; p = 0.04). CT rates were similar in the two groups (27.8% vs 80.0%, p = 0.75). Scans were performed more quickly in younger patients (p<0.01). MRI scan rates were higher in younger patients (26% vs 4%, p<0.01), as was carotid Doppler imaging (92% vs 77%, p<0.01). There were no differences in prescribed secondary preventive treatments. Older patients experienced less delay for carotid endarterectomy (49 vs 90 days, p<0.01). Younger patients were more likely to be given advice on weight reduction (30.2% vs 12.9%, p<0.01) and diet (46.3% vs 31.7%, p = 0.02) than older patients.\n\nQuestion: Do older patients receive adequate stroke care?", "question_only": "Do older patients receive adequate stroke care?", "context": "National guidelines and government directives have adopted policies for urgent assessment of patients with a transient ischaemic attack or minor stroke not admitted to hospital. The risk of recurrent stroke increases substantially with age, as does the potential benefit of secondary prevention. In order to develop effective strategies for older patients, it is important to identify how stroke care is currently provided for this patient group. Between 2004 and 2006, older patients (>75 years) referred to a neurovascular clinic were compared with younger patients (<or =75 years). Sociodemographic details, clinical features, resource use and secondary prevention in a neurovascular clinic were collected. Of 379 patients referred to the clinic, 129 (34%) were given a non-stroke diagnosis. Of the remaining 250 patients, 149 (60%) were<or =75 years. Median time from symptom onset to clinic appointment was similar for the two groups (24 (IQR 15-42) vs 24 (IQR 14-43) days; p = 0.58). Older patients were more likely to be in atrial fibrillation (10.1% vs 22.8%, p<0.001) and have lacunar stroke (34.7% vs 22.1%; p = 0.04). CT rates were similar in the two groups (27.8% vs 80.0%, p = 0.75). Scans were performed more quickly in younger patients (p<0.01). MRI scan rates were higher in younger patients (26% vs 4%, p<0.01), as was carotid Doppler imaging (92% vs 77%, p<0.01). There were no differences in prescribed secondary preventive treatments. Older patients experienced less delay for carotid endarterectomy (49 vs 90 days, p<0.01). Younger patients were more likely to be given advice on weight reduction (30.2% vs 12.9%, p<0.01) and diet (46.3% vs 31.7%, p = 0.02) than older patients.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Older patients were less likely to receive diagnostic investigations and lifestyle modification advice than younger patients. Guidelines need to be adopted to ensure prompt evidence-based stroke care in the outpatient setting.", "meshes": ["Age Factors", "Aged", "Aged, 80 and over", "Delivery of Health Care", "Emergency Service, Hospital", "England", "Female", "Health Services for the Aged", "Humans", "Magnetic Resonance Angiography", "Male", "Quality of Health Care", "Risk Assessment", "Stroke", "Waiting Lists"], "year": "2009"}
{"id": "pubmedqa_10340286", "dataset": "pubmedqa", "question": "Context: The diagnosis of acute appendicitis is still difficult and the results are unsatisfactory in three particular patient groups: in children, in fertile-age women and in elderly patients. As our population ages, the challenge for expedient diagnosis and intervention in older age groups will become more and more significant. The present study aimed at clarifying the role of leukocyte count and C-reactive protein (CRP) measurements in the diagnosis of acute appendicitis in the elderly. In particular, are there patients with acute appendicitis but unelevated leukocyte count and CRP? Eighty-three consecutive elderly patients underwent appendectomy for suspected acute appendicitis. The mean leukocyte count and CRP value were calculated in patients with an uninflamed appendix (group A) and in those with acute appendicitis (group B). The percentages of patients with: (1) both values unelevated; (2) only leukocyte count elevated; (3) only CRP value elevated; (4) both values elevated were calculated within the groups A and B. There was no statistically significant difference in leukocyte counts or CRP values between patients with an uninflamed appendix (group A) and those with acute appendicitis (group B). When the patients were divided into the four subgroups, the most conspicuous finding was that group B (acute appendicitis, n = 73) contained no patients with both values unelevated.\n\nQuestion: Is there a role for leukocyte and CRP measurements in the diagnosis of acute appendicitis in the elderly?", "question_only": "Is there a role for leukocyte and CRP measurements in the diagnosis of acute appendicitis in the elderly?", "context": "The diagnosis of acute appendicitis is still difficult and the results are unsatisfactory in three particular patient groups: in children, in fertile-age women and in elderly patients. As our population ages, the challenge for expedient diagnosis and intervention in older age groups will become more and more significant. The present study aimed at clarifying the role of leukocyte count and C-reactive protein (CRP) measurements in the diagnosis of acute appendicitis in the elderly. In particular, are there patients with acute appendicitis but unelevated leukocyte count and CRP? Eighty-three consecutive elderly patients underwent appendectomy for suspected acute appendicitis. The mean leukocyte count and CRP value were calculated in patients with an uninflamed appendix (group A) and in those with acute appendicitis (group B). The percentages of patients with: (1) both values unelevated; (2) only leukocyte count elevated; (3) only CRP value elevated; (4) both values elevated were calculated within the groups A and B. There was no statistically significant difference in leukocyte counts or CRP values between patients with an uninflamed appendix (group A) and those with acute appendicitis (group B). When the patients were divided into the four subgroups, the most conspicuous finding was that group B (acute appendicitis, n = 73) contained no patients with both values unelevated.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Although elevated leukocyte count and CRP value cannot effectively establish the diagnosis of acute appendicitis in the elderly, unelevated values exclude it. Accordingly, appendectomy is not recommended to be performed in an elderly patient with unelevated leukocyte count and CRP value, although clinical symptoms and signs indicate acute appendicitis.", "meshes": ["Acute Disease", "Aged", "Appendectomy", "Appendicitis", "C-Reactive Protein", "Case-Control Studies", "Female", "Humans", "Leukocyte Count", "Male"], "year": "1999"}
{"id": "pubmedqa_18801797", "dataset": "pubmedqa", "question": "Context: Fruit and vegetables are protective of a number of chronic diseases; however, their intakes have been shown to vary by socioeconomic position (SEP). Household and food shopping environmental factors are thought to contribute to these differences. To determine whether household and food shopping environmental factors are associated with fruit and vegetable (FV) intakes, and contribute to socioeconomic inequalities in FV consumption. Cross-sectional data were obtained by a postal questionnaire among 4333 adults (23-85 years) living in 168 neighbourhoods in the south-eastern Netherlands. Participants agreed/disagreed with a number of statements about the characteristics of their household and food shopping environments, including access, prices and quality. Education was used to characterise socioeconomic position (SEP). Main outcome measures were whether or not participants consumed fruit or vegetables on a daily basis. Multilevel logistic regression models examined between-area variance in FV consumption and associations between characteristics of the household and food shopping environments and FV consumption. Only a few household and food shopping environmental factors were significantly associated with fruit and vegetable consumption, and their prevalence was low. Participants who perceived FV to be expensive were more likely to consume them. There were significant socioeconomic inequalities in fruit and vegetable consumption (ORs of not consuming fruit and vegetables were 4.26 and 5.47 among the lowest-educated groups for fruit and vegetables, respectively); however, these were not explained by any household or food shopping environmental factors.\n\nQuestion: Household and food shopping environments: do they play a role in socioeconomic inequalities in fruit and vegetable consumption?", "question_only": "Household and food shopping environments: do they play a role in socioeconomic inequalities in fruit and vegetable consumption?", "context": "Fruit and vegetables are protective of a number of chronic diseases; however, their intakes have been shown to vary by socioeconomic position (SEP). Household and food shopping environmental factors are thought to contribute to these differences. To determine whether household and food shopping environmental factors are associated with fruit and vegetable (FV) intakes, and contribute to socioeconomic inequalities in FV consumption. Cross-sectional data were obtained by a postal questionnaire among 4333 adults (23-85 years) living in 168 neighbourhoods in the south-eastern Netherlands. Participants agreed/disagreed with a number of statements about the characteristics of their household and food shopping environments, including access, prices and quality. Education was used to characterise socioeconomic position (SEP). Main outcome measures were whether or not participants consumed fruit or vegetables on a daily basis. Multilevel logistic regression models examined between-area variance in FV consumption and associations between characteristics of the household and food shopping environments and FV consumption. Only a few household and food shopping environmental factors were significantly associated with fruit and vegetable consumption, and their prevalence was low. Participants who perceived FV to be expensive were more likely to consume them. There were significant socioeconomic inequalities in fruit and vegetable consumption (ORs of not consuming fruit and vegetables were 4.26 and 5.47 among the lowest-educated groups for fruit and vegetables, respectively); however, these were not explained by any household or food shopping environmental factors.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Improving access to FV in the household and food shopping environments will only make a small contribution to improving population consumption levels, and may only have a limited effect in reducing socioeconomic inequalities in their consumption.", "meshes": ["Adult", "Age Factors", "Aged", "Aged, 80 and over", "Commerce", "Educational Status", "Environment", "Epidemiologic Methods", "Feeding Behavior", "Food Supply", "Fruit", "Humans", "Middle Aged", "Netherlands", "Sex Factors", "Social Class", "Vegetables", "Young Adult"], "year": "2009"}
{"id": "pubmedqa_18399830", "dataset": "pubmedqa", "question": "Context: To evaluate whether robotically assisted laparoscopic prostatectomy (RALP) is less invasive than radical retropubic prostatectomy (RRP), as experimental studies suggest that the acute phase reaction is proportional to surgery-induced tissue damage. Between May and November 2006, all patients undergoing RRP or RALP in our department were prospectively assessed. Blood samples were collected 24 h before (T0), during surgery (T1), at the end of anaesthesia (T2), and 12 (T3) and 24 h after surgery (T4), and assayed for interleukin(IL)-6 and IL-1 alpha, C-reactive protein (CRP), and lactate. The Mann-Whitney U-, Student's t- and Friedman tests were used to compare continuous variables, and the Pearson chi-square and Fisher test for categorical variables, with a two-sided P<0.05 considered to indicate significance. In all, 35 and 26 patients were assessed for RALP and RRP, respectively; the median (interquartile range) age was 62 (56-68) and 68.5 (59.2-71.2) years, respectively (P<0.009). Baseline levels (T0) of IL-1, IL-6, CRP and lactate were comparable in both arms. IL-6, CRP and lactates levels increased during both kinds of surgery. The mean IL-6 and CPR values were higher for RRP at T1 (P = 0.01 and 0.001), T2 (P = 0.001 and<0.001), T3 (P = 0.002 and<0.001) and T4 (P<0.001 and 0.02), respectively. Lactate was higher for RRP at T2 (P = 0.001), T3 (P = 0.001) and T4 (P = 0.004), although remaining within the normal ranges. IL-1 alpha did not change at the different sample times.\n\nQuestion: Is robotically assisted laparoscopic radical prostatectomy less invasive than retropubic radical prostatectomy?", "question_only": "Is robotically assisted laparoscopic radical prostatectomy less invasive than retropubic radical prostatectomy?", "context": "To evaluate whether robotically assisted laparoscopic prostatectomy (RALP) is less invasive than radical retropubic prostatectomy (RRP), as experimental studies suggest that the acute phase reaction is proportional to surgery-induced tissue damage. Between May and November 2006, all patients undergoing RRP or RALP in our department were prospectively assessed. Blood samples were collected 24 h before (T0), during surgery (T1), at the end of anaesthesia (T2), and 12 (T3) and 24 h after surgery (T4), and assayed for interleukin(IL)-6 and IL-1 alpha, C-reactive protein (CRP), and lactate. The Mann-Whitney U-, Student's t- and Friedman tests were used to compare continuous variables, and the Pearson chi-square and Fisher test for categorical variables, with a two-sided P<0.05 considered to indicate significance. In all, 35 and 26 patients were assessed for RALP and RRP, respectively; the median (interquartile range) age was 62 (56-68) and 68.5 (59.2-71.2) years, respectively (P<0.009). Baseline levels (T0) of IL-1, IL-6, CRP and lactate were comparable in both arms. IL-6, CRP and lactates levels increased during both kinds of surgery. The mean IL-6 and CPR values were higher for RRP at T1 (P = 0.01 and 0.001), T2 (P = 0.001 and<0.001), T3 (P = 0.002 and<0.001) and T4 (P<0.001 and 0.02), respectively. Lactate was higher for RRP at T2 (P = 0.001), T3 (P = 0.001) and T4 (P = 0.004), although remaining within the normal ranges. IL-1 alpha did not change at the different sample times.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "This study showed for the first time that RALP induces lower tissue trauma than RRP.", "meshes": ["Acute-Phase Reaction", "Aged", "C-Reactive Protein", "Humans", "Interleukin-1alpha", "Interleukin-6", "Lactic Acid", "Laparoscopy", "Male", "Middle Aged", "Postoperative Complications", "Prospective Studies", "Prostatectomy", "Prostatic Neoplasms", "Robotics", "Statistics, Nonparametric", "Treatment Outcome"], "year": "2008"}
{"id": "pubmedqa_10781708", "dataset": "pubmedqa", "question": "Context: Most studies on thrombosis prophylaxis focus on postoperative venous thrombosis. In medical wards thrombosis prophylaxis is generally restricted to patients who are immobilised. Our primary aim was to investigate the incidence of venous thrombosis in a general internal ward, to assess whether more rigorous prophylaxis would be feasible. We investigated the incidence of venous thrombosis in patients hospitalised from 1992 to 1996 and related our findings to literature reports. The incidence of symptomatic venous thrombosis in internal patients during hospitalisation was 39/6332 (0.6%). Among these 39 patients, 24 had a malignancy, whereas 876 out of all 6332 patients had a known malignancy. So, the incidence in this group with cancer was 2.7% compared with 0.3% (15/5456) in the non-cancer group (relative risk for venous thrombosis due to malignancy was 10.0 (95%C.I. 5.3-18.9).\n\nQuestion: Thrombosis prophylaxis in hospitalised medical patients: does prophylaxis in all patients make sense?", "question_only": "Thrombosis prophylaxis in hospitalised medical patients: does prophylaxis in all patients make sense?", "context": "Most studies on thrombosis prophylaxis focus on postoperative venous thrombosis. In medical wards thrombosis prophylaxis is generally restricted to patients who are immobilised. Our primary aim was to investigate the incidence of venous thrombosis in a general internal ward, to assess whether more rigorous prophylaxis would be feasible. We investigated the incidence of venous thrombosis in patients hospitalised from 1992 to 1996 and related our findings to literature reports. The incidence of symptomatic venous thrombosis in internal patients during hospitalisation was 39/6332 (0.6%). Among these 39 patients, 24 had a malignancy, whereas 876 out of all 6332 patients had a known malignancy. So, the incidence in this group with cancer was 2.7% compared with 0.3% (15/5456) in the non-cancer group (relative risk for venous thrombosis due to malignancy was 10.0 (95%C.I. 5.3-18.9).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The incidence of venous thrombosis during hospitalisation in a department of general internal medicine is low and does not justify prophylaxis in all internal patients. Cancer is a strong risk factor for hospital-acquired thrombosis in the medical ward. Further studies may answer the question as to whether thrombosis prophylaxis in this subgroup is feasible.", "meshes": ["Adolescent", "Adult", "Aged", "Aged, 80 and over", "Enoxaparin", "Female", "Humans", "Incidence", "Male", "Neoplasms", "Postoperative Care", "Postoperative Complications", "Retrospective Studies", "Risk Factors", "Venous Thrombosis"], "year": "2000"}
{"id": "pubmedqa_21402341", "dataset": "pubmedqa", "question": "Context: Extracranial internal carotid artery stenosis is a risk factor for perioperative stroke in patients undergoing coronary artery bypass surgery (CAB). Although selective and non-selective methods of preoperative carotid screening have been advocated, it remains unclear if this screening is clinically relevant.AIM: To test whether selective carotid screening is as effective as non-selective screening in detecting significant carotid disease. The case records of patients consecutively undergoing CAB were reviewed. Patients were stratified retrospectively into high- or low-risk groups according to risk factors for significant carotid stenosis and perioperative stroke: peripheral vascular disease (PVD), carotid bruit, diabetes mellitus, age>70 years and/or history of cerebrovascular disease. Prevalence of carotid stenosis detected by ultrasonography, surgical management and perioperative stroke rates were determined in each group. Overall, 205 consecutive patients underwent preoperative carotid screening. The prevalence of significant carotid stenosis was 5.8%. Univariate analysis confirmed that PVD (P=0.005), carotid bruit (P=0.003) and diabetes mellitus (P=0.05) were significant risk factors for stenosis. Carotid stenosis was a risk factor for stroke (P=0.03). Prevalence of carotid stenosis was higher in the high-risk group (9.1%) than the low-risk group (1.2%) (P<0.05). All concomitant or staged carotid endarterectomies/CAB (5/205) and all patients who had perioperative strokes (5/205) were in the high-risk group (P=0.01).\n\nQuestion: Assessment of carotid artery stenosis before coronary artery bypass surgery. Is it always necessary?", "question_only": "Assessment of carotid artery stenosis before coronary artery bypass surgery. Is it always necessary?", "context": "Extracranial internal carotid artery stenosis is a risk factor for perioperative stroke in patients undergoing coronary artery bypass surgery (CAB). Although selective and non-selective methods of preoperative carotid screening have been advocated, it remains unclear if this screening is clinically relevant.AIM: To test whether selective carotid screening is as effective as non-selective screening in detecting significant carotid disease. The case records of patients consecutively undergoing CAB were reviewed. Patients were stratified retrospectively into high- or low-risk groups according to risk factors for significant carotid stenosis and perioperative stroke: peripheral vascular disease (PVD), carotid bruit, diabetes mellitus, age>70 years and/or history of cerebrovascular disease. Prevalence of carotid stenosis detected by ultrasonography, surgical management and perioperative stroke rates were determined in each group. Overall, 205 consecutive patients underwent preoperative carotid screening. The prevalence of significant carotid stenosis was 5.8%. Univariate analysis confirmed that PVD (P=0.005), carotid bruit (P=0.003) and diabetes mellitus (P=0.05) were significant risk factors for stenosis. Carotid stenosis was a risk factor for stroke (P=0.03). Prevalence of carotid stenosis was higher in the high-risk group (9.1%) than the low-risk group (1.2%) (P<0.05). All concomitant or staged carotid endarterectomies/CAB (5/205) and all patients who had perioperative strokes (5/205) were in the high-risk group (P=0.01).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "In our cohort, selective screening of patients aged>70 years, with carotid bruit, a history of cerebrovascular disease, diabetes mellitus or PVD would have reduced the screening load by 40%, with trivial impact on surgical management or neurological outcomes.", "meshes": ["Aged", "Algorithms", "Carotid Stenosis", "Chi-Square Distribution", "Coronary Artery Bypass", "Coronary Artery Disease", "Endarterectomy, Carotid", "Female", "France", "Humans", "Male", "Patient Selection", "Predictive Value of Tests", "Preoperative Care", "Prevalence", "Retrospective Studies", "Risk Assessment", "Risk Factors", "Severity of Illness Index", "Stroke", "Ultrasonography, Doppler, Duplex"], "year": "2011"}
{"id": "pubmedqa_19575307", "dataset": "pubmedqa", "question": "Context: We aimed to investigate the glomerular hyperfiltration due to pregnancy in women with more parities. Five hundred women aged 52.57 +/- 8.08 years, without a history of hypertension, diabetes mellitus or complicated pregnancy were involved in the study. They were divided into three groups. Group 1: women with no or one parity (n = 76); group 2: women with two or three parities (n = 333); group 3: women with four or more parities (n = 91). Laboratory parameters and demographical data were compared between the three groups. Mean age, serum urea and serum creatinine were similar between three groups. Patients in group 3 had significantly higher GFR values compared to groups 1 and 2 (109.44 +/- 30.99, 110.76 +/- 30.22 and 121.92 +/- 34.73 mL/min/1.73 m(2) for groups 1, 2 and 3, respectively; P = 0.008 for group 1 vs group 3; P = 0.002 for group 2 vs group 3).\n\nQuestion: Does glomerular hyperfiltration in pregnancy damage the kidney in women with more parities?", "question_only": "Does glomerular hyperfiltration in pregnancy damage the kidney in women with more parities?", "context": "We aimed to investigate the glomerular hyperfiltration due to pregnancy in women with more parities. Five hundred women aged 52.57 +/- 8.08 years, without a history of hypertension, diabetes mellitus or complicated pregnancy were involved in the study. They were divided into three groups. Group 1: women with no or one parity (n = 76); group 2: women with two or three parities (n = 333); group 3: women with four or more parities (n = 91). Laboratory parameters and demographical data were compared between the three groups. Mean age, serum urea and serum creatinine were similar between three groups. Patients in group 3 had significantly higher GFR values compared to groups 1 and 2 (109.44 +/- 30.99, 110.76 +/- 30.22 and 121.92 +/- 34.73 mL/min/1.73 m(2) for groups 1, 2 and 3, respectively; P = 0.008 for group 1 vs group 3; P = 0.002 for group 2 vs group 3).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "In our study, we suggest that glomerular hyperfiltration due to pregnancy does not have adverse effects on kidney in women with more parities. Pregnancy may have possible protective mechanisms for kidney against adverse effects of glomerular hyperfiltration.", "meshes": ["Adult", "Aged", "Analysis of Variance", "Body Mass Index", "Cohort Studies", "Confidence Intervals", "Creatinine", "Female", "Glomerular Filtration Rate", "Humans", "Kidney Diseases", "Kidney Function Tests", "Middle Aged", "Parity", "Pregnancy", "Pregnancy Complications", "Risk Factors", "Sensitivity and Specificity", "Urea", "Uric Acid", "Urinalysis"], "year": "2009"}
{"id": "pubmedqa_12690589", "dataset": "pubmedqa", "question": "Context: To compare the myoelectric onset of muscle fatigue in physically active trained young skiers with respect to elderly skiers and to test whether continuous training can counteract the selective loss of type II muscle fibers usually observed with aging. An observational, cross-sectional study of the myoelectric onset of muscle fatigue in the left tibialis anterior muscles. Surface electromyography recorded with portable devices at a downhill ski rescue lodge in the Italian Alps. Fifty-four physically trained, active skiers (43 men, 11 women; age range, 24-85y). Questionnaire on physical activity and 2 sustained isometric voluntary contractions at 20% and 2 at 80% of the maximal voluntary contraction level. Isometric contractions and mean and median spectral frequencies calculated to monitor the myoelectric manifestations of muscle fatigue. Fatigue indices did not differ significantly between younger and older subjects and, thus, did not show a correlation between myoelectric manifestations of muscle fatigue and age in physically active subjects.\n\nQuestion: Can continuous physical training counteract aging effect on myoelectric fatigue?", "question_only": "Can continuous physical training counteract aging effect on myoelectric fatigue?", "context": "To compare the myoelectric onset of muscle fatigue in physically active trained young skiers with respect to elderly skiers and to test whether continuous training can counteract the selective loss of type II muscle fibers usually observed with aging. An observational, cross-sectional study of the myoelectric onset of muscle fatigue in the left tibialis anterior muscles. Surface electromyography recorded with portable devices at a downhill ski rescue lodge in the Italian Alps. Fifty-four physically trained, active skiers (43 men, 11 women; age range, 24-85y). Questionnaire on physical activity and 2 sustained isometric voluntary contractions at 20% and 2 at 80% of the maximal voluntary contraction level. Isometric contractions and mean and median spectral frequencies calculated to monitor the myoelectric manifestations of muscle fatigue. Fatigue indices did not differ significantly between younger and older subjects and, thus, did not show a correlation between myoelectric manifestations of muscle fatigue and age in physically active subjects.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "It appears possible that aging skeletal muscles subjected to continuous exercise develop an adaptive response that counteracts the selective loss of type II muscle fibers usually observed in the muscles of elderly sedentary subjects. Our results suggest that physical activity could be considered in the elderly within a broad rehabilitative framework in which appropriate and even tailored physical training could be planned to counteract the physiologic effects of aging on muscle fiber distribution.", "meshes": ["Adult", "Age Factors", "Aged", "Aged, 80 and over", "Aging", "Cross-Sectional Studies", "Electromyography", "Exercise", "Female", "Humans", "Isometric Contraction", "Leg", "Male", "Middle Aged", "Muscle Fatigue", "Muscle, Skeletal", "Physical Fitness"], "year": "2003"}
{"id": "pubmedqa_22825590", "dataset": "pubmedqa", "question": "Context: Longitudinal cohort studies in sub-Saharan Africa are urgently needed to understand cardiovascular disease development. We, therefore, explored health behaviours and conventional risk factors of African individuals with optimal blood pressure (BP) (≤ 120/80 mm Hg), and their 5-year prediction for the development of hypertension. The Prospective Urban Rural Epidemiology study in the North West Province, South Africa, started in 2005 and included African volunteers (n = 1994; aged>30 years) from a sample of 6000 randomly selected households in rural and urban areas. At baseline, 48% of the participants were hypertensive (≥ 140/90 mmHg). Those with optimal BP (n = 478) were followed at a success rate of 70% for 5 years (213 normotensive, 68 hypertensive, 57 deceased). Africans that became hypertensive smoked more than the normotensive individuals (68.2% vs 49.8%), and they also had a greater waist circumference [ratio of geometric means of 0.94 cm (95% CI: 0.86-0.99)] and greater amount of γ-glutamyltransferase [0.74 U/l (95% CI: 0.62-0.88)]at baseline. The 5-year change in BP was independently explained by baseline γ-glutamyltransferase [R(2) = 0.23, β = 0.13 U/l (95% CI: 0.01-0.19)]. Alcohol intake also predicted central systolic BP and carotid cross-sectional wall area (CSWA) at follow-up. Waist circumference was another predictor of BP changes [β = 0.18 cm (95% CI: 0.05-0.24)]and CSWA. HIV infection was inversely associated with increased BP.\n\nQuestion: Are behavioural risk factors to be blamed for the conversion from optimal blood pressure to hypertensive status in Black South Africans?", "question_only": "Are behavioural risk factors to be blamed for the conversion from optimal blood pressure to hypertensive status in Black South Africans?", "context": "Longitudinal cohort studies in sub-Saharan Africa are urgently needed to understand cardiovascular disease development. We, therefore, explored health behaviours and conventional risk factors of African individuals with optimal blood pressure (BP) (≤ 120/80 mm Hg), and their 5-year prediction for the development of hypertension. The Prospective Urban Rural Epidemiology study in the North West Province, South Africa, started in 2005 and included African volunteers (n = 1994; aged>30 years) from a sample of 6000 randomly selected households in rural and urban areas. At baseline, 48% of the participants were hypertensive (≥ 140/90 mmHg). Those with optimal BP (n = 478) were followed at a success rate of 70% for 5 years (213 normotensive, 68 hypertensive, 57 deceased). Africans that became hypertensive smoked more than the normotensive individuals (68.2% vs 49.8%), and they also had a greater waist circumference [ratio of geometric means of 0.94 cm (95% CI: 0.86-0.99)] and greater amount of γ-glutamyltransferase [0.74 U/l (95% CI: 0.62-0.88)]at baseline. The 5-year change in BP was independently explained by baseline γ-glutamyltransferase [R(2) = 0.23, β = 0.13 U/l (95% CI: 0.01-0.19)]. Alcohol intake also predicted central systolic BP and carotid cross-sectional wall area (CSWA) at follow-up. Waist circumference was another predictor of BP changes [β = 0.18 cm (95% CI: 0.05-0.24)]and CSWA. HIV infection was inversely associated with increased BP.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "During the 5 years, 24% of Africans with optimal BP developed hypertension. The surge in hypertension in Africa is largely explained by modifiable risk factors. Public health strategies should focus aggressively on lifestyle to prevent a catastrophic burden on the national health system.", "meshes": ["African Continental Ancestry Group", "Anthropometry", "Biomarkers", "Blood Pressure", "C-Reactive Protein", "Chi-Square Distribution", "Creatinine", "Female", "Health Behavior", "Humans", "Hypertension", "Linear Models", "Lipids", "Male", "Middle Aged", "Predictive Value of Tests", "Prospective Studies", "Risk Factors", "South Africa", "gamma-Glutamyltransferase"], "year": "2012"}
{"id": "pubmedqa_27581329", "dataset": "pubmedqa", "question": "Context: Measurement of basal metabolic rate (BMR) is suggested as a tool to estimate energy requirements. Therefore, BMR prediction equations have been developed in multiple populations because indirect calorimetry is not always feasible. However, there is a paucity of data on BMR measured in overweight and obese adults living in Asia and equations developed for this group of interest. The aim of this study was to develop a new BMR prediction equation for Chinese adults applicable for a large BMI range and compare it with commonly used prediction equations. Subjects were 121 men and 111 women (age: 21-67 years, BMI: 16-41 kg/m(2)). Height, weight, and BMR were measured. Continuous open-circuit indirect calorimetry using a ventilated hood system for 30 min was used to measure BMR. A regression equation was derived using stepwise regression and accuracy was compared to 6 existing equations (Harris-Benedict, Henry, Liu, Yang, Owen and Mifflin). Additionally, the newly derived equation was cross-validated in a separate group of 70 Chinese subjects (26 men and 44 women, age: 21-69 years, BMI: 17-39 kg/m(2)). The equation developed from our data was: BMR (kJ/d) = 52.6 x weight (kg) + 828 x gender + 1960 (women = 0, men = 1; R(2) = 0.81). The accuracy rate (within 10 % accurate) was 78 % which compared well to Owen (70 %), Henry (67 %), Mifflin (67 %), Liu (58 %), Harris-Benedict (45 %) and Yang (37 %) for the whole range of BMI. For a BMI greater than 23, the Singapore equation reached an accuracy rate of 76 %. Cross-validation proved an accuracy rate of 80 %.\n\nQuestion: Estimation of basal metabolic rate in Chinese: are the current prediction equations applicable?", "question_only": "Estimation of basal metabolic rate in Chinese: are the current prediction equations applicable?", "context": "Measurement of basal metabolic rate (BMR) is suggested as a tool to estimate energy requirements. Therefore, BMR prediction equations have been developed in multiple populations because indirect calorimetry is not always feasible. However, there is a paucity of data on BMR measured in overweight and obese adults living in Asia and equations developed for this group of interest. The aim of this study was to develop a new BMR prediction equation for Chinese adults applicable for a large BMI range and compare it with commonly used prediction equations. Subjects were 121 men and 111 women (age: 21-67 years, BMI: 16-41 kg/m(2)). Height, weight, and BMR were measured. Continuous open-circuit indirect calorimetry using a ventilated hood system for 30 min was used to measure BMR. A regression equation was derived using stepwise regression and accuracy was compared to 6 existing equations (Harris-Benedict, Henry, Liu, Yang, Owen and Mifflin). Additionally, the newly derived equation was cross-validated in a separate group of 70 Chinese subjects (26 men and 44 women, age: 21-69 years, BMI: 17-39 kg/m(2)). The equation developed from our data was: BMR (kJ/d) = 52.6 x weight (kg) + 828 x gender + 1960 (women = 0, men = 1; R(2) = 0.81). The accuracy rate (within 10 % accurate) was 78 % which compared well to Owen (70 %), Henry (67 %), Mifflin (67 %), Liu (58 %), Harris-Benedict (45 %) and Yang (37 %) for the whole range of BMI. For a BMI greater than 23, the Singapore equation reached an accuracy rate of 76 %. Cross-validation proved an accuracy rate of 80 %.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "To date, the newly developed Singapore equation is the most accurate BMR prediction equation in Chinese and is applicable for use in a large BMI range including those overweight and obese.", "meshes": ["Adult", "Aged", "Asian Continental Ancestry Group", "Basal Metabolism", "Body Mass Index", "Body Weight", "Calorimetry, Indirect", "Cross-Sectional Studies", "Exercise", "Fasting", "Female", "Humans", "Male", "Middle Aged", "Obesity", "Overweight", "Reproducibility of Results", "Singapore", "Young Adult"], "year": "2016"}
{"id": "pubmedqa_24785562", "dataset": "pubmedqa", "question": "Context: A short course of systemic corticosteroids is an important therapy in the treatment of pediatric asthma exacerbations. Although a 5-day course of oral prednisone or prednisolone has become the most commonly used regimen, dexamethasone has also been used for a shorter duration (1-2 days) with potential for improvement in compliance and palatability. We reviewed the literature to determine if there is sufficient evidence that dexamethasone can be used as an effective alternative in the treatment of pediatric asthma exacerbations in the inpatient setting. A Medline search was conducted on the use of dexamethasone in the treatment of asthma exacerbations in children. The studies selected were clinical trials comparing the efficacy of dexamethasone with prednisone. Meta-analysis was performed examining physician revisitation rates and symptomatic return to baseline. Six completed pediatric clinical trials met the inclusion criteria. All of the pediatric trials found that prednisone is not superior to dexamethasone in treating mild to moderate asthma exacerbations. Meta-analysis demonstrated homogeneity between the dexamethasone and prednisone groups when examining symptomatic return to baseline and unplanned physician revisits after the initial emergency department encounter. Some studies found potential additional benefits of dexamethasone, including improved compliance and less vomiting.\n\nQuestion: Is dexamethasone an effective alternative to oral prednisone in the treatment of pediatric asthma exacerbations?", "question_only": "Is dexamethasone an effective alternative to oral prednisone in the treatment of pediatric asthma exacerbations?", "context": "A short course of systemic corticosteroids is an important therapy in the treatment of pediatric asthma exacerbations. Although a 5-day course of oral prednisone or prednisolone has become the most commonly used regimen, dexamethasone has also been used for a shorter duration (1-2 days) with potential for improvement in compliance and palatability. We reviewed the literature to determine if there is sufficient evidence that dexamethasone can be used as an effective alternative in the treatment of pediatric asthma exacerbations in the inpatient setting. A Medline search was conducted on the use of dexamethasone in the treatment of asthma exacerbations in children. The studies selected were clinical trials comparing the efficacy of dexamethasone with prednisone. Meta-analysis was performed examining physician revisitation rates and symptomatic return to baseline. Six completed pediatric clinical trials met the inclusion criteria. All of the pediatric trials found that prednisone is not superior to dexamethasone in treating mild to moderate asthma exacerbations. Meta-analysis demonstrated homogeneity between the dexamethasone and prednisone groups when examining symptomatic return to baseline and unplanned physician revisits after the initial emergency department encounter. Some studies found potential additional benefits of dexamethasone, including improved compliance and less vomiting.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The current literature suggests that dexamethasone can be used as an effective alternative to prednisone in the treatment of mild to moderate acute asthma exacerbations in children, with the added benefits of improved compliance, palatability, and cost. However, more research is needed to examine the role of dexamethasone in hospitalized children.", "meshes": ["Administration, Oral", "Anti-Inflammatory Agents", "Asthma", "Child", "Dexamethasone", "Disease Progression", "Glucocorticoids", "Humans", "Prednisone", "Treatment Outcome"], "year": "2014"}
{"id": "pubmedqa_21845457", "dataset": "pubmedqa", "question": "Context: Trauma centers are designated to provide systematized multidisciplinary care to injured patients. Effective trauma systems reduce patient mortality by facilitating the treatment of injured patients at appropriately resourced hospitals. Several U.S. studies report reduced mortality among patients admitted directly to a level I trauma center compared with those admitted to hospitals with less resources. It has yet to be shown whether there is an outcome benefit associated with the \"level of hospital\" initially treating severely injured trauma patients in Australia. This study was designed to determine whether the level of trauma center providing treatment impacts mortality and/or hospital length of stay. Outcomes were evaluated for severely injured trauma patients with an Injury Severity Score (ISS)>15 using NSW Institute of Trauma and Injury Management data from 2002-2007 for our regional health service. To assess the association between trauma centers and binary outcomes, a logistic regression model was used. To assess the association between trauma centers and continuous outcomes, a multivariable linear regression model was used. Sex, age, and ISS were included as covariates in all models. There were 1,986 trauma presentations during the 6-year period. Patients presenting to a level III trauma center had a significantly higher risk of death than those presenting to the level I center, regardless of age, sex, ISS, or prehospital time. Peer review of deaths at the level III center identified problems in care delivery in 15 cases associated with technical errors, delay in decision making, or errors of judgement.\n\nQuestion: Outcomes of severely injured adult trauma patients in an Australian health service: does trauma center level make a difference?", "question_only": "Outcomes of severely injured adult trauma patients in an Australian health service: does trauma center level make a difference?", "context": "Trauma centers are designated to provide systematized multidisciplinary care to injured patients. Effective trauma systems reduce patient mortality by facilitating the treatment of injured patients at appropriately resourced hospitals. Several U.S. studies report reduced mortality among patients admitted directly to a level I trauma center compared with those admitted to hospitals with less resources. It has yet to be shown whether there is an outcome benefit associated with the \"level of hospital\" initially treating severely injured trauma patients in Australia. This study was designed to determine whether the level of trauma center providing treatment impacts mortality and/or hospital length of stay. Outcomes were evaluated for severely injured trauma patients with an Injury Severity Score (ISS)>15 using NSW Institute of Trauma and Injury Management data from 2002-2007 for our regional health service. To assess the association between trauma centers and binary outcomes, a logistic regression model was used. To assess the association between trauma centers and continuous outcomes, a multivariable linear regression model was used. Sex, age, and ISS were included as covariates in all models. There were 1,986 trauma presentations during the 6-year period. Patients presenting to a level III trauma center had a significantly higher risk of death than those presenting to the level I center, regardless of age, sex, ISS, or prehospital time. Peer review of deaths at the level III center identified problems in care delivery in 15 cases associated with technical errors, delay in decision making, or errors of judgement.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Severely injured patients treated at a level III center had a higher mortality rate than those treated at a level I center. Most problems identified occurred in the emergency department and were related to delays in care provision. This research highlights the importance of efficient prehospital, in-hospital, and regional trauma systems, performance monitoring, peer review, and adherence to protocols and guidelines.", "meshes": ["Adolescent", "Adult", "Aged", "Australia", "Female", "Humans", "Injury Severity Score", "Male", "Middle Aged", "Trauma Centers", "Treatment Outcome", "Wounds and Injuries", "Young Adult"], "year": "2011"}
{"id": "pubmedqa_20363841", "dataset": "pubmedqa", "question": "Context: The US Preventive Services Task Force recommends against spirometry in the absence of symptoms. However, as much as 50% of COPD cases in the United States remain undiagnosed. Report of symptoms, smoking history, and spirometric data were collected from subjects screened for a work-related medical evaluation (N = 3,955). Prevalence of airflow obstruction and respiratory symptoms were assessed. Sensitivity, specificity, positive and negative predictive values, and relative risks of predicting symptoms and smoking history for COPD were calculated. Forty-four percent of smokers in our sample had airways obstruction (AO). Of these, 36% reported a diagnosis of or treatment for COPD. Odds ratio (95% CI) for AO with smoking (>or = 20 pack-years) was 3.73 (3.12- 4.45), 1.98 (1.73-2.27) for cough, 1.79 (1.55-2.08) for dyspnea, 1.95 (1.70-2.34) for sputum, and 2.59 (2.26-2.97) for wheeze. Respiratory symptoms were reported by 92% of smokers with AO, 86% smokers with restriction, 76% smokers with normal spirometry, and 73% of nonsmokers. Sensitivity (92% vs 90%), specificity (19% vs 22%), positive (47% vs 40%) and negative (75% vs 80%) predictive values for the presence of one or more symptoms were similar between smokers and all subjects.\n\nQuestion: Do symptoms predict COPD in smokers?", "question_only": "Do symptoms predict COPD in smokers?", "context": "The US Preventive Services Task Force recommends against spirometry in the absence of symptoms. However, as much as 50% of COPD cases in the United States remain undiagnosed. Report of symptoms, smoking history, and spirometric data were collected from subjects screened for a work-related medical evaluation (N = 3,955). Prevalence of airflow obstruction and respiratory symptoms were assessed. Sensitivity, specificity, positive and negative predictive values, and relative risks of predicting symptoms and smoking history for COPD were calculated. Forty-four percent of smokers in our sample had airways obstruction (AO). Of these, 36% reported a diagnosis of or treatment for COPD. Odds ratio (95% CI) for AO with smoking (>or = 20 pack-years) was 3.73 (3.12- 4.45), 1.98 (1.73-2.27) for cough, 1.79 (1.55-2.08) for dyspnea, 1.95 (1.70-2.34) for sputum, and 2.59 (2.26-2.97) for wheeze. Respiratory symptoms were reported by 92% of smokers with AO, 86% smokers with restriction, 76% smokers with normal spirometry, and 73% of nonsmokers. Sensitivity (92% vs 90%), specificity (19% vs 22%), positive (47% vs 40%) and negative (75% vs 80%) predictive values for the presence of one or more symptoms were similar between smokers and all subjects.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "COPD is underdiagnosed in the United States. Symptoms are frequent in subjects with AO and increase their risk for COPD, but add little beyond age and smoking history to the predictive value of spirometry. In view of the high prevalence of symptoms and their poor predictive value, a simpler and more effective approach would be to screen older smokers.", "meshes": ["Aged", "Female", "Humans", "Logistic Models", "Male", "Middle Aged", "Predictive Value of Tests", "Prevalence", "Pulmonary Disease, Chronic Obstructive", "Radiography, Thoracic", "Respiratory Function Tests", "Risk Factors", "Sensitivity and Specificity", "Smoking", "Spirometry", "Surveys and Questionnaires"], "year": "2010"}
{"id": "pubmedqa_7860319", "dataset": "pubmedqa", "question": "Context: We compare 30-day and 180-day postadmission hospital mortality rates for all Medicare patients and those in three categories of cardiac care: coronary artery bypass graft surgery, acute myocardial infarction, and congestive heart failure. DATA SOURCES/ Health Care Financing Administration (HCFA) hospital mortality data for FY 1989. Using hospital level public use files of actual and predicted mortality at 30 and 180 days, we constructed residual mortality measures for each hospital. We ranked hospitals and used receiver operating characteristic (ROC) curves to compare 0-30, 31-180, and 0-180-day postadmission mortality. For the admissions we studied, we found a broad range of hospital performance when we ranked hospitals using the 30-day data; some hospitals had much lower than predicted 30-day mortality rates, while others had much higher than predicted mortality rates. Data from the time period 31-180 days postadmission yield results that corroborate the 0-30 day postadmission data. Moreover, we found evidence that hospital performance on one condition is related to performance on the other conditions, but that the correlation is much weaker in the 31-180-day interval than in the 0-30-day period. Using ROC curves, we found that the 30-day data discriminated the top and bottom fifths of the 180-day data extremely well, especially for AMI outcomes.\n\nQuestion: Measuring hospital mortality rates: are 30-day data enough?", "question_only": "Measuring hospital mortality rates: are 30-day data enough?", "context": "We compare 30-day and 180-day postadmission hospital mortality rates for all Medicare patients and those in three categories of cardiac care: coronary artery bypass graft surgery, acute myocardial infarction, and congestive heart failure. DATA SOURCES/ Health Care Financing Administration (HCFA) hospital mortality data for FY 1989. Using hospital level public use files of actual and predicted mortality at 30 and 180 days, we constructed residual mortality measures for each hospital. We ranked hospitals and used receiver operating characteristic (ROC) curves to compare 0-30, 31-180, and 0-180-day postadmission mortality. For the admissions we studied, we found a broad range of hospital performance when we ranked hospitals using the 30-day data; some hospitals had much lower than predicted 30-day mortality rates, while others had much higher than predicted mortality rates. Data from the time period 31-180 days postadmission yield results that corroborate the 0-30 day postadmission data. Moreover, we found evidence that hospital performance on one condition is related to performance on the other conditions, but that the correlation is much weaker in the 31-180-day interval than in the 0-30-day period. Using ROC curves, we found that the 30-day data discriminated the top and bottom fifths of the 180-day data extremely well, especially for AMI outcomes.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Using data on cumulative hospital mortality from 180 days postadmission does not yield a different perspective from using data from 30 days postadmission for the conditions we studied.", "meshes": ["Cardiology Service, Hospital", "Centers for Medicare and Medicaid Services (U.S.)", "Coronary Artery Bypass", "Forecasting", "Heart Failure", "Hospital Mortality", "Humans", "Medicare", "Myocardial Infarction", "Patient Admission", "ROC Curve", "Survival Rate", "Time Factors", "United States"], "year": "1995"}
{"id": "pubmedqa_21880023", "dataset": "pubmedqa", "question": "Context: To study whether exercise during pregnancy reduces the risk of postnatal depression. Randomized controlled trial. Trondheim and Stavanger University Hospitals, Norway. Eight hundred and fifty-five pregnant women were randomized to intervention or control groups. The intervention was a 12 week exercise program, including aerobic and strengthening exercises, conducted between week 20 and 36 of pregnancy. One weekly group session was led by physiotherapists, and home exercises were encouraged twice a week. Control women received regular antenatal care. Edinburgh Postnatal Depression Scale (EPDS) completed three months after birth. Scores of 10 or more and 13 or more suggested probable minor and major depression, respectively. Fourteen of 379 (3.7%) women in the intervention group and 17 of 340 (5.0%) in the control group had an EPDS score of ≥10 (p=0.46), and four of 379 (1.2%) women in the intervention group and eight of 340 (2.4%) in the control group had an EPDS score of ≥13 (p=0.25). Among women who did not exercise prior to pregnancy, two of 100 (2.0%) women in the intervention group and nine of 95 (9.5%) in the control group had an EPDS score of ≥10 (p=0.03).\n\nQuestion: Does exercise during pregnancy prevent postnatal depression?", "question_only": "Does exercise during pregnancy prevent postnatal depression?", "context": "To study whether exercise during pregnancy reduces the risk of postnatal depression. Randomized controlled trial. Trondheim and Stavanger University Hospitals, Norway. Eight hundred and fifty-five pregnant women were randomized to intervention or control groups. The intervention was a 12 week exercise program, including aerobic and strengthening exercises, conducted between week 20 and 36 of pregnancy. One weekly group session was led by physiotherapists, and home exercises were encouraged twice a week. Control women received regular antenatal care. Edinburgh Postnatal Depression Scale (EPDS) completed three months after birth. Scores of 10 or more and 13 or more suggested probable minor and major depression, respectively. Fourteen of 379 (3.7%) women in the intervention group and 17 of 340 (5.0%) in the control group had an EPDS score of ≥10 (p=0.46), and four of 379 (1.2%) women in the intervention group and eight of 340 (2.4%) in the control group had an EPDS score of ≥13 (p=0.25). Among women who did not exercise prior to pregnancy, two of 100 (2.0%) women in the intervention group and nine of 95 (9.5%) in the control group had an EPDS score of ≥10 (p=0.03).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "We did not find a lower prevalence of high EPDS scores among women randomized to regular exercise during pregnancy compared with the control group. However, a subgroup of women in the intervention group who did not exercise regularly prior to pregnancy had a reduced risk of postnatal depression.", "meshes": ["Adult", "Depression, Postpartum", "Exercise", "Exercise Therapy", "Female", "Humans", "Pregnancy", "Prenatal Care", "Self Report", "Treatment Outcome"], "year": "2012"}
{"id": "pubmedqa_25885219", "dataset": "pubmedqa", "question": "Context: Pregnancy induces adaptations in maternal metabolism to meet the increased need for nutrients by the placenta and fetus. Creatine is an important intracellular metabolite obtained from the diet and also synthesised endogenously. Experimental evidence suggests that the fetus relies on a maternal supply of creatine for much of gestation. However, the impact of pregnancy on maternal creatine homeostasis is unclear. We hypothesise that alteration of maternal creatine homeostasis occurs during pregnancy to ensure adequate levels of this essential substrate are available for maternal tissues, the placenta and fetus. This study aimed to describe maternal creatine homeostasis from mid to late gestation in the precocial spiny mouse. Plasma creatine concentration and urinary excretion were measured from mid to late gestation in pregnant (n = 8) and age-matched virgin female spiny mice (n = 6). At term, body composition and organ weights were assessed and tissue total creatine content determined. mRNA expression of the creatine synthesising enzymes arginine:glycine amidinotransferase (AGAT) and guanidinoacetate methyltransferase (GAMT), and the creatine transporter (CrT1) were assessed by RT-qPCR. Protein expression of AGAT and GAMT was also assessed by western blot analysis. Plasma creatine and renal creatine excretion decreased significantly from mid to late gestation (P<0.001, P<0.05, respectively). Pregnancy resulted in increased lean tissue (P<0.01), kidney (P<0.01), liver (P<0.01) and heart (P<0.05) mass at term. CrT1 expression was increased in the heart (P<0.05) and skeletal muscle (P<0.05) at term compared to non-pregnant tissues, and creatine content of the heart (P<0.05) and kidney (P<0.001) were also increased at this time. CrT1 mRNA expression was down-regulated in the liver (<0.01) and brain (<0.01) of pregnant spiny mice at term. Renal AGAT mRNA (P<0.01) and protein (P<0.05) expression were both significantly up-regulated at term, with decreased expression of AGAT mRNA (<0.01) and GAMT protein (<0.05) observed in the term pregnant heart. Brain AGAT (<0.01) and GAMT (<0.001) mRNA expression were also decreased at term.\n\nQuestion: Maternal creatine homeostasis is altered during gestation in the spiny mouse: is this a metabolic adaptation to pregnancy?", "question_only": "Maternal creatine homeostasis is altered during gestation in the spiny mouse: is this a metabolic adaptation to pregnancy?", "context": "Pregnancy induces adaptations in maternal metabolism to meet the increased need for nutrients by the placenta and fetus. Creatine is an important intracellular metabolite obtained from the diet and also synthesised endogenously. Experimental evidence suggests that the fetus relies on a maternal supply of creatine for much of gestation. However, the impact of pregnancy on maternal creatine homeostasis is unclear. We hypothesise that alteration of maternal creatine homeostasis occurs during pregnancy to ensure adequate levels of this essential substrate are available for maternal tissues, the placenta and fetus. This study aimed to describe maternal creatine homeostasis from mid to late gestation in the precocial spiny mouse. Plasma creatine concentration and urinary excretion were measured from mid to late gestation in pregnant (n = 8) and age-matched virgin female spiny mice (n = 6). At term, body composition and organ weights were assessed and tissue total creatine content determined. mRNA expression of the creatine synthesising enzymes arginine:glycine amidinotransferase (AGAT) and guanidinoacetate methyltransferase (GAMT), and the creatine transporter (CrT1) were assessed by RT-qPCR. Protein expression of AGAT and GAMT was also assessed by western blot analysis. Plasma creatine and renal creatine excretion decreased significantly from mid to late gestation (P<0.001, P<0.05, respectively). Pregnancy resulted in increased lean tissue (P<0.01), kidney (P<0.01), liver (P<0.01) and heart (P<0.05) mass at term. CrT1 expression was increased in the heart (P<0.05) and skeletal muscle (P<0.05) at term compared to non-pregnant tissues, and creatine content of the heart (P<0.05) and kidney (P<0.001) were also increased at this time. CrT1 mRNA expression was down-regulated in the liver (<0.01) and brain (<0.01) of pregnant spiny mice at term. Renal AGAT mRNA (P<0.01) and protein (P<0.05) expression were both significantly up-regulated at term, with decreased expression of AGAT mRNA (<0.01) and GAMT protein (<0.05) observed in the term pregnant heart. Brain AGAT (<0.01) and GAMT (<0.001) mRNA expression were also decreased at term.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Change of maternal creatine status (increased creatine synthesis and reduced creatine excretion) may be a necessary adjustment of maternal physiology to pregnancy to meet the metabolic demands of maternal tissues, the placenta and developing fetus.", "meshes": ["Amidinotransferases", "Animals", "Blotting, Western", "Creatine", "Female", "Gene Expression Regulation", "Guanidinoacetate N-Methyltransferase", "Homeostasis", "Membrane Transport Proteins", "Murinae", "Pregnancy", "RNA, Messenger", "Reverse Transcriptase Polymerase Chain Reaction"], "year": "2015"}
{"id": "pubmedqa_19398929", "dataset": "pubmedqa", "question": "Context: Cholecystectomy for GB polyps that are larger than 10 mm is generally recommended because of the high probability of neoplasm. In contrast, a follow-up strategy is preferred for GB polyps smaller than 10 mm. However, there are no treatment guidelines for polyps that grow in size during the follow-up period.STUDY: We retrospectively investigated 145 patients with GB polyps who underwent at least 1 ultrasonographic follow-up examination over an interval greater than 6 months, before cholecystectomy at Samsung medical center, South Korea, from 1994 to 2007. The growth rate was determined based on the change in size per time interval between 2 ultrasonographic examinations (mm/mo). The median age of the patients was 48 years (range: 25 to 75). One hundred twenty-five non-neoplastic polyps and 20 neoplastic polyps were found. Neoplastic polyps were more frequently found in patients older than 60 years, those with hypertension, a polyp size greater than 10 mm, and a rapid growth rate greater than 0.6 mm/mo. On multivariate analysis, however, the growth rate was not related to the neoplastic nature of a polyp, but older age (>60 y) and large size (>10 mm) were significantly associated with neoplastic polyps.\n\nQuestion: Can the growth rate of a gallbladder polyp predict a neoplastic polyp?", "question_only": "Can the growth rate of a gallbladder polyp predict a neoplastic polyp?", "context": "Cholecystectomy for GB polyps that are larger than 10 mm is generally recommended because of the high probability of neoplasm. In contrast, a follow-up strategy is preferred for GB polyps smaller than 10 mm. However, there are no treatment guidelines for polyps that grow in size during the follow-up period.STUDY: We retrospectively investigated 145 patients with GB polyps who underwent at least 1 ultrasonographic follow-up examination over an interval greater than 6 months, before cholecystectomy at Samsung medical center, South Korea, from 1994 to 2007. The growth rate was determined based on the change in size per time interval between 2 ultrasonographic examinations (mm/mo). The median age of the patients was 48 years (range: 25 to 75). One hundred twenty-five non-neoplastic polyps and 20 neoplastic polyps were found. Neoplastic polyps were more frequently found in patients older than 60 years, those with hypertension, a polyp size greater than 10 mm, and a rapid growth rate greater than 0.6 mm/mo. On multivariate analysis, however, the growth rate was not related to the neoplastic nature of a polyp, but older age (>60 y) and large size (>10 mm) were significantly associated with neoplastic polyps.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Patient's age (>60 y) and large polyp size (>10 mm) were significant predictive factors for neoplastic GB polyps. GB polyps less than 10 mm in diameter do not require surgical intervention simply because they grow.", "meshes": ["Adult", "Age Factors", "Aged", "Chi-Square Distribution", "Cholecystectomy", "Disease Progression", "Female", "Gallbladder Diseases", "Gallbladder Neoplasms", "Humans", "Logistic Models", "Male", "Middle Aged", "Odds Ratio", "Polyps", "Precancerous Conditions", "Republic of Korea", "Retrospective Studies", "Risk Assessment", "Risk Factors", "Time Factors", "Ultrasonography"], "year": "2009"}
{"id": "pubmedqa_9107172", "dataset": "pubmedqa", "question": "Context: If long-term use of left ventricular assist devices (LVADs) as bridges to transplantation is successful, the issue of permanent device implantation in lieu of transplantation could be addressed through the creation of appropriately designed trials. Our medium-term experience with both pneumatically and electrically powered ThermoCardiosystems LVADs is presented to outline the benefits and limitations of device support in lieu of transplantation. Detailed records were kept prospectively for all patients undergoing LVAD insertion. Fifty-eight LVADs were inserted over 5 years, with a survival rate of 74%. Mean patient age was 50 years, and duration of support averaged 98 days. Although common, both preexisting infection and infection during LVAD support were not associated with increased mortality or decreased rate of successful transplantation. Thromboembolic complications were rare, occurring in only three patients (5%) despite the absence of anticoagulation. Ventricular arrhythmias were well tolerated in all patients except in cases of early perioperative right ventricular failure, with no deaths. Right ventricular failure occurred in one third of patients and was managed in a small percentage by right ventricular assist device (RVAD) support and/or inhaled nitric oxide therapy. There were no serious device malfunctions, but five graft-related hemorrhages resulted in two deaths. Finally, a variety of noncardiac surgical procedures were performed in LVAD recipients, with no major morbidity and mortality.\n\nQuestion: Bridge experience with long-term implantable left ventricular assist devices. Are they an alternative to transplantation?", "question_only": "Bridge experience with long-term implantable left ventricular assist devices. Are they an alternative to transplantation?", "context": "If long-term use of left ventricular assist devices (LVADs) as bridges to transplantation is successful, the issue of permanent device implantation in lieu of transplantation could be addressed through the creation of appropriately designed trials. Our medium-term experience with both pneumatically and electrically powered ThermoCardiosystems LVADs is presented to outline the benefits and limitations of device support in lieu of transplantation. Detailed records were kept prospectively for all patients undergoing LVAD insertion. Fifty-eight LVADs were inserted over 5 years, with a survival rate of 74%. Mean patient age was 50 years, and duration of support averaged 98 days. Although common, both preexisting infection and infection during LVAD support were not associated with increased mortality or decreased rate of successful transplantation. Thromboembolic complications were rare, occurring in only three patients (5%) despite the absence of anticoagulation. Ventricular arrhythmias were well tolerated in all patients except in cases of early perioperative right ventricular failure, with no deaths. Right ventricular failure occurred in one third of patients and was managed in a small percentage by right ventricular assist device (RVAD) support and/or inhaled nitric oxide therapy. There were no serious device malfunctions, but five graft-related hemorrhages resulted in two deaths. Finally, a variety of noncardiac surgical procedures were performed in LVAD recipients, with no major morbidity and mortality.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Over all, our medium-term experience with implantable LVAD support is encouraging. Although additional areas of investigation exist, improvements in patients selection and management together with device alterations that have reduced the thromboembolic incidence and facilitated patient rehabilitation lead us to believe that a prospective, randomized trial is indicated to study the role that LVADs may have as an alternative to medical management.", "meshes": ["Aged", "Arrhythmias, Cardiac", "Assisted Circulation", "Cardiomyopathies", "Combined Modality Therapy", "Comorbidity", "Cost-Benefit Analysis", "Endocarditis", "Equipment Design", "Female", "Follow-Up Studies", "Heart Failure", "Heart Transplantation", "Heart-Assist Devices", "Hemorrhage", "Humans", "Infection", "Length of Stay", "Male", "Middle Aged", "Nitric Oxide", "Postoperative Complications", "Prospective Studies", "Prostheses and Implants", "Survival Analysis", "Thromboembolism"], "year": "1997"}
{"id": "pubmedqa_16962519", "dataset": "pubmedqa", "question": "Context: To estimate changes in uterine myoma volume during pregnancy. Review of departmental electronic perinatal database and medical records. Canadian Task Force Classification II-3. Obstetrical ultrasound unit in an academic tertiary care center. One hundred-seven patients diagnosed with uterine myomas during pregnancy and who had two or more obstetrical ultrasounds in different periods of pregnancy. We analyzed the change in volume of uterine myomas between the first half of pregnancy (up until 19 weeks), third quarter (20-30 weeks), and last quarter (31 weeks to term). The volume of largest uterine myoma was calculated using the formula Volume (mm3)=Pi/6x(length mm)x(width mm)x(height mm). The mean age of the population was 31+/-6 years. Between the first and the second study periods, the percentage of uterine myomas that decreased in size was 55.1% (95% CI: 43-66), with a mean decrease in volume of 35%+/-4%; while the percentage of uterine myomas that enlarged was 44.9% (95% CI: 34-56), with a mean increase in volume of 69%+/-11%. Between the second and the third study periods, 75% (95% CI: 56-87) became smaller, with a mean decrease in volume of 30%+/-3%; while 25% (95% CI: 13-43) enlarged, with a mean increase in volume of 102%+/-62%.\n\nQuestion: Volume change of uterine myomas during pregnancy: do myomas really grow?", "question_only": "Volume change of uterine myomas during pregnancy: do myomas really grow?", "context": "To estimate changes in uterine myoma volume during pregnancy. Review of departmental electronic perinatal database and medical records. Canadian Task Force Classification II-3. Obstetrical ultrasound unit in an academic tertiary care center. One hundred-seven patients diagnosed with uterine myomas during pregnancy and who had two or more obstetrical ultrasounds in different periods of pregnancy. We analyzed the change in volume of uterine myomas between the first half of pregnancy (up until 19 weeks), third quarter (20-30 weeks), and last quarter (31 weeks to term). The volume of largest uterine myoma was calculated using the formula Volume (mm3)=Pi/6x(length mm)x(width mm)x(height mm). The mean age of the population was 31+/-6 years. Between the first and the second study periods, the percentage of uterine myomas that decreased in size was 55.1% (95% CI: 43-66), with a mean decrease in volume of 35%+/-4%; while the percentage of uterine myomas that enlarged was 44.9% (95% CI: 34-56), with a mean increase in volume of 69%+/-11%. Between the second and the third study periods, 75% (95% CI: 56-87) became smaller, with a mean decrease in volume of 30%+/-3%; while 25% (95% CI: 13-43) enlarged, with a mean increase in volume of 102%+/-62%.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Contrary to common belief, we found that uterine myomas commonly decrease in volume over the course of pregnancy.", "meshes": ["Adult", "Female", "Humans", "Leiomyoma", "Longitudinal Studies", "Pregnancy", "Pregnancy Complications, Neoplastic", "ROC Curve", "Retrospective Studies", "Tumor Burden", "Ultrasonography, Prenatal", "Uterine Neoplasms"], "year": null}
{"id": "pubmedqa_21346501", "dataset": "pubmedqa", "question": "Context: This study was designed to determine whether preclerkship performance examinations could accurately identify medical students at risk for failing a senior clinical performance examination (CPE). This study used a retrospective case-control, multiyear design, with contingency table analyses, to examine the performance of 412 students in the classes of 2005 to 2010 at a midwestern medical school. During their second year, these students took four CPEs that each used three standardized patient (SP) cases, for a total of 12 cases. The authors correlated each student's average year 2 case score with the student's average case score on a senior (year 4) CPE. Contingency table analysis was carried out using performance on the year 2 CPEs and passing/failing the senior CPE. Similar analyses using each student's United States Medical Licensing Examination (USMLE) Step 1 scores were also performed. Sensitivity, specificity, odds ratio, and relative risk were calculated for two year 2 performance standards. Students' low performances relative to their class on the year 2 CPEs were a strong predictor that they would fail the senior CPE. Their USMLE Step 1 scores also correlated with their performance on the senior CPE, although the predictive values for these scores were considerably weaker.\n\nQuestion: Can students' scores on preclerkship clinical performance examinations predict that they will fail a senior clinical performance examination?", "question_only": "Can students' scores on preclerkship clinical performance examinations predict that they will fail a senior clinical performance examination?", "context": "This study was designed to determine whether preclerkship performance examinations could accurately identify medical students at risk for failing a senior clinical performance examination (CPE). This study used a retrospective case-control, multiyear design, with contingency table analyses, to examine the performance of 412 students in the classes of 2005 to 2010 at a midwestern medical school. During their second year, these students took four CPEs that each used three standardized patient (SP) cases, for a total of 12 cases. The authors correlated each student's average year 2 case score with the student's average case score on a senior (year 4) CPE. Contingency table analysis was carried out using performance on the year 2 CPEs and passing/failing the senior CPE. Similar analyses using each student's United States Medical Licensing Examination (USMLE) Step 1 scores were also performed. Sensitivity, specificity, odds ratio, and relative risk were calculated for two year 2 performance standards. Students' low performances relative to their class on the year 2 CPEs were a strong predictor that they would fail the senior CPE. Their USMLE Step 1 scores also correlated with their performance on the senior CPE, although the predictive values for these scores were considerably weaker.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Under the conditions of this study, preclerkship (year 2) CPEs strongly predicted medical students at risk for failing a senior CPE. This finding opens the opportunity for remediation of deficits prior to or during clerkships.", "meshes": ["Case-Control Studies", "Chi-Square Distribution", "Clinical Clerkship", "Clinical Competence", "Education, Medical, Undergraduate", "Educational Measurement", "Educational Status", "Humans", "Illinois", "Licensure", "Patient Simulation", "Predictive Value of Tests", "Retrospective Studies", "Risk Factors", "Sensitivity and Specificity", "United States"], "year": "2011"}
{"id": "pubmedqa_27288618", "dataset": "pubmedqa", "question": "Context: To determine whether prophylactic inhaled heparin is effective for the prevention and treatment of pneumonia patients receiving mechanical ventilation (MV) in the intensive care unit. A phase 2, double blind randomized controlled trial stratified for study center and patient type (non-operative, post-operative) was conducted in three university-affiliated intensive care units. Patients aged ≥18years and requiring invasive MV for more than 48hours were randomized to usual care, nebulization of unfractionated sodium heparin (5000 units in 2mL) or placebo nebulization with 0.9% sodium chloride (2mL) four times daily with the main outcome measures of the development of ventilator associated pneumonia (VAP), ventilator associated complication (VAC) and sequential organ failure assessment scores in patients with pneumonia on admission or who developed VAP. Australian and New Zealand Clinical Trials Registry ACTRN12612000038897. Two hundred and fourteen patients were enrolled (72 usual care, 71 inhaled sodium heparin, 71 inhaled sodium chloride). There were no differences between treatment groups in terms of the development of VAP, using either Klompas criteria (6-7%, P=1.00) or clinical diagnosis (24-26%, P=0.85). There was no difference in the clinical consistency (P=0.70), number (P=0.28) or the total volume of secretions per day (P=.54). The presence of blood in secretions was significantly less in the usual care group (P=0.005).\n\nQuestion: Is inhaled prophylactic heparin useful for prevention and Management of Pneumonia in ventilated ICU patients?", "question_only": "Is inhaled prophylactic heparin useful for prevention and Management of Pneumonia in ventilated ICU patients?", "context": "To determine whether prophylactic inhaled heparin is effective for the prevention and treatment of pneumonia patients receiving mechanical ventilation (MV) in the intensive care unit. A phase 2, double blind randomized controlled trial stratified for study center and patient type (non-operative, post-operative) was conducted in three university-affiliated intensive care units. Patients aged ≥18years and requiring invasive MV for more than 48hours were randomized to usual care, nebulization of unfractionated sodium heparin (5000 units in 2mL) or placebo nebulization with 0.9% sodium chloride (2mL) four times daily with the main outcome measures of the development of ventilator associated pneumonia (VAP), ventilator associated complication (VAC) and sequential organ failure assessment scores in patients with pneumonia on admission or who developed VAP. Australian and New Zealand Clinical Trials Registry ACTRN12612000038897. Two hundred and fourteen patients were enrolled (72 usual care, 71 inhaled sodium heparin, 71 inhaled sodium chloride). There were no differences between treatment groups in terms of the development of VAP, using either Klompas criteria (6-7%, P=1.00) or clinical diagnosis (24-26%, P=0.85). There was no difference in the clinical consistency (P=0.70), number (P=0.28) or the total volume of secretions per day (P=.54). The presence of blood in secretions was significantly less in the usual care group (P=0.005).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Nebulized heparin cannot be recommended for prophylaxis against VAP or to hasten recovery from pneumonia in patients receiving MV.", "meshes": ["Administration, Inhalation", "Adolescent", "Adult", "Aged", "Aged, 80 and over", "Australia", "Double-Blind Method", "Female", "Fibrinolytic Agents", "Heparin", "Humans", "Intensive Care Units", "Male", "Middle Aged", "Nebulizers and Vaporizers", "New Zealand", "Pneumonia, Ventilator-Associated", "Respiration, Artificial", "Young Adult"], "year": "2016"}
{"id": "pubmedqa_15369037", "dataset": "pubmedqa", "question": "Context: Regular inhalation of tobacco smoke, whether it be voluntary or not, may have profound negative effects on the body. Also intervertebral discs may be affected. The objective of the present study was to test the hypothesis that nurses' aides who were exposed to environmental tobacco smoke (ETS) at home during childhood have an increased risk of long-term sick leave. The sample comprised 5563 Norwegian nurses' aides, not on sick leave when they completed a mailed questionnaire in 1999. Of these, 4744 (85.3%) completed a second questionnaire 15 months later. The outcome measure was the incidence proportion of long-term sick leave during the 12 months prior to the follow-up. Respondents who reported at baseline that they had been exposed to ETS at home during childhood had increased risk of sick leave exceeding 14 days attributed to neck pain (odds ratio (OR) = 1.34; 95% confidence interval (CI): 1.04-1.73), high back pain (OR=1.49; CI: 1.07-2.06), low back pain (OR=1.21; CI: 0.97-1.50), and any illness (OR=1.23; CI: 1.07-1.42), after adjustments for demographic and familial characteristics, former smoking, current smoking, physical leisure-time activities, work factors, prior neck injury, and affective symptoms. They also had increased risk of sick leave exceeding 8 weeks (OR=1.29; CI: 1.08-1.55).\n\nQuestion: Do people who were passive smokers during childhood have increased risk of long-term work disability?", "question_only": "Do people who were passive smokers during childhood have increased risk of long-term work disability?", "context": "Regular inhalation of tobacco smoke, whether it be voluntary or not, may have profound negative effects on the body. Also intervertebral discs may be affected. The objective of the present study was to test the hypothesis that nurses' aides who were exposed to environmental tobacco smoke (ETS) at home during childhood have an increased risk of long-term sick leave. The sample comprised 5563 Norwegian nurses' aides, not on sick leave when they completed a mailed questionnaire in 1999. Of these, 4744 (85.3%) completed a second questionnaire 15 months later. The outcome measure was the incidence proportion of long-term sick leave during the 12 months prior to the follow-up. Respondents who reported at baseline that they had been exposed to ETS at home during childhood had increased risk of sick leave exceeding 14 days attributed to neck pain (odds ratio (OR) = 1.34; 95% confidence interval (CI): 1.04-1.73), high back pain (OR=1.49; CI: 1.07-2.06), low back pain (OR=1.21; CI: 0.97-1.50), and any illness (OR=1.23; CI: 1.07-1.42), after adjustments for demographic and familial characteristics, former smoking, current smoking, physical leisure-time activities, work factors, prior neck injury, and affective symptoms. They also had increased risk of sick leave exceeding 8 weeks (OR=1.29; CI: 1.08-1.55).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The study supports the hypothesis that nurses' aides who were exposed to ETS at home during childhood have an increased risk of long-term sickness absence.", "meshes": ["Adult", "Affective Symptoms", "Age Factors", "Aged", "Confidence Intervals", "Exercise", "Female", "Follow-Up Studies", "Humans", "Leisure Activities", "Low Back Pain", "Male", "Middle Aged", "Neck Injuries", "Neck Pain", "Norway", "Nurses' Aides", "Odds Ratio", "Prospective Studies", "Risk Factors", "Sick Leave", "Surveys and Questionnaires", "Time Factors", "Tobacco Smoke Pollution"], "year": "2004"}
{"id": "pubmedqa_8375607", "dataset": "pubmedqa", "question": "Context: Previous studies reported that breast-feeding protects children against a variety of diseases, but these studies were generally conducted on \"high-risk\" or hospitalized children. This paper describes the results of our study on the effects of breast-feeding on rate of illness in normal children with a family history of atopy. A historic cohort approach of 794 children with a family history of atopy was used to assess the effects of breast-feeding on illness rates. Family history of atopy was based on allergic diseases in family members as registered by the family physician. Illness data from birth onwards were available from the Continuous Morbidity Registration of the Department of Family Medicine. Information on breast-feeding was collected by postal questionnaire. We then compared rates of illness between children with a family history of atopy who were and who were not breast-fed. Breast-feeding was related to lower levels of childhood illness both in the first and the first three years of life. In the first year of life they had fewer episodes of gastroenteritis, lower respiratory tract infections, and digestive tract disorders. Over the next three years of life they had fewer respiratory tract infections and skin infections.\n\nQuestion: Is the breast best for children with a family history of atopy?", "question_only": "Is the breast best for children with a family history of atopy?", "context": "Previous studies reported that breast-feeding protects children against a variety of diseases, but these studies were generally conducted on \"high-risk\" or hospitalized children. This paper describes the results of our study on the effects of breast-feeding on rate of illness in normal children with a family history of atopy. A historic cohort approach of 794 children with a family history of atopy was used to assess the effects of breast-feeding on illness rates. Family history of atopy was based on allergic diseases in family members as registered by the family physician. Illness data from birth onwards were available from the Continuous Morbidity Registration of the Department of Family Medicine. Information on breast-feeding was collected by postal questionnaire. We then compared rates of illness between children with a family history of atopy who were and who were not breast-fed. Breast-feeding was related to lower levels of childhood illness both in the first and the first three years of life. In the first year of life they had fewer episodes of gastroenteritis, lower respiratory tract infections, and digestive tract disorders. Over the next three years of life they had fewer respiratory tract infections and skin infections.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Our results suggest a protective effect of breast-feeding among children with a family history of atopy that is not confined to the period of breast-feeding but continues during the first three years of life. Breast-feeding should be promoted in children with a family history of atopy.", "meshes": ["Breast Feeding", "Cohort Studies", "Humans", "Hypersensitivity, Immediate", "Infant, Newborn", "Morbidity", "Risk Factors"], "year": null}
{"id": "pubmedqa_22680064", "dataset": "pubmedqa", "question": "Context: To determine the ability of early sonogram to predict the presentation of twin A at birth. A retrospective cohort study was conducted on all twin pregnancies evaluated at our Fetal Evaluation Unit from 2007 to 2009. Sonogram records were reviewed for the presentation of twin A at seven gestational age intervals and inpatient medical records were reviewed for the presentation of twin A at delivery. The positive predictive value, sensitivity, and specificity of presentation as determined by ultrasound, at each gestational age interval, for the same presentation at delivery were calculated. Two hundred and thirty-eight twin pregnancies met inclusion criteria. A total of 896 ultrasounds were reviewed. The positive predictive value of cephalic presentation of twin A as determined by ultrasound for the persistence of cephalic presentation at delivery reached 95% after 28 weeks gestation. The positive predictive value for noncephalic presentation as established by sonogram for noncephalic at delivery was>90% after 32 weeks gestation.\n\nQuestion: Can third trimester ultrasound predict the presentation of the first twin at delivery?", "question_only": "Can third trimester ultrasound predict the presentation of the first twin at delivery?", "context": "To determine the ability of early sonogram to predict the presentation of twin A at birth. A retrospective cohort study was conducted on all twin pregnancies evaluated at our Fetal Evaluation Unit from 2007 to 2009. Sonogram records were reviewed for the presentation of twin A at seven gestational age intervals and inpatient medical records were reviewed for the presentation of twin A at delivery. The positive predictive value, sensitivity, and specificity of presentation as determined by ultrasound, at each gestational age interval, for the same presentation at delivery were calculated. Two hundred and thirty-eight twin pregnancies met inclusion criteria. A total of 896 ultrasounds were reviewed. The positive predictive value of cephalic presentation of twin A as determined by ultrasound for the persistence of cephalic presentation at delivery reached 95% after 28 weeks gestation. The positive predictive value for noncephalic presentation as established by sonogram for noncephalic at delivery was>90% after 32 weeks gestation.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The presentation of the first twin at delivery can be determined by sonogram by the 32nd week of gestation in over 90% of twin pregnancies.", "meshes": ["Adult", "Birth Order", "Birth Weight", "Cohort Studies", "Delivery, Obstetric", "Female", "Humans", "Infant, Newborn", "Labor Presentation", "Predictive Value of Tests", "Pregnancy", "Pregnancy Trimester, Third", "Pregnancy, Twin", "Retrospective Studies", "Twins", "Ultrasonography, Prenatal"], "year": "2012"}
{"id": "pubmedqa_23240452", "dataset": "pubmedqa", "question": "Context: In recent years the role of trace elements in lithogenesis has received steadily increasing attention. This study was aimed to attempt to find the correlations between the chemical content of the stones and the concentration of chosen elements in the urine and hair of stone formers. The proposal for the study was approved by the local ethics committee. Specimens were taken from 219 consecutive stone-formers. The content of the stone was evaluated using atomic absorption spectrometry, spectrophotometry, and colorimetric methods. An analysis of 29 elements in hair and 21 elements in urine was performed using inductively coupled plasma-atomic emission spectrometry. Only a few correlations between the composition of stones and the distribution of elements in urine and in hair were found. All were considered incidental.\n\nQuestion: Can we predict urinary stone composition based on an analysis of microelement concentration in the hair and urine?", "question_only": "Can we predict urinary stone composition based on an analysis of microelement concentration in the hair and urine?", "context": "In recent years the role of trace elements in lithogenesis has received steadily increasing attention. This study was aimed to attempt to find the correlations between the chemical content of the stones and the concentration of chosen elements in the urine and hair of stone formers. The proposal for the study was approved by the local ethics committee. Specimens were taken from 219 consecutive stone-formers. The content of the stone was evaluated using atomic absorption spectrometry, spectrophotometry, and colorimetric methods. An analysis of 29 elements in hair and 21 elements in urine was performed using inductively coupled plasma-atomic emission spectrometry. Only a few correlations between the composition of stones and the distribution of elements in urine and in hair were found. All were considered incidental.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The data obtained did not allow for the creation of a proper and practical algorithm to predict stone chemical composition based on hair and urine analysis.", "meshes": ["Adult", "Hair", "Humans", "Middle Aged", "Spectrophotometry", "Trace Elements", "Urinary Calculi", "Urine"], "year": null}
{"id": "pubmedqa_19401574", "dataset": "pubmedqa", "question": "Context: To evaluate the diagnostic accuracy of gadofosveset-enhanced magnetic resonance (MR) angiography in the assessment of carotid artery stenosis, with digital subtraction angiography (DSA) as the reference standard, and to determine the value of reading first-pass, steady-state, and \"combined\" (first-pass plus steady-state) MR angiograms. This study was approved by the local ethics committee, and all subjects gave written informed consent. MR angiography and DSA were performed in 84 patients (56 men, 28 women; age range, 61-76 years) with carotid artery stenosis at Doppler ultrasonography. Three readers reviewed the first-pass, steady-state, and combined MR data sets, and one independent observer evaluated the DSA images to assess stenosis degree, plaque morphology and ulceration, stenosis length, and tandem lesions. Interobserver agreement regarding MR angiographic findings was analyzed by using intraclass correlation and Cohen kappa coefficients. Sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were calculated by using the McNemar test to determine possible significant differences (P<.05). Interobserver agreement regarding all MR angiogram readings was substantial. For grading stenosis, sensitivity, specificity, PPV, and NPV were, respectively, 90%, 92%, 91%, and 91% for first-pass imaging; 95% each for steady-state imaging; and 96%, 99%, 99%, and 97% for combined imaging. For evaluation of plaque morphology, respective values were 84%, 86%, 88%, and 82% for first-pass imaging; 98%, 97%, 98%, and 97% for steady-state imaging; and 98%, 100%, 100%, and 97% for combined imaging. Differences between the first-pass, steady-state, and combined image readings for assessment of stenosis degree and plaque morphology were significant (P<.001).\n\nQuestion: Gadofosveset-enhanced MR angiography of carotid arteries: does steady-state imaging improve accuracy of first-pass imaging?", "question_only": "Gadofosveset-enhanced MR angiography of carotid arteries: does steady-state imaging improve accuracy of first-pass imaging?", "context": "To evaluate the diagnostic accuracy of gadofosveset-enhanced magnetic resonance (MR) angiography in the assessment of carotid artery stenosis, with digital subtraction angiography (DSA) as the reference standard, and to determine the value of reading first-pass, steady-state, and \"combined\" (first-pass plus steady-state) MR angiograms. This study was approved by the local ethics committee, and all subjects gave written informed consent. MR angiography and DSA were performed in 84 patients (56 men, 28 women; age range, 61-76 years) with carotid artery stenosis at Doppler ultrasonography. Three readers reviewed the first-pass, steady-state, and combined MR data sets, and one independent observer evaluated the DSA images to assess stenosis degree, plaque morphology and ulceration, stenosis length, and tandem lesions. Interobserver agreement regarding MR angiographic findings was analyzed by using intraclass correlation and Cohen kappa coefficients. Sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were calculated by using the McNemar test to determine possible significant differences (P<.05). Interobserver agreement regarding all MR angiogram readings was substantial. For grading stenosis, sensitivity, specificity, PPV, and NPV were, respectively, 90%, 92%, 91%, and 91% for first-pass imaging; 95% each for steady-state imaging; and 96%, 99%, 99%, and 97% for combined imaging. For evaluation of plaque morphology, respective values were 84%, 86%, 88%, and 82% for first-pass imaging; 98%, 97%, 98%, and 97% for steady-state imaging; and 98%, 100%, 100%, and 97% for combined imaging. Differences between the first-pass, steady-state, and combined image readings for assessment of stenosis degree and plaque morphology were significant (P<.001).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Gadofosveset-enhanced MR angiography is a promising technique for imaging carotid artery stenosis. Steady-state image reading is superior to first-pass image reading, but the combined reading protocol is more accurate.", "meshes": ["Aged", "Angiography, Digital Subtraction", "Carotid Stenosis", "Contrast Media", "Female", "Gadolinium", "Humans", "Image Enhancement", "Magnetic Resonance Angiography", "Male", "Middle Aged", "Organometallic Compounds", "Reproducibility of Results", "Sensitivity and Specificity"], "year": "2009"}
{"id": "pubmedqa_26852225", "dataset": "pubmedqa", "question": "Context: Anchoring vignettes are brief texts describing a hypothetical character who illustrates a certain fixed level of a trait under evaluation. This research uses vignettes to elucidate factors associated with sleep disorders in adult Japanese before and after adjustment for reporting heterogeneity in self-reports. This study also evaluates the need for adjusting for reporting heterogeneity in the management of sleep and energy related problems in Japan. We investigated a dataset of 1002 respondents aged 18 years and over from the Japanese World Health Survey, which collected information through face-to-face interview from 2002 to 2003. The ordered probit model and the Compound Hierarchical Ordered Probit (CHOPIT) model, which incorporated anchoring vignettes, were employed to estimate and compare associations of sleep and energy with socio-demographic and life-style factors before and after adjustment for differences in response category cut-points for each individual. The prevalence of self-reported problems with sleep and energy was 53 %. Without correction of cut-point shifts, age, sex, and the number of comorbidities were significantly associated with a greater severity of sleep-related problems. After correction, age, the number of comorbidities, and regular exercise were significantly associated with a greater severity of sleep-related problems; sex was no longer a significant factor. Compared to the ordered probit model, the CHOPIT model provided two changes with a subtle difference in the magnitude of regression coefficients after correction for reporting heterogeneity.\n\nQuestion: Is adjustment for reporting heterogeneity necessary in sleep disorders?", "question_only": "Is adjustment for reporting heterogeneity necessary in sleep disorders?", "context": "Anchoring vignettes are brief texts describing a hypothetical character who illustrates a certain fixed level of a trait under evaluation. This research uses vignettes to elucidate factors associated with sleep disorders in adult Japanese before and after adjustment for reporting heterogeneity in self-reports. This study also evaluates the need for adjusting for reporting heterogeneity in the management of sleep and energy related problems in Japan. We investigated a dataset of 1002 respondents aged 18 years and over from the Japanese World Health Survey, which collected information through face-to-face interview from 2002 to 2003. The ordered probit model and the Compound Hierarchical Ordered Probit (CHOPIT) model, which incorporated anchoring vignettes, were employed to estimate and compare associations of sleep and energy with socio-demographic and life-style factors before and after adjustment for differences in response category cut-points for each individual. The prevalence of self-reported problems with sleep and energy was 53 %. Without correction of cut-point shifts, age, sex, and the number of comorbidities were significantly associated with a greater severity of sleep-related problems. After correction, age, the number of comorbidities, and regular exercise were significantly associated with a greater severity of sleep-related problems; sex was no longer a significant factor. Compared to the ordered probit model, the CHOPIT model provided two changes with a subtle difference in the magnitude of regression coefficients after correction for reporting heterogeneity.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Sleep disorders are common in the general adult population of Japan. Correction for reporting heterogeneity using anchoring vignettes is not a necessary tool for proper management of sleep and energy related problems among Japanese adults. Older age, gender differences in communicating sleep-related problems, the presence of multiple morbidities, and regular exercise should be the focus of policies and clinical practice to improve sleep and energy management in Japan.", "meshes": ["Adult", "Aged", "Female", "Health Status Disparities", "Health Surveys", "Humans", "Japan", "Male", "Middle Aged", "Physical Fitness", "Prevalence", "Self Report", "Self-Assessment", "Sleep Wake Disorders", "Socioeconomic Factors"], "year": "2016"}
{"id": "pubmedqa_27858166", "dataset": "pubmedqa", "question": "Context: Traumatic aortic injury (TAI) is a rare but life-threatening type of injury. We investigate whether the anatomy of the aortic arch influences the severity of aortic injury. This is a retrospective study of twenty-two cases treated with TEVAR for TAI in our department from 2009 to 2014. Aortic injury was assessed in accordance with the recommendations of the Society of Vascular Surgery. We measured the aortic arch angle and the aortic arch index, based on the initial angio-CT scan, in each of the analyzed cases. The mean aortic arch index and mean aortic arch angle were 6.8 cm and 58.3°, respectively, in the type I injury group; 4.4 cm and 45.9° in the type III group; 3.3 cm and 37° in the type IV group. There were substantial differences in both the aortic arch index and the aortic arch angle of the type III and IV groups. A multivariate analysis confirmed that the aortic arch angle was significantly associated with the occurrence of type III damage (OR 1.5; 95% CI 1.03-2.2).\n\nQuestion: Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?", "question_only": "Traumatic aortic injury: does the anatomy of the aortic arch influence aortic trauma severity?", "context": "Traumatic aortic injury (TAI) is a rare but life-threatening type of injury. We investigate whether the anatomy of the aortic arch influences the severity of aortic injury. This is a retrospective study of twenty-two cases treated with TEVAR for TAI in our department from 2009 to 2014. Aortic injury was assessed in accordance with the recommendations of the Society of Vascular Surgery. We measured the aortic arch angle and the aortic arch index, based on the initial angio-CT scan, in each of the analyzed cases. The mean aortic arch index and mean aortic arch angle were 6.8 cm and 58.3°, respectively, in the type I injury group; 4.4 cm and 45.9° in the type III group; 3.3 cm and 37° in the type IV group. There were substantial differences in both the aortic arch index and the aortic arch angle of the type III and IV groups. A multivariate analysis confirmed that the aortic arch angle was significantly associated with the occurrence of type III damage (OR 1.5; 95% CI 1.03-2.2).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The severity of TAI is influenced by the sharpness of the aortic arch. There is an inverse relationship between the severity of aortic injury and the aortic arch index.", "meshes": ["Adult", "Aorta", "Aorta, Thoracic", "Endovascular Procedures", "Female", "Humans", "Male", "Middle Aged", "Multivariate Analysis", "Retrospective Studies", "Stents", "Thoracic Injuries", "Tomography, X-Ray Computed", "Trauma Severity Indices", "Treatment Outcome", "Young Adult"], "year": "2017"}
{"id": "pubmedqa_22427593", "dataset": "pubmedqa", "question": "Context: The purpose of this study is to measure the accuracy and reliability of normally sighted, visually impaired, and blind pedestrians at making street crossing decisions using visual and/or auditory information. Using a 5-point rating scale, safety ratings for vehicular gaps of different durations were measured along a two-lane street of one-way traffic without a traffic signal. Safety ratings were collected from 12 normally sighted, 10 visually impaired, and 10 blind subjects for eight different gap times under three sensory conditions: (1) visual plus auditory information, (2) visual information only, and (3) auditory information only. Accuracy and reliability in street crossing decision-making were calculated for each subject under each sensory condition. We found that normally sighted and visually impaired pedestrians were accurate and reliable in their street crossing decision-making ability when using either vision plus hearing or vision only (P>0.05). Under the hearing only condition, all subjects were reliable (P>0.05) but inaccurate with their street crossing decisions (P<0.05). Compared to either the normally sighted (P = 0.018) or visually impaired subjects (P = 0.019), blind subjects were the least accurate with their street crossing decisions under the hearing only condition.\n\nQuestion: Are normally sighted, visually impaired, and blind pedestrians accurate and reliable at making street crossing decisions?", "question_only": "Are normally sighted, visually impaired, and blind pedestrians accurate and reliable at making street crossing decisions?", "context": "The purpose of this study is to measure the accuracy and reliability of normally sighted, visually impaired, and blind pedestrians at making street crossing decisions using visual and/or auditory information. Using a 5-point rating scale, safety ratings for vehicular gaps of different durations were measured along a two-lane street of one-way traffic without a traffic signal. Safety ratings were collected from 12 normally sighted, 10 visually impaired, and 10 blind subjects for eight different gap times under three sensory conditions: (1) visual plus auditory information, (2) visual information only, and (3) auditory information only. Accuracy and reliability in street crossing decision-making were calculated for each subject under each sensory condition. We found that normally sighted and visually impaired pedestrians were accurate and reliable in their street crossing decision-making ability when using either vision plus hearing or vision only (P>0.05). Under the hearing only condition, all subjects were reliable (P>0.05) but inaccurate with their street crossing decisions (P<0.05). Compared to either the normally sighted (P = 0.018) or visually impaired subjects (P = 0.019), blind subjects were the least accurate with their street crossing decisions under the hearing only condition.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Our data suggested that visually impaired pedestrians can make accurate and reliable street crossing decisions like those of normally sighted pedestrians. When using auditory information only, all subjects significantly overestimated the vehicular gap time. Our finding that blind pedestrians performed significantly worse than either the normally sighted or visually impaired subjects under the hearing only condition suggested that they may benefit from training to improve their detection ability and/or interpretation of vehicular gap times.", "meshes": ["Adult", "Automobiles", "Blindness", "Contrast Sensitivity", "Humans", "Middle Aged", "Psychomotor Performance", "Reproducibility of Results", "Safety", "Sensory Aids", "Signal Detection, Psychological", "Vision, Low", "Visual Acuity", "Visually Impaired Persons", "Walking"], "year": "2012"}
{"id": "pubmedqa_26348845", "dataset": "pubmedqa", "question": "Context: Rapid prescreening (RPS) is one of the quality assurance (QA) methods used in gynecologic cytology. The efficacy of RPS has been previously studied but mostly with respect to squamous lesions; in fact, there has been no study so far specifically looking at the sensitivity of RPS for detecting glandular cell abnormalities. A total of 80,565 Papanicolaou (Pap) smears underwent RPS during a 25-month period. A sample was designated as \"review for abnormality\" (R) if any abnormal cells (at the threshold of atypical squamous cells of undetermined significance/atypical glandular cells [AGC]) were thought to be present or was designated as negative (N) if none were detected. Each sample then underwent full screening (FS) and was designated as either R or N and also given a cytologic interpretation. The final cytologic interpretation was a glandular cell abnormality (≥AGC) in 107 samples (0.13%); 39 of these (36.4%) were flagged as R on RPS. Twenty-four patients (33.8%) out of 71 who had histologic follow-up were found to harbor a high-grade squamous intraepithelial lesion or carcinoma; 13 of those 24 Pap smears (54.2%) had been flagged as R on RPS. Notably, 11 AGC cases were picked up by RPS only and not by FS and represented false-negative cases; 2 of these showed endometrial adenocarcinoma on histologic follow-up.\n\nQuestion: Pap smears with glandular cell abnormalities: Are they detected by rapid prescreening?", "question_only": "Pap smears with glandular cell abnormalities: Are they detected by rapid prescreening?", "context": "Rapid prescreening (RPS) is one of the quality assurance (QA) methods used in gynecologic cytology. The efficacy of RPS has been previously studied but mostly with respect to squamous lesions; in fact, there has been no study so far specifically looking at the sensitivity of RPS for detecting glandular cell abnormalities. A total of 80,565 Papanicolaou (Pap) smears underwent RPS during a 25-month period. A sample was designated as \"review for abnormality\" (R) if any abnormal cells (at the threshold of atypical squamous cells of undetermined significance/atypical glandular cells [AGC]) were thought to be present or was designated as negative (N) if none were detected. Each sample then underwent full screening (FS) and was designated as either R or N and also given a cytologic interpretation. The final cytologic interpretation was a glandular cell abnormality (≥AGC) in 107 samples (0.13%); 39 of these (36.4%) were flagged as R on RPS. Twenty-four patients (33.8%) out of 71 who had histologic follow-up were found to harbor a high-grade squamous intraepithelial lesion or carcinoma; 13 of those 24 Pap smears (54.2%) had been flagged as R on RPS. Notably, 11 AGC cases were picked up by RPS only and not by FS and represented false-negative cases; 2 of these showed endometrial adenocarcinoma on histologic follow-up.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Pap smears with glandular cell abnormalities are often flagged as abnormal by RPS, and this results in a sensitivity of 36.4% (at the AGC threshold). Most importantly, some cases of AGC are detected on Pap smears by RPS only, and this demonstrates that RPS is a valuable QA method.", "meshes": ["Adenocarcinoma", "Adult", "Aged", "Aged, 80 and over", "Female", "Humans", "Mass Screening", "Middle Aged", "Papanicolaou Test", "Sensitivity and Specificity", "Uterine Cervical Neoplasms", "Vaginal Smears", "Young Adult"], "year": "2015"}
{"id": "pubmedqa_27909738", "dataset": "pubmedqa", "question": "Context: Using high-quality CT-on-rails imaging, the daily motion of the prostate bed clinical target volume (PB-CTV) based on consensus Radiation Therapy Oncology Group (RTOG) definitions (instead of surgical clips/fiducials) was studied. It was assessed whether PB motion in the superior portion of PB-CTV (SUP-CTV) differed from the inferior PB-CTV (INF-CTV). Eight pT2-3bN0-1M0 patients underwent postprostatectomy intensity-modulated radiotherapy, totaling 300 fractions. INF-CTV and SUP-CTV were defined as PB-CTV located inferior and superior to the superior border of the pubic symphysis, respectively. Daily pretreatment CT-on-rails images were compared to the planning CT in the left-right (LR), superoinferior (SI), and anteroposterior (AP) directions. Two parameters were defined: \"total PB-CTV motion\" represented total shifts from skin tattoos to RTOG-defined anatomic areas; \"PB-CTV target motion\" (performed for both SUP-CTV and INF-CTV) represented shifts from bone to RTOG-defined anatomic areas (i. e., subtracting shifts from skin tattoos to bone). Mean (± standard deviation, SD) total PB-CTV motion was -1.5 (± 6.0), 1.3 (± 4.5), and 3.7 (± 5.7) mm in LR, SI, and AP directions, respectively. Mean (± SD) PB-CTV target motion was 0.2 (±1.4), 0.3 (±2.4), and 0 (±3.1) mm in the LR, SI, and AP directions, respectively. Mean (± SD) INF-CTV target motion was 0.1 (± 2.8), 0.5 (± 2.2), and 0.2 (± 2.5) mm, and SUP-CTV target motion was 0.3 (± 1.8), 0.5 (± 2.3), and 0 (± 5.0) mm in LR, SI, and AP directions, respectively. No statistically significant differences between INF-CTV and SUP-CTV motion were present in any direction.\n\nQuestion: Prostate bed target interfractional motion using RTOG consensus definitions and daily CT on rails : Does target motion differ between superior and inferior portions of the clinical target volume?", "question_only": "Prostate bed target interfractional motion using RTOG consensus definitions and daily CT on rails : Does target motion differ between superior and inferior portions of the clinical target volume?", "context": "Using high-quality CT-on-rails imaging, the daily motion of the prostate bed clinical target volume (PB-CTV) based on consensus Radiation Therapy Oncology Group (RTOG) definitions (instead of surgical clips/fiducials) was studied. It was assessed whether PB motion in the superior portion of PB-CTV (SUP-CTV) differed from the inferior PB-CTV (INF-CTV). Eight pT2-3bN0-1M0 patients underwent postprostatectomy intensity-modulated radiotherapy, totaling 300 fractions. INF-CTV and SUP-CTV were defined as PB-CTV located inferior and superior to the superior border of the pubic symphysis, respectively. Daily pretreatment CT-on-rails images were compared to the planning CT in the left-right (LR), superoinferior (SI), and anteroposterior (AP) directions. Two parameters were defined: \"total PB-CTV motion\" represented total shifts from skin tattoos to RTOG-defined anatomic areas; \"PB-CTV target motion\" (performed for both SUP-CTV and INF-CTV) represented shifts from bone to RTOG-defined anatomic areas (i. e., subtracting shifts from skin tattoos to bone). Mean (± standard deviation, SD) total PB-CTV motion was -1.5 (± 6.0), 1.3 (± 4.5), and 3.7 (± 5.7) mm in LR, SI, and AP directions, respectively. Mean (± SD) PB-CTV target motion was 0.2 (±1.4), 0.3 (±2.4), and 0 (±3.1) mm in the LR, SI, and AP directions, respectively. Mean (± SD) INF-CTV target motion was 0.1 (± 2.8), 0.5 (± 2.2), and 0.2 (± 2.5) mm, and SUP-CTV target motion was 0.3 (± 1.8), 0.5 (± 2.3), and 0 (± 5.0) mm in LR, SI, and AP directions, respectively. No statistically significant differences between INF-CTV and SUP-CTV motion were present in any direction.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "There are no statistically apparent motion differences between SUP-CTV and INF-CTV. Current uniform planning target volume (PTV) margins are adequate to cover both portions of the CTV.", "meshes": ["Aged", "Artifacts", "Guideline Adherence", "Humans", "Male", "Middle Aged", "Motion", "Patient Positioning", "Practice Guidelines as Topic", "Radiation Oncology", "Radiotherapy Dosage", "Radiotherapy Planning, Computer-Assisted", "Radiotherapy, Image-Guided", "Radiotherapy, Intensity-Modulated", "Reproducibility of Results", "Sensitivity and Specificity", "Tomography, X-Ray Computed", "Treatment Outcome", "Tumor Burden", "United States"], "year": "2017"}
{"id": "pubmedqa_25255719", "dataset": "pubmedqa", "question": "Context: This prospective case-control study consisted of 33 patients with pre-eclampsia and 32 normotensive pregnant patients as controls. All of the subjects underwent otoscopic examinations - pure tone audiometry (0.25-16 kHz) and transient evoked otoacoustic emission (1-4 kHz) tests - during their third trimester of pregnancy. The mean ages of the patients with pre-eclampsia and the control subjects were 29.6 ± 5.7 and 28.6 ± 5.3 years, respectively. The baseline demographic characteristics, including age, gravidity, parity number, and gestational week, were similar between the two patient groups. Hearing thresholds in the right ear at 1, 4, 8, and 10 kHz and in the left ear at 8 and 10 kHz were significantly higher in the patients with pre-eclampsia compared to the control subjects. The degree of systolic blood pressure measured at the time of diagnosis had a deteriorating effect on hearing at 8, 10, and 12 kHz in the right ear and at 10 kHz in the left ear.\n\nQuestion: Hearing loss: an unknown complication of pre-eclampsia?", "question_only": "Hearing loss: an unknown complication of pre-eclampsia?", "context": "This prospective case-control study consisted of 33 patients with pre-eclampsia and 32 normotensive pregnant patients as controls. All of the subjects underwent otoscopic examinations - pure tone audiometry (0.25-16 kHz) and transient evoked otoacoustic emission (1-4 kHz) tests - during their third trimester of pregnancy. The mean ages of the patients with pre-eclampsia and the control subjects were 29.6 ± 5.7 and 28.6 ± 5.3 years, respectively. The baseline demographic characteristics, including age, gravidity, parity number, and gestational week, were similar between the two patient groups. Hearing thresholds in the right ear at 1, 4, 8, and 10 kHz and in the left ear at 8 and 10 kHz were significantly higher in the patients with pre-eclampsia compared to the control subjects. The degree of systolic blood pressure measured at the time of diagnosis had a deteriorating effect on hearing at 8, 10, and 12 kHz in the right ear and at 10 kHz in the left ear.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Pre-eclampsia is a potential risk factor for cochlear damage and sensorineural hearing loss. Further studies that include routine audiological examinations are needed in these patients.", "meshes": ["Adult", "Auditory Threshold", "Blood Pressure", "Case-Control Studies", "Cochlea", "Female", "Hearing Loss, Sensorineural", "Humans", "Pre-Eclampsia", "Pregnancy", "Prospective Studies", "Risk Factors", "Systole", "Young Adult"], "year": "2015"}
{"id": "pubmedqa_27884344", "dataset": "pubmedqa", "question": "Context: This study sought to compare general surgery research residents' survey information regarding self-efficacy ratings to their observed performance during a simulated small bowel repair. Their observed performance ratings were based on their leadership skills in directing their assistant. Participants were given 15 min to perform a bowel repair using bovine intestines with standardized injuries. Operative assistants were assigned to help assist with the repair. Before the procedure, participants were asked to rate their expected skills decay, task difficulty, and confidence in addressing the small bowel injury. Interactions were coded to identify the number of instructions given by the participants to the assistant during the repair. Statistical analyses assessed the relationship between the number of directional instructions and participants' perceptions self-efficacy measures. Directional instructions were defined as any dialog by the participant who guided the assistant to perform an action. Thirty-six residents (58.3% female) participated in the study. Participants who rated lower levels of decay in their intraoperative decision-making and small bowel repair skills were noted to use their assistant more by giving more instructions. Similarly, a higher number of instructions correlated with lower perceived difficulty in selecting the correct suture, suture pattern, and completing the entire surgical task.\n\nQuestion: Do resident's leadership skills relate to ratings of technical skill?", "question_only": "Do resident's leadership skills relate to ratings of technical skill?", "context": "This study sought to compare general surgery research residents' survey information regarding self-efficacy ratings to their observed performance during a simulated small bowel repair. Their observed performance ratings were based on their leadership skills in directing their assistant. Participants were given 15 min to perform a bowel repair using bovine intestines with standardized injuries. Operative assistants were assigned to help assist with the repair. Before the procedure, participants were asked to rate their expected skills decay, task difficulty, and confidence in addressing the small bowel injury. Interactions were coded to identify the number of instructions given by the participants to the assistant during the repair. Statistical analyses assessed the relationship between the number of directional instructions and participants' perceptions self-efficacy measures. Directional instructions were defined as any dialog by the participant who guided the assistant to perform an action. Thirty-six residents (58.3% female) participated in the study. Participants who rated lower levels of decay in their intraoperative decision-making and small bowel repair skills were noted to use their assistant more by giving more instructions. Similarly, a higher number of instructions correlated with lower perceived difficulty in selecting the correct suture, suture pattern, and completing the entire surgical task.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "General surgery research residents' intraoperative leadership skills showed significant correlations to their perceptions of skill decay and task difficulty during a bowel repair. Evaluating resident's directional instructions may provide an additional individualized intraoperative assessment metric. Further evaluation relating to operative performance outcomes is warranted.", "meshes": ["Animals", "Cattle", "Clinical Competence", "Clinical Decision-Making", "Female", "General Surgery", "Humans", "Internship and Residency", "Interprofessional Relations", "Intestines", "Leadership", "Male", "Midwestern United States", "Self Efficacy"], "year": "2016"}
{"id": "pubmedqa_14968373", "dataset": "pubmedqa", "question": "Context: Treatment of obstructive hydrocephalus in children with tuberculous meningitis (TBM) depends on the level of the cerebrospinal fluid (CSF) block. Air-encephalography is regarded as the gold standard for differentiating communicating and non-communicating hydrocephalus. Since air-encephalography involves a lumbar puncture, it carries the risk of cerebral herniation. AIM. The aim of this study was to determine whether communicating and non-communicating hydrocephalus in TBM can be differentiated by means of cranial computerised tomography (CT). A number of CT indices were measured in 50 children with communicating and 34 children with non-communicating hydrocephalus according to air-encephalographic findings. The only CT finding that correlated with the type of hydrocephalus was the shape of the third ventricle. Significantly more children with non-communicating hydrocephalus had a rounded third ventricle than those with communicating hydrocephalus.\n\nQuestion: Can CT predict the level of CSF block in tuberculous hydrocephalus?", "question_only": "Can CT predict the level of CSF block in tuberculous hydrocephalus?", "context": "Treatment of obstructive hydrocephalus in children with tuberculous meningitis (TBM) depends on the level of the cerebrospinal fluid (CSF) block. Air-encephalography is regarded as the gold standard for differentiating communicating and non-communicating hydrocephalus. Since air-encephalography involves a lumbar puncture, it carries the risk of cerebral herniation. AIM. The aim of this study was to determine whether communicating and non-communicating hydrocephalus in TBM can be differentiated by means of cranial computerised tomography (CT). A number of CT indices were measured in 50 children with communicating and 34 children with non-communicating hydrocephalus according to air-encephalographic findings. The only CT finding that correlated with the type of hydrocephalus was the shape of the third ventricle. Significantly more children with non-communicating hydrocephalus had a rounded third ventricle than those with communicating hydrocephalus.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "CT is therefore not useful in determining the level of CSF block in TBM. Air-encephalography remains the most reliable way of determining the level of CSF obstruction.", "meshes": ["Cerebrospinal Fluid", "Child, Preschool", "Diagnosis, Differential", "Female", "Humans", "Hydrocephalus", "Infant", "Male", "Pneumoencephalography", "Radiographic Image Interpretation, Computer-Assisted", "Retrospective Studies", "Sensitivity and Specificity", "Third Ventricle", "Tomography, X-Ray Computed", "Tuberculosis, Meningeal"], "year": "2004"}
{"id": "pubmedqa_24433626", "dataset": "pubmedqa", "question": "Context: Medicare beneficiaries who have chronic conditions are responsible for a disproportionate share of Medicare fee-for-service expenditures. The objective of this study was to analyze the change in the health of Medicare beneficiaries enrolled in Part A (hospital insurance) between 2008 and 2010 by comparing the prevalence of 11 chronic conditions. We conducted descriptive analyses using the 2008 and 2010 Chronic Conditions Public Use Files, which are newly available from the Centers for Medicare and Medicaid Services and have administrative (claims) data on 100% of the Medicare fee-for-service population. We examined the data by age, sex, and dual eligibility (eligibility for both Medicare and Medicaid). Medicare Part A beneficiaries had more chronic conditions on average in 2010 than in 2008. The percentage increase in the average number of chronic conditions was larger for dual-eligible beneficiaries (2.8%) than for nondual-eligible beneficiaries (1.2%). The prevalence of some chronic conditions, such as congestive heart failure, ischemic heart disease, and stroke/transient ischemic attack, decreased. The deterioration of average health was due to other chronic conditions: chronic kidney disease, depression, diabetes, osteoporosis, rheumatoid arthritis/osteoarthritis. Trends in Alzheimer's disease, cancer, and chronic obstructive pulmonary disease showed differences by sex or dual eligibility or both.\n\nQuestion: Prevalence of chronic conditions among Medicare Part A beneficiaries in 2008 and 2010: are Medicare beneficiaries getting sicker?", "question_only": "Prevalence of chronic conditions among Medicare Part A beneficiaries in 2008 and 2010: are Medicare beneficiaries getting sicker?", "context": "Medicare beneficiaries who have chronic conditions are responsible for a disproportionate share of Medicare fee-for-service expenditures. The objective of this study was to analyze the change in the health of Medicare beneficiaries enrolled in Part A (hospital insurance) between 2008 and 2010 by comparing the prevalence of 11 chronic conditions. We conducted descriptive analyses using the 2008 and 2010 Chronic Conditions Public Use Files, which are newly available from the Centers for Medicare and Medicaid Services and have administrative (claims) data on 100% of the Medicare fee-for-service population. We examined the data by age, sex, and dual eligibility (eligibility for both Medicare and Medicaid). Medicare Part A beneficiaries had more chronic conditions on average in 2010 than in 2008. The percentage increase in the average number of chronic conditions was larger for dual-eligible beneficiaries (2.8%) than for nondual-eligible beneficiaries (1.2%). The prevalence of some chronic conditions, such as congestive heart failure, ischemic heart disease, and stroke/transient ischemic attack, decreased. The deterioration of average health was due to other chronic conditions: chronic kidney disease, depression, diabetes, osteoporosis, rheumatoid arthritis/osteoarthritis. Trends in Alzheimer's disease, cancer, and chronic obstructive pulmonary disease showed differences by sex or dual eligibility or both.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Analyzing the prevalence of 11 chronic conditions by using Medicare claims data provides a monitoring tool that can guide health care providers and policy makers in devising strategies to address chronic conditions and rising health care costs.", "meshes": ["Aged", "Aged, 80 and over", "Chronic Disease", "Female", "Health Services Research", "Humans", "Insurance Claim Review", "Male", "Medicare Part A", "Middle Aged", "Prevalence", "Time Factors", "United States"], "year": "2014"}
{"id": "pubmedqa_12790890", "dataset": "pubmedqa", "question": "Context: Mesial temporal sclerosis (MTS) is characterized by neuronal loss in the hippocampus. Studies on experimental models and patients with intractable epilepsy suggest that apoptosis may be involved in neuronal death induced by recurrent seizures. We searched evidence for apoptotic cell death in temporal lobes resected from drug-resistant epilepsy patients with MTS by using the terminal deoxynucleotidyl transferase (TdT) and digoxigenin-11-dUTP (TUNEL) method and immunohistochemistry for Bcl-2, Bax, and caspase-cleaved actin fragment, fractin. The temporal lobe specimens were obtained from 15 patients (six women and nine men; mean age, 29 +/- 8 years). Unlike that in normal adult brain, we observed Bcl-2 immunoreactivity in some of the remaining neurons dispersed throughout the hippocampus proper as well as in most of the reactive astroglia. Bax immunopositivity was increased in almost all neurons. Fractin immunostaining, an indicator of caspase activity, was detected in approximately 10% of these neurons. Despite increased Bax expression and activation of caspases, we could not find evidence for DNA fragmentation by TUNEL staining. We also could not detect typical apoptotic changes in nuclear morphology by Hoechst-33258 or hematoxylin counterstaining.\n\nQuestion: Is the cell death in mesial temporal sclerosis apoptotic?", "question_only": "Is the cell death in mesial temporal sclerosis apoptotic?", "context": "Mesial temporal sclerosis (MTS) is characterized by neuronal loss in the hippocampus. Studies on experimental models and patients with intractable epilepsy suggest that apoptosis may be involved in neuronal death induced by recurrent seizures. We searched evidence for apoptotic cell death in temporal lobes resected from drug-resistant epilepsy patients with MTS by using the terminal deoxynucleotidyl transferase (TdT) and digoxigenin-11-dUTP (TUNEL) method and immunohistochemistry for Bcl-2, Bax, and caspase-cleaved actin fragment, fractin. The temporal lobe specimens were obtained from 15 patients (six women and nine men; mean age, 29 +/- 8 years). Unlike that in normal adult brain, we observed Bcl-2 immunoreactivity in some of the remaining neurons dispersed throughout the hippocampus proper as well as in most of the reactive astroglia. Bax immunopositivity was increased in almost all neurons. Fractin immunostaining, an indicator of caspase activity, was detected in approximately 10% of these neurons. Despite increased Bax expression and activation of caspases, we could not find evidence for DNA fragmentation by TUNEL staining. We also could not detect typical apoptotic changes in nuclear morphology by Hoechst-33258 or hematoxylin counterstaining.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "These data suggest that either apoptosis is not involved in cell loss in MTS, or a very slow rate of cell demise may have precluded detecting TUNEL-positive neurons dying through apoptosis. Increased Bax expression and activation of caspases support the latter possibility.", "meshes": ["Actin Cytoskeleton", "Adult", "Age of Onset", "Apoptosis", "Astrocytes", "Cell Count", "Cell Death", "DNA Fragmentation", "Epilepsy, Temporal Lobe", "Female", "Hippocampus", "Humans", "Immunohistochemistry", "In Situ Nick-End Labeling", "Male", "Neurons", "Sclerosis", "Temporal Lobe"], "year": "2003"}
{"id": "pubmedqa_19299238", "dataset": "pubmedqa", "question": "Context: Aromatase inhibitors (AIs) are an effective treatment for postmenopausal women with hormone receptor-positive breast cancer. However, patients receiving AIs report a higher incidence of musculoskeletal symptoms and bone fractures; the mechanism and risk factors for this correlation are not well studied. The aim of this study was to correlate these musculoskeletal symptoms and bone fractures in patients receiving AIs with bone mineral density (BMD), previous tamoxifen use, and administration of calcium/bisphosphonate (Ca/Bis). We reviewed charts of 856 patients with hormone receptor-positive nonmetastatic breast cancer seen at our institution between January 1999 and October 2007. A total of 316 patients met the inclusion criteria of treatment with one of the AIs for>or = 3 months and availability of a dualenergy X-ray absorptiometry (DEXA) during this treatment. Arthralgia, generalized bone pain and/or myalgia, bone fracture after beginning AIs, any tamoxifen treatment, and Ca/Bis therapy were recorded. Our study demonstrates a significant association between symptoms and DEXA-BMD results (P<.001). Similarly, the group receiving tamoxifen before AIs had fewer patients with arthralgia or generalized bone pain/myalgia or bone fracture (P<.001). Furthermore, the group receiving AIs plus Ca/Bis had more patients without musculoskeletal symptoms and had fewer fractures. Finally, the group receiving steroidal AIs compared with nonsteroidal AIs had more patients with arthralgia or generalized bone pain and/or myalgia, and bone fractures (P<.001).\n\nQuestion: Aromatase inhibitor-related musculoskeletal symptoms: is preventing osteoporosis the key to eliminating these symptoms?", "question_only": "Aromatase inhibitor-related musculoskeletal symptoms: is preventing osteoporosis the key to eliminating these symptoms?", "context": "Aromatase inhibitors (AIs) are an effective treatment for postmenopausal women with hormone receptor-positive breast cancer. However, patients receiving AIs report a higher incidence of musculoskeletal symptoms and bone fractures; the mechanism and risk factors for this correlation are not well studied. The aim of this study was to correlate these musculoskeletal symptoms and bone fractures in patients receiving AIs with bone mineral density (BMD), previous tamoxifen use, and administration of calcium/bisphosphonate (Ca/Bis). We reviewed charts of 856 patients with hormone receptor-positive nonmetastatic breast cancer seen at our institution between January 1999 and October 2007. A total of 316 patients met the inclusion criteria of treatment with one of the AIs for>or = 3 months and availability of a dualenergy X-ray absorptiometry (DEXA) during this treatment. Arthralgia, generalized bone pain and/or myalgia, bone fracture after beginning AIs, any tamoxifen treatment, and Ca/Bis therapy were recorded. Our study demonstrates a significant association between symptoms and DEXA-BMD results (P<.001). Similarly, the group receiving tamoxifen before AIs had fewer patients with arthralgia or generalized bone pain/myalgia or bone fracture (P<.001). Furthermore, the group receiving AIs plus Ca/Bis had more patients without musculoskeletal symptoms and had fewer fractures. Finally, the group receiving steroidal AIs compared with nonsteroidal AIs had more patients with arthralgia or generalized bone pain and/or myalgia, and bone fractures (P<.001).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Patients on AIs who develop osteoporosis are at increased risk of musculoskeletal symptoms and bone fracture. Comedication with Ca/Bis reduces the likelihood for osteoporosis and musculoskeletal symptoms. Patients who received tamoxifen before AIs were less likely to develop AI-related musculoskeletal symptoms. We recommend that patients on AIs should be offered Ca/Bis to reduce the incidence of musculoskeletal symptoms and fracture, especially if patients are receiving steroidal AI and/or did not receive tamoxifen before AIs.", "meshes": ["Absorptiometry, Photon", "Aged", "Antineoplastic Agents, Hormonal", "Aromatase Inhibitors", "Bone Density", "Breast Neoplasms", "Calcium Phosphates", "Diphosphonates", "Female", "Humans", "Middle Aged", "Musculoskeletal Diseases", "Osteoporosis", "Retrospective Studies", "Tamoxifen"], "year": "2009"}
{"id": "pubmedqa_22237146", "dataset": "pubmedqa", "question": "Context: Epidermal growth factor receptor (EGFR) mutations as prognostic or predictive marker in patients with non-small cell lung cancer (NSCLC) have been used widely. However, it may be difficult to get tumor tissue for analyzing the status of EGFR mutation status in large proportion of patients with advanced disease. We obtained pairs of tumor and serum samples from 57 patients with advanced NSCLC, between March 2006 and January 2009. EGFR mutation status from tumor samples was analyzed by genomic polymerase chain reaction and direct sequence and EGFR mutation status from serum samples was determined by the peptide nucleic acid locked nucleic acid polymerase chain reaction clamp. EGFR mutations were detected in the serum samples of 11 patients and in the tumor samples of 12 patients. EGFR mutation status in the serum and tumor samples was consistent in 50 of the 57 pairs (87.7%). There was a high correlation between the mutations detected in serum sample and the mutations detected in the matched tumor sample (correlation index 0.62; P<0.001). Twenty-two of 57 patients (38.5%) received EGFR-tyrosine kinase inhibitors as any line therapy. The response for EGFR-tyrosine kinase inhibitors was significantly associated with EGFR mutations in both tumor samples and serum samples (P<0.05). There was no significant differences in overall survival according to the status of EGFR mutations in both serum and tumor samples (P>0.05).\n\nQuestion: Can serum be used for analyzing the EGFR mutation status in patients with advanced non-small cell lung cancer?", "question_only": "Can serum be used for analyzing the EGFR mutation status in patients with advanced non-small cell lung cancer?", "context": "Epidermal growth factor receptor (EGFR) mutations as prognostic or predictive marker in patients with non-small cell lung cancer (NSCLC) have been used widely. However, it may be difficult to get tumor tissue for analyzing the status of EGFR mutation status in large proportion of patients with advanced disease. We obtained pairs of tumor and serum samples from 57 patients with advanced NSCLC, between March 2006 and January 2009. EGFR mutation status from tumor samples was analyzed by genomic polymerase chain reaction and direct sequence and EGFR mutation status from serum samples was determined by the peptide nucleic acid locked nucleic acid polymerase chain reaction clamp. EGFR mutations were detected in the serum samples of 11 patients and in the tumor samples of 12 patients. EGFR mutation status in the serum and tumor samples was consistent in 50 of the 57 pairs (87.7%). There was a high correlation between the mutations detected in serum sample and the mutations detected in the matched tumor sample (correlation index 0.62; P<0.001). Twenty-two of 57 patients (38.5%) received EGFR-tyrosine kinase inhibitors as any line therapy. The response for EGFR-tyrosine kinase inhibitors was significantly associated with EGFR mutations in both tumor samples and serum samples (P<0.05). There was no significant differences in overall survival according to the status of EGFR mutations in both serum and tumor samples (P>0.05).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Serum sample might be alternatively used in the difficult time of getting tumor tissue for analyzing the status of EGFR mutation status in patients with advanced NSCLC.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Biomarkers, Tumor", "Carcinoma, Non-Small-Cell Lung", "DNA Mutational Analysis", "Female", "Genes, erbB-1", "Humans", "Lung Neoplasms", "Male", "Middle Aged", "Mutation", "Neoplasm Staging", "Polymerase Chain Reaction", "Sensitivity and Specificity"], "year": "2013"}
{"id": "pubmedqa_25218577", "dataset": "pubmedqa", "question": "Context: Reconstruction of the joint line is crucial in total knee arthroplasty (TKA). A routine height of tibial cut to maintain the natural joint line may compromise the preservation of the PCL. Since the PCL footprint is not accessible prior to tibial osteotomy, it seems beneficial to identify a reliable extraarticular anatomic landmark for predicting the PCL footprint and being visible within standard TKA approach. The fibula head predicts reliably the location of PCL footprint; however, it is not accessible during TKA. The aim of this study now was to analyze whether the tibial tuberosity can serve as a reliable referencing landmark to estimate the PCL footprint height prior to tibial cut. The first consecutive case series included 216 CR TKA. Standing postoperative lateral view radiographs were utilized to measure the vertical distance between tibial tuberosity and tibial osteotomy plane. In the second case series, 223 knee MRIs were consecutively analyzed to measure the vertical distance between tibial tuberosity and PCL footprint. The probability of partial or total PCL removal was calculated for different vertical distances between tibial tuberosity and tibial cutting surface. The vertical distance between the tibial tuberosity and tibial cut averaged 24.7 ± 4 mm. The average vertical distance from tibial tuberosity to proximal and to distal PCL footprint was found to be 22 ± 4.4 and 16 ± 4.4 mm, respectively. Five knees were considered at 50% risk of an entire PCL removal after CR TKA.\n\nQuestion: Preservation of the PCL when performing cruciate-retaining TKA: Is the tibial tuberosity a reliable predictor of the PCL footprint location?", "question_only": "Preservation of the PCL when performing cruciate-retaining TKA: Is the tibial tuberosity a reliable predictor of the PCL footprint location?", "context": "Reconstruction of the joint line is crucial in total knee arthroplasty (TKA). A routine height of tibial cut to maintain the natural joint line may compromise the preservation of the PCL. Since the PCL footprint is not accessible prior to tibial osteotomy, it seems beneficial to identify a reliable extraarticular anatomic landmark for predicting the PCL footprint and being visible within standard TKA approach. The fibula head predicts reliably the location of PCL footprint; however, it is not accessible during TKA. The aim of this study now was to analyze whether the tibial tuberosity can serve as a reliable referencing landmark to estimate the PCL footprint height prior to tibial cut. The first consecutive case series included 216 CR TKA. Standing postoperative lateral view radiographs were utilized to measure the vertical distance between tibial tuberosity and tibial osteotomy plane. In the second case series, 223 knee MRIs were consecutively analyzed to measure the vertical distance between tibial tuberosity and PCL footprint. The probability of partial or total PCL removal was calculated for different vertical distances between tibial tuberosity and tibial cutting surface. The vertical distance between the tibial tuberosity and tibial cut averaged 24.7 ± 4 mm. The average vertical distance from tibial tuberosity to proximal and to distal PCL footprint was found to be 22 ± 4.4 and 16 ± 4.4 mm, respectively. Five knees were considered at 50% risk of an entire PCL removal after CR TKA.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Current surgical techniques of tibial preparation may result in partial or total PCL damage. Tibial tuberosity is a useful anatomical landmark to locate the PCL footprint and to predict the probability of its detachment pre-, intra-, and postoperatively. This knowledge might be useful to predict and avoid instability, consecutive pain, and dissatisfaction after TKA related to PCL insufficiency.", "meshes": ["Arthroplasty, Replacement, Knee", "Female", "Humans", "Male", "Middle Aged", "Osteotomy", "Posterior Cruciate Ligament", "Radiography", "Tibia"], "year": "2016"}
{"id": "pubmedqa_23386371", "dataset": "pubmedqa", "question": "Context: The recent literature shows an increased incidence of obstructive sleep apnea (OSA) in patients with idiopathic pulmonary fibrosis (IPF). On the other hand, there are no published studies related to continuous positive airway pressure (CPAP) treatment in this patient group. Our aim was to assess the effect of CPAP on sleep and overall life quality parameters in IPF patients with OSA and to recognize and overcome possible difficulties in CPAP initiation and acceptance by these patients. Twelve patients (ten males and two females, age 67.1 ± 7.2 years) with newly diagnosed IPF and moderate to severe OSA, confirmed by overnight attended polysomnography, were included. Therapy with CPAP was initiated after a formal in-lab CPAP titration study. The patients completed the Epworth Sleepiness Scale (ESS), the Pittsburgh Sleep Quality Index (PSQI), the Functional Outcomes in Sleep Questionnaire (FOSQ), the Fatigue Severity Scale (FSS), the SF-36 quality of life questionnaire, and the Beck Depression Inventory (BDI) at CPAP initiation and after 1, 3, and 6 months of effective CPAP therapy. A statistically significant improvement was observed in the FOSQ at 1, 3, and 6 months after CPAP initiation (baseline 12.9 ± 2.9 vs. 14.7 ± 2.6 vs. 15.8 ± 2.1 vs. 16.9 ± 1.9, respectively, p = 0.02). Improvement, although not statistically significant, was noted in ESS score (9.2 ± 5.6 vs. 7.6 ± 4.9 vs. 7.5 ± 5.3 vs. 7.7 ± 5.2, p = 0.84), PSQI (10.7 ± 4.4 vs. 10.1 ± 4.3 vs. 9.4 ± 4.7 vs. 8.6 ± 5.2, p = 0.66), FSS (39.5 ± 10.2 vs. 34.8 ± 8.5 vs. 33.6 ± 10.7 vs. 33.4 ± 10.9, p = 0.44), SF-36 (63.2 ± 13.9 vs. 68.9 ± 13.5 vs. 72.1 ± 12.9 vs. 74.4 ± 11.3, p = 0.27), and BDI (12.9 ± 5.5 vs. 10.7 ± 4.3 vs. 9.4 ± 4.8 vs. 9.6 ± 4.5, p = 0.40). Two patients had difficulty complying with CPAP for a variety of reasons (nocturnal cough, claustrophobia, insomnia) and stopped CPAP use after the first month, despite intense follow-up by the CPAP clinic staff. Heated humidification was added for all patients in order to improve the common complaint of disabling nocturnal cough.\n\nQuestion: CPAP therapy in patients with idiopathic pulmonary fibrosis and obstructive sleep apnea: does it offer a better quality of life and sleep?", "question_only": "CPAP therapy in patients with idiopathic pulmonary fibrosis and obstructive sleep apnea: does it offer a better quality of life and sleep?", "context": "The recent literature shows an increased incidence of obstructive sleep apnea (OSA) in patients with idiopathic pulmonary fibrosis (IPF). On the other hand, there are no published studies related to continuous positive airway pressure (CPAP) treatment in this patient group. Our aim was to assess the effect of CPAP on sleep and overall life quality parameters in IPF patients with OSA and to recognize and overcome possible difficulties in CPAP initiation and acceptance by these patients. Twelve patients (ten males and two females, age 67.1 ± 7.2 years) with newly diagnosed IPF and moderate to severe OSA, confirmed by overnight attended polysomnography, were included. Therapy with CPAP was initiated after a formal in-lab CPAP titration study. The patients completed the Epworth Sleepiness Scale (ESS), the Pittsburgh Sleep Quality Index (PSQI), the Functional Outcomes in Sleep Questionnaire (FOSQ), the Fatigue Severity Scale (FSS), the SF-36 quality of life questionnaire, and the Beck Depression Inventory (BDI) at CPAP initiation and after 1, 3, and 6 months of effective CPAP therapy. A statistically significant improvement was observed in the FOSQ at 1, 3, and 6 months after CPAP initiation (baseline 12.9 ± 2.9 vs. 14.7 ± 2.6 vs. 15.8 ± 2.1 vs. 16.9 ± 1.9, respectively, p = 0.02). Improvement, although not statistically significant, was noted in ESS score (9.2 ± 5.6 vs. 7.6 ± 4.9 vs. 7.5 ± 5.3 vs. 7.7 ± 5.2, p = 0.84), PSQI (10.7 ± 4.4 vs. 10.1 ± 4.3 vs. 9.4 ± 4.7 vs. 8.6 ± 5.2, p = 0.66), FSS (39.5 ± 10.2 vs. 34.8 ± 8.5 vs. 33.6 ± 10.7 vs. 33.4 ± 10.9, p = 0.44), SF-36 (63.2 ± 13.9 vs. 68.9 ± 13.5 vs. 72.1 ± 12.9 vs. 74.4 ± 11.3, p = 0.27), and BDI (12.9 ± 5.5 vs. 10.7 ± 4.3 vs. 9.4 ± 4.8 vs. 9.6 ± 4.5, p = 0.40). Two patients had difficulty complying with CPAP for a variety of reasons (nocturnal cough, claustrophobia, insomnia) and stopped CPAP use after the first month, despite intense follow-up by the CPAP clinic staff. Heated humidification was added for all patients in order to improve the common complaint of disabling nocturnal cough.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Effective CPAP treatment in IPF patients with OSA results in a significant improvement in daily living activities based on the FOSQ, namely an OSA-specific follow-up instrument. Improvement was also noted in other questionnaires assessing quality of life, though not to a statistically significant degree, probably because of the multifactorial influences of IPF on physical and mental health. The probability of poor CPAP compliance was high and could only be eliminated with intense follow-up by the CPAP clinic staff.", "meshes": ["Activities of Daily Living", "Aged", "Continuous Positive Airway Pressure", "Disorders of Excessive Somnolence", "Female", "Greece", "Humans", "Male", "Middle Aged", "Polysomnography", "Pulmonary Fibrosis", "Quality of Life", "Sleep Apnea, Obstructive", "Surveys and Questionnaires", "Treatment Outcome"], "year": "2013"}
{"id": "pubmedqa_10759659", "dataset": "pubmedqa", "question": "Context: To compare the accuracy achieved by a trained urology nurse practitioner (UNP) and consultant urologist in detecting bladder tumours during flexible cystoscopy. Eighty-three patients underwent flexible cystoscopy by both the UNP and consultant urologist, each unaware of the other's findings. Before comparing the findings, each declared whether there was tumour or any suspicious lesion requiring biopsy. Of 83 patients examined by flexible cystoscopy, 26 were found to have a tumour or a suspicious lesion. One tumour was missed by the UNP and one by the urologist; each tumour was minute. Analysis using the chance-corrected proportional agreement (Kappa) was 0.94, indicating very close agreement.\n\nQuestion: The nurse cystoscopist: a feasible option?", "question_only": "The nurse cystoscopist: a feasible option?", "context": "To compare the accuracy achieved by a trained urology nurse practitioner (UNP) and consultant urologist in detecting bladder tumours during flexible cystoscopy. Eighty-three patients underwent flexible cystoscopy by both the UNP and consultant urologist, each unaware of the other's findings. Before comparing the findings, each declared whether there was tumour or any suspicious lesion requiring biopsy. Of 83 patients examined by flexible cystoscopy, 26 were found to have a tumour or a suspicious lesion. One tumour was missed by the UNP and one by the urologist; each tumour was minute. Analysis using the chance-corrected proportional agreement (Kappa) was 0.94, indicating very close agreement.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "A UNP can be trained to perform cystoscopy and detect suspicious lesions as accurately as can a consultant urologist. Legal and training issues in implementation are important.", "meshes": ["Cystoscopy", "Evaluation Studies as Topic", "Feasibility Studies", "Humans", "Liability, Legal", "Nurse Practitioners", "Sensitivity and Specificity", "Urinary Bladder Neoplasms", "Urology"], "year": "2000"}
{"id": "pubmedqa_9444542", "dataset": "pubmedqa", "question": "Context: To investigate whether the presence of hippocampal atrophy (HCA) on MRI in Alzheimer's disease (AD) leads to a more rapid decline in cognitive function. To investigate whether cognitively unimpaired controls and depressed subjects with HCA are at higher risk than those without HCA of developing dementia. A prospective follow-up of subjects from a previously reported MRI study. Melbourne, Australia. Five controls with HCA and five age-matched controls without HCA, seven depressed subjects with HCA and seven without HCA, and 12 subjects with clinically diagnosed probable AD with HCA and 12 without HCA were studied. They were followed up at approximately 2 years with repeat cognitive testing, blind to initial diagnosis and MRI result. HCA was rated by two radiologists blind to cognitive test score results. Cognitive assessment was by the Cambridge Cognitive Examination (CAMCOG). No significant differences in rate of cognitive decline, mortality or progression to dementia were found between subjects with or without HCA.\n\nQuestion: Does hippocampal atrophy on MRI predict cognitive decline?", "question_only": "Does hippocampal atrophy on MRI predict cognitive decline?", "context": "To investigate whether the presence of hippocampal atrophy (HCA) on MRI in Alzheimer's disease (AD) leads to a more rapid decline in cognitive function. To investigate whether cognitively unimpaired controls and depressed subjects with HCA are at higher risk than those without HCA of developing dementia. A prospective follow-up of subjects from a previously reported MRI study. Melbourne, Australia. Five controls with HCA and five age-matched controls without HCA, seven depressed subjects with HCA and seven without HCA, and 12 subjects with clinically diagnosed probable AD with HCA and 12 without HCA were studied. They were followed up at approximately 2 years with repeat cognitive testing, blind to initial diagnosis and MRI result. HCA was rated by two radiologists blind to cognitive test score results. Cognitive assessment was by the Cambridge Cognitive Examination (CAMCOG). No significant differences in rate of cognitive decline, mortality or progression to dementia were found between subjects with or without HCA.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "HCA was not found to be a predictor of subsequent cognitive decline in this series.", "meshes": ["Aged", "Aged, 80 and over", "Alzheimer Disease", "Atrophy", "Case-Control Studies", "Cognition Disorders", "Depression", "Disease Progression", "Female", "Follow-Up Studies", "Geriatric Assessment", "Hippocampus", "Humans", "Magnetic Resonance Imaging", "Male", "Middle Aged", "Prognosis", "Prospective Studies"], "year": "1997"}
{"id": "pubmedqa_16249670", "dataset": "pubmedqa", "question": "Context: The placement of the superficial cervical plexus block has been the subject of controversy. Although the investing cervical fascia has been considered as an impenetrable barrier, clinically, the placement of the block deep or superficial to the fascia provides the same effective anesthesia. The underlying mechanism is unclear. The aim of this study was to investigate the three-dimensional organization of connective tissues in the anterior region of the neck. Using a combination of dissection, E12 sheet plastination, and confocal microscopy, fascial structures in the anterior cervical triangle were examined in 10 adult human cadavers. In the upper cervical region, the fascia of strap muscles in the middle and the fasciae of the submandibular glands on both sides formed a dumbbell-like fascia sheet that had free lateral margins and did not continue with the sternocleidomastoid fascia. In the lower cervical region, no single connective tissue sheet extended directly between the sternocleidomastoid muscles. The fascial structure deep to platysma in the anterior cervical triangle comprised the strap fascia.\n\nQuestion: Does the investing layer of the deep cervical fascia exist?", "question_only": "Does the investing layer of the deep cervical fascia exist?", "context": "The placement of the superficial cervical plexus block has been the subject of controversy. Although the investing cervical fascia has been considered as an impenetrable barrier, clinically, the placement of the block deep or superficial to the fascia provides the same effective anesthesia. The underlying mechanism is unclear. The aim of this study was to investigate the three-dimensional organization of connective tissues in the anterior region of the neck. Using a combination of dissection, E12 sheet plastination, and confocal microscopy, fascial structures in the anterior cervical triangle were examined in 10 adult human cadavers. In the upper cervical region, the fascia of strap muscles in the middle and the fasciae of the submandibular glands on both sides formed a dumbbell-like fascia sheet that had free lateral margins and did not continue with the sternocleidomastoid fascia. In the lower cervical region, no single connective tissue sheet extended directly between the sternocleidomastoid muscles. The fascial structure deep to platysma in the anterior cervical triangle comprised the strap fascia.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "This study provides anatomical evidence to indicate that the so-called investing cervical fascia does not exist in the anterior triangle of the neck. Taking the previous reports together, the authors' findings strongly suggest that deep potential spaces in the neck are directly continuous with the subcutaneous tissue.", "meshes": ["Aged", "Aged, 80 and over", "Cervical Plexus", "Collagen", "Connective Tissue", "Epoxy Compounds", "Female", "Humans", "Male", "Microscopy, Confocal", "Neck", "Neck Muscles", "Plastic Embedding"], "year": "2005"}
{"id": "pubmedqa_24614851", "dataset": "pubmedqa", "question": "Context: The purpose of this study was to clarify the prognostic factors for cervical spondylotic amyotrophy (CSA). The authors retrospectively reviewed the medical records of 47 consecutive patients with CSA in whom the presence/absence of the pyramidal tract sign was noted. We analyzed whether the age, sex, presence of diabetes mellitus, medication (vitamin B12), type of the most atrophic and impaired muscle, the muscle strength at the presentation, the presence of the pyramidal tract sign, magnetic resonance imaging (MRI) findings, including the presence and number of T2 high signal intensity areas (T2 HIA) in the spinal cord and the conversion to surgery were associated with the recovery of muscle strength in the patients. In addition, we also investigated whether the duration of symptoms before surgery and the type of surgery were associated with the recovery of muscle strength in patients who required conversion to surgical treatment. The presence of T2 HIA on MRI (P=0.002), the number of T2 HIA on MRI (P=0.002) and conversion to surgery (P=0.015) were found to be significantly associated with a poorer recovery at the observational final follow-up. Further, the presence of the pyramidal tract sign (P=0.043) was significantly associated with a poor recovery at the final follow-up after surgery.\n\nQuestion: Prognostic factors for cervical spondylotic amyotrophy: are signs of spinal cord involvement associated with the neurological prognosis?", "question_only": "Prognostic factors for cervical spondylotic amyotrophy: are signs of spinal cord involvement associated with the neurological prognosis?", "context": "The purpose of this study was to clarify the prognostic factors for cervical spondylotic amyotrophy (CSA). The authors retrospectively reviewed the medical records of 47 consecutive patients with CSA in whom the presence/absence of the pyramidal tract sign was noted. We analyzed whether the age, sex, presence of diabetes mellitus, medication (vitamin B12), type of the most atrophic and impaired muscle, the muscle strength at the presentation, the presence of the pyramidal tract sign, magnetic resonance imaging (MRI) findings, including the presence and number of T2 high signal intensity areas (T2 HIA) in the spinal cord and the conversion to surgery were associated with the recovery of muscle strength in the patients. In addition, we also investigated whether the duration of symptoms before surgery and the type of surgery were associated with the recovery of muscle strength in patients who required conversion to surgical treatment. The presence of T2 HIA on MRI (P=0.002), the number of T2 HIA on MRI (P=0.002) and conversion to surgery (P=0.015) were found to be significantly associated with a poorer recovery at the observational final follow-up. Further, the presence of the pyramidal tract sign (P=0.043) was significantly associated with a poor recovery at the final follow-up after surgery.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The presence of a high signal intensity change on T2-weighted MRI and the pyramidal tract sign can be used as prognostic factors for patients with CSA.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Diabetes Mellitus", "Female", "Follow-Up Studies", "Humans", "Magnetic Resonance Imaging", "Male", "Middle Aged", "Muscle Strength", "Muscle, Skeletal", "Nervous System Diseases", "Pyramidal Tracts", "Retrospective Studies", "Spinal Cord Injuries", "Spondylosis", "Statistics, Nonparametric"], "year": "2014"}
{"id": "pubmedqa_15918864", "dataset": "pubmedqa", "question": "Context: Little is known about how information needs change over time in the early postpartum period or about how these needs might differ given socioeconomic circumstances. This study's aim was to examine women's concerns at the time of hospital discharge and unmet learning needs as self-identified at 4 weeks after discharge. Data were collected as part of a cross-sectional survey of postpartum health outcomes, service use, and costs of care in the first 4 weeks after postpartum hospital discharge. Recruitment of 250 women was conducted from each of 5 hospitals in Ontario, Canada (n = 1,250). Women who had given vaginal birth to a single live infant, and who were being discharged at the same time as their infant, assuming care of their infant, competent to give consent, and able to communicate in one of the study languages were eligible. Participants completed a self-report questionnaire in hospital; 890 (71.2%) took part in a structured telephone interview 4 weeks after hospital discharge. Approximately 17 percent of participants were of low socioeconomic status. Breastfeeding and signs of infant illness were the most frequently identified concerns by women, regardless of their socioeconomic status. Signs of infant illness and infant care/behavior were the main unmet learning needs. Although few differences in identified concerns were evident, women of low socioeconomic status were significantly more likely to report unmet learning needs related to 9 of 10 topics compared with women of higher socioeconomic status. For most topics, significantly more women of both groups identified learning needs 4 weeks after discharge compared with the number who identified corresponding concerns while in hospital.\n\nQuestion: Learning needs of postpartum women: does socioeconomic status matter?", "question_only": "Learning needs of postpartum women: does socioeconomic status matter?", "context": "Little is known about how information needs change over time in the early postpartum period or about how these needs might differ given socioeconomic circumstances. This study's aim was to examine women's concerns at the time of hospital discharge and unmet learning needs as self-identified at 4 weeks after discharge. Data were collected as part of a cross-sectional survey of postpartum health outcomes, service use, and costs of care in the first 4 weeks after postpartum hospital discharge. Recruitment of 250 women was conducted from each of 5 hospitals in Ontario, Canada (n = 1,250). Women who had given vaginal birth to a single live infant, and who were being discharged at the same time as their infant, assuming care of their infant, competent to give consent, and able to communicate in one of the study languages were eligible. Participants completed a self-report questionnaire in hospital; 890 (71.2%) took part in a structured telephone interview 4 weeks after hospital discharge. Approximately 17 percent of participants were of low socioeconomic status. Breastfeeding and signs of infant illness were the most frequently identified concerns by women, regardless of their socioeconomic status. Signs of infant illness and infant care/behavior were the main unmet learning needs. Although few differences in identified concerns were evident, women of low socioeconomic status were significantly more likely to report unmet learning needs related to 9 of 10 topics compared with women of higher socioeconomic status. For most topics, significantly more women of both groups identified learning needs 4 weeks after discharge compared with the number who identified corresponding concerns while in hospital.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "It is important to ensure that new mothers are adequately informed about topics important to them while in hospital. The findings highlight the need for accessible and appropriate community-based information resources for women in the postpartum period, especially for those of low socioeconomic status.", "meshes": ["Adolescent", "Adult", "Attitude to Health", "Breast Feeding", "Female", "Humans", "Ontario", "Patient Education as Topic", "Postpartum Period", "Socioeconomic Factors", "Surveys and Questionnaires", "Time Factors"], "year": "2005"}
{"id": "pubmedqa_10473855", "dataset": "pubmedqa", "question": "Context: Delayed gastric emptying (DGE) is the most frequent postoperative complication after pylorus-preserving pancreaticoduodenectomy (PPPD). This prospective, non-randomized study was undertaken to determine whether the incidence of DGE may be reduced by modifying the original reconstructive anatomy with a retrocolic duodenojejunostomy towards an antecolic duodenojejunostomy. The study was comprised of 51 patients who underwent PPPD between August 1994 and November 1997. The operation was carried out as originally described but was modified by performing the duodenojejunostomy antecolically. Clinical data were recorded prospectively, with special regard to DGE. After PPPD, the nasogastric tube could be removed at a median of 2 days (range 1-22 days) postoperatively; in two patients, the nasogastric tube was reinserted because of vomiting and nausea. A liquid diet was started at a median of 5 days (3-11 days); the patients were able to tolerate a full, regular diet at a median of 10 days (7-28 days). The overall incidence of DGE was 12% (n=6). No postoperative complications other than DGE were exhibited by 36 patients (71%). In this group, DGE was only seen in one patient (3%). In the second group, where postoperative complications other than DGE occurred (n=15), five patients (30%) exhibited DGE (P=0.002).\n\nQuestion: Is delayed gastric emptying following pancreaticoduodenectomy related to pylorus preservation?", "question_only": "Is delayed gastric emptying following pancreaticoduodenectomy related to pylorus preservation?", "context": "Delayed gastric emptying (DGE) is the most frequent postoperative complication after pylorus-preserving pancreaticoduodenectomy (PPPD). This prospective, non-randomized study was undertaken to determine whether the incidence of DGE may be reduced by modifying the original reconstructive anatomy with a retrocolic duodenojejunostomy towards an antecolic duodenojejunostomy. The study was comprised of 51 patients who underwent PPPD between August 1994 and November 1997. The operation was carried out as originally described but was modified by performing the duodenojejunostomy antecolically. Clinical data were recorded prospectively, with special regard to DGE. After PPPD, the nasogastric tube could be removed at a median of 2 days (range 1-22 days) postoperatively; in two patients, the nasogastric tube was reinserted because of vomiting and nausea. A liquid diet was started at a median of 5 days (3-11 days); the patients were able to tolerate a full, regular diet at a median of 10 days (7-28 days). The overall incidence of DGE was 12% (n=6). No postoperative complications other than DGE were exhibited by 36 patients (71%). In this group, DGE was only seen in one patient (3%). In the second group, where postoperative complications other than DGE occurred (n=15), five patients (30%) exhibited DGE (P=0.002).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "DGE after PPPD seems to be of minor clinical importance following uncomplicated surgery. When taking the results into consideration, it can be said that, despite the lack of a control group, antecolic duodenojejunostomy might be the key to a low incidence of DGE after PPPD. In our experience, DGE is linked to the occurrence of other postoperative complications rather than to pylorus preservation.", "meshes": ["Female", "Gastric Emptying", "Humans", "Incidence", "Male", "Middle Aged", "Pancreaticoduodenectomy", "Postoperative Complications", "Prospective Studies", "Pylorus"], "year": "1999"}
{"id": "pubmedqa_23972333", "dataset": "pubmedqa", "question": "Context: (1) To describe the prevalence of general practitioner visits and hospitalization according to sex and age groups; (2) to identify which factors are independently associated with a higher use of health care services among elderly Spanish; and (3) to study the time trends in the prevalence of use of health care services 2001-2009. Observational study. We analyzed data from the Spanish National Health Surveys conducted in 2001 (n=21,058), 2003 (n=21,650), 2006 (n=29,478) and 2009 (n=22,188). We included responses from adults aged 65 years and older. The main variables were the number of general practitioner visits in the last 4 weeks and hospitalization in the past year. We stratified the adjusted models by the main variables. We analyzed socio-demographic characteristics, health related variables, using multivariate logistic regression models. The total number of subjects was 24,349 (15,041 woman, 9309 men). Women were significantly older than men (P<0.001). Women had higher prevalence of general practitioner visits than men in all surveys. Men had significantly higher prevalence of hospitalizations than women in the years 2001, 2006 and 2009. When we adjusted the hospitalization by possible confounders using logistic regressions, men had a higher probability of being hospitalized than women (OR 1.53, 1.39-1.69). The variables that were significantly associated with a higher use of health care services were lower educational level, worse self-rated health, chronic conditions, polypharmacy, and the level of disability. The number of general practitioner visits among women and men significantly increased from 2001 to 2009 (women: OR 1.43, 1.27-1.61; men: OR 1.71, 1.49-1.97).\n\nQuestion: Has the prevalence of health care services use increased over the last decade (2001-2009) in elderly people?", "question_only": "Has the prevalence of health care services use increased over the last decade (2001-2009) in elderly people?", "context": "(1) To describe the prevalence of general practitioner visits and hospitalization according to sex and age groups; (2) to identify which factors are independently associated with a higher use of health care services among elderly Spanish; and (3) to study the time trends in the prevalence of use of health care services 2001-2009. Observational study. We analyzed data from the Spanish National Health Surveys conducted in 2001 (n=21,058), 2003 (n=21,650), 2006 (n=29,478) and 2009 (n=22,188). We included responses from adults aged 65 years and older. The main variables were the number of general practitioner visits in the last 4 weeks and hospitalization in the past year. We stratified the adjusted models by the main variables. We analyzed socio-demographic characteristics, health related variables, using multivariate logistic regression models. The total number of subjects was 24,349 (15,041 woman, 9309 men). Women were significantly older than men (P<0.001). Women had higher prevalence of general practitioner visits than men in all surveys. Men had significantly higher prevalence of hospitalizations than women in the years 2001, 2006 and 2009. When we adjusted the hospitalization by possible confounders using logistic regressions, men had a higher probability of being hospitalized than women (OR 1.53, 1.39-1.69). The variables that were significantly associated with a higher use of health care services were lower educational level, worse self-rated health, chronic conditions, polypharmacy, and the level of disability. The number of general practitioner visits among women and men significantly increased from 2001 to 2009 (women: OR 1.43, 1.27-1.61; men: OR 1.71, 1.49-1.97).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The current study revealed an increase in health care services utilization from 2001 to 2009 in the older Spanish population.", "meshes": ["Aged", "Aged, 80 and over", "Female", "Health Care Surveys", "Health Services", "Health Status", "Hospitalization", "Humans", "Logistic Models", "Male", "Odds Ratio", "Office Visits", "Patient Acceptance of Health Care", "Polypharmacy", "Prevalence", "Self Report", "Socioeconomic Factors", "Spain"], "year": "2013"}
{"id": "pubmedqa_23147106", "dataset": "pubmedqa", "question": "Context: We analyzed the pharmacokinetic-pharmacodynamic relationship of vancomycin to determine the drug exposure parameters that correlate with the efficacy and nephrotoxicity of vancomycin in patients with methicillin-resistant Staphylococcus aureus pneumonia and evaluated the need to use peak concentration in therapeutic drug monitoring (TDM). Serum drug concentrations of 31 hospitalized patients treated with vancomycin for methicillin-resistant S. aureus pneumonia were collected. Significant differences in trough concentration (Cmin)/minimum inhibitory concentration (MIC) and area under the serum concentration-time curve (AUC0-24)/MIC were observed between the response and non-response groups. Significant differences in Cmin and AUC0-24 were observed between the nephrotoxicity and non-nephrotoxicity groups. Receiver operating characteristic curves revealed high predictive values of Cmin/MIC and AUC0-24/MIC for efficacy and of Cmin and AUC0-24 for safety of vancomycin.\n\nQuestion: Is peak concentration needed in therapeutic drug monitoring of vancomycin?", "question_only": "Is peak concentration needed in therapeutic drug monitoring of vancomycin?", "context": "We analyzed the pharmacokinetic-pharmacodynamic relationship of vancomycin to determine the drug exposure parameters that correlate with the efficacy and nephrotoxicity of vancomycin in patients with methicillin-resistant Staphylococcus aureus pneumonia and evaluated the need to use peak concentration in therapeutic drug monitoring (TDM). Serum drug concentrations of 31 hospitalized patients treated with vancomycin for methicillin-resistant S. aureus pneumonia were collected. Significant differences in trough concentration (Cmin)/minimum inhibitory concentration (MIC) and area under the serum concentration-time curve (AUC0-24)/MIC were observed between the response and non-response groups. Significant differences in Cmin and AUC0-24 were observed between the nephrotoxicity and non-nephrotoxicity groups. Receiver operating characteristic curves revealed high predictive values of Cmin/MIC and AUC0-24/MIC for efficacy and of Cmin and AUC0-24 for safety of vancomycin.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "These results suggest little need to use peak concentration in vancomycin TDM because Cmin/MIC and Cmin are sufficient to predict the efficacy and safety of vancomycin.", "meshes": ["Aged", "Aged, 80 and over", "Anti-Bacterial Agents", "Area Under Curve", "Female", "Half-Life", "Humans", "Male", "Methicillin-Resistant Staphylococcus aureus", "Microbial Sensitivity Tests", "Middle Aged", "Pneumonia", "ROC Curve", "Retrospective Studies", "Vancomycin"], "year": "2012"}
{"id": "pubmedqa_21256734", "dataset": "pubmedqa", "question": "Context: A secondary analysis of one-hundred-sixty-seven patients referred for treatment of cancer-related pain was conducted. Pain intensity at admission was recorded and patients were divided in three categories of pain intensity: mild, moderate and severe. Patients were offered a treatment with opioid dose titration, according to department policy. Data regarding opioid doses and pain intensity were collected after dose titration was completed. Four levels of opioid response were considered: (a) good pain control, with minimal opioid escalation and without relevant adverse effects; (b) good pain control requiring more aggressive opioid escalation, for example doubling the doses in four days; (c) adequate pain control associated with the occurrence of adverse effects; (d) poor pain control with adverse effects. Seventy-six, forty-four, forty-one and six patients showed a response a, b, c, and d, respectively. No correlation between baseline pain intensity categories and opioid response was found. Patients with response 'b' and 'd' showed higher values of OEImg.\n\nQuestion: Does pain intensity predict a poor opioid response in cancer patients?", "question_only": "Does pain intensity predict a poor opioid response in cancer patients?", "context": "A secondary analysis of one-hundred-sixty-seven patients referred for treatment of cancer-related pain was conducted. Pain intensity at admission was recorded and patients were divided in three categories of pain intensity: mild, moderate and severe. Patients were offered a treatment with opioid dose titration, according to department policy. Data regarding opioid doses and pain intensity were collected after dose titration was completed. Four levels of opioid response were considered: (a) good pain control, with minimal opioid escalation and without relevant adverse effects; (b) good pain control requiring more aggressive opioid escalation, for example doubling the doses in four days; (c) adequate pain control associated with the occurrence of adverse effects; (d) poor pain control with adverse effects. Seventy-six, forty-four, forty-one and six patients showed a response a, b, c, and d, respectively. No correlation between baseline pain intensity categories and opioid response was found. Patients with response 'b' and 'd' showed higher values of OEImg.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Baseline pain intensity does not predict the outcome after an appropriate opioid titration. It is likely that non-homogeneous pain treatment would have biased the outcome of a previous work.", "meshes": ["Aged", "Analgesics, Opioid", "Analysis of Variance", "Female", "Humans", "Karnofsky Performance Status", "Male", "Middle Aged", "Neoplasms", "Pain", "Pain Measurement", "Prospective Studies", "Treatment Outcome"], "year": "2011"}
{"id": "pubmedqa_16991071", "dataset": "pubmedqa", "question": "Context: In literature there are only few data which describe the influence of occupation on the development of rotator cuff disease. In a retrospective study, 760 open rotator cuff repairs were analysed and related to the profession and occupational load. Exclusion criteria were traumatic tears and sports injuries. All male persons were statistically analysed and the data compared with occupational patterns of the region, obtained from the Federal Statistical State Office. Rotator cuff repairs were performed in 472 males who had no evidence for a traumatic origin. After statistical analysis (p<0.001) we found significantly more patients working in agriculture and forestry (6.38% versus 1.07% in Bavaria) and in the building industry (35.11% versus 13.40% in Bavaria).\n\nQuestion: Rotator cuff tear--an occupational disease?", "question_only": "Rotator cuff tear--an occupational disease?", "context": "In literature there are only few data which describe the influence of occupation on the development of rotator cuff disease. In a retrospective study, 760 open rotator cuff repairs were analysed and related to the profession and occupational load. Exclusion criteria were traumatic tears and sports injuries. All male persons were statistically analysed and the data compared with occupational patterns of the region, obtained from the Federal Statistical State Office. Rotator cuff repairs were performed in 472 males who had no evidence for a traumatic origin. After statistical analysis (p<0.001) we found significantly more patients working in agriculture and forestry (6.38% versus 1.07% in Bavaria) and in the building industry (35.11% versus 13.40% in Bavaria).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Our data suggest that working exposure increases the risk or leads to the clinical manifestation of rotator cuff tears. Although a detailed analysis of individual physical exposure is not available yet, the statistical results indicate that rotator cuff tears must be taken into consideration as a result of ergonomic exposure.", "meshes": ["Adult", "Aged", "Comorbidity", "Employment", "Female", "Germany", "Humans", "Male", "Middle Aged", "Occupational Diseases", "Prevalence", "Risk Assessment", "Risk Factors", "Rotator Cuff Injuries", "Rupture", "Shoulder Impingement Syndrome", "Workload"], "year": null}
{"id": "pubmedqa_17489316", "dataset": "pubmedqa", "question": "Context: To determine whether there is a relationship between VEGF expression and renal vein and vena cava invasion in stage pT3 renal cell carcinoma and to evaluate the impact of VEGF expression on survival in pT3 renal cell carcinoma. 78 patients with a pT3a or pT3b tumour without vena cava invasion or pT3b tumour with vena cava invasion were compared for age, gender, Fuhrman grade and immunohistochemical expression of VEGF. All these variables were submitted to univariate and multivariate analysis to establish their impact on survival. Only tumour size appeared to be significantly different between the 3 groups. On univariate analysis, invasion of the perirenal fat, lymph node involvement, distant metastases and VEGF expression were significantly associated with survival (p<0.01). On multivariate analysis, lymph node involvement, distant metastases and VEGF expression (OR 6.07) were identified as independent predictive factors of survival.\n\nQuestion: Is tumour expression of VEGF associated with venous invasion and survival in pT3 renal cell carcinoma?", "question_only": "Is tumour expression of VEGF associated with venous invasion and survival in pT3 renal cell carcinoma?", "context": "To determine whether there is a relationship between VEGF expression and renal vein and vena cava invasion in stage pT3 renal cell carcinoma and to evaluate the impact of VEGF expression on survival in pT3 renal cell carcinoma. 78 patients with a pT3a or pT3b tumour without vena cava invasion or pT3b tumour with vena cava invasion were compared for age, gender, Fuhrman grade and immunohistochemical expression of VEGF. All these variables were submitted to univariate and multivariate analysis to establish their impact on survival. Only tumour size appeared to be significantly different between the 3 groups. On univariate analysis, invasion of the perirenal fat, lymph node involvement, distant metastases and VEGF expression were significantly associated with survival (p<0.01). On multivariate analysis, lymph node involvement, distant metastases and VEGF expression (OR 6.07) were identified as independent predictive factors of survival.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Progression of a pT3 tumour into the renal vein and vena cava is not associated with increased tumour expression of VEGF. However, VEGF is an independent prognostic factor in this group of poor prognosis renal tumours.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Carcinoma, Renal Cell", "Cause of Death", "Disease-Free Survival", "Female", "Follow-Up Studies", "Humans", "Kidney Neoplasms", "Lymphatic Metastasis", "Male", "Middle Aged", "Neoplasm Invasiveness", "Neoplasm Staging", "Prognosis", "Renal Veins", "Survival Rate", "Vascular Endothelial Growth Factor A", "Vascular Neoplasms", "Vena Cava, Inferior"], "year": "2007"}
{"id": "pubmedqa_19504993", "dataset": "pubmedqa", "question": "Context: Fournier's gangrene is known to have an impact in the morbidity and despite antibiotics and aggressive debridement, the mortality rate remains high. To assess the morbidity and mortality in the treatment of Fournier's gangrene in our experience. The medical records of 14 patients with Fournier's gangrene who presented at the University Hospital Center \"Mother Teresa\" from January 1997 to December 2006 were reviewed retrospectively to analyze the outcome and identify the risk factor and prognostic indicators of mortality. Of the 14 patients, 5 died and 9 survived. Mean age was 54 years (range from 41-61): it was 53 years in the group of survivors and 62 years in deceased group. There was a significant difference in leukocyte count between patients who survived (range 4900-17000/mm) and those died (range 20.300-31000/mm3). Mean hospital stay was about 19 days (range 2-57 days).\n\nQuestion: It's Fournier's gangrene still dangerous?", "question_only": "It's Fournier's gangrene still dangerous?", "context": "Fournier's gangrene is known to have an impact in the morbidity and despite antibiotics and aggressive debridement, the mortality rate remains high. To assess the morbidity and mortality in the treatment of Fournier's gangrene in our experience. The medical records of 14 patients with Fournier's gangrene who presented at the University Hospital Center \"Mother Teresa\" from January 1997 to December 2006 were reviewed retrospectively to analyze the outcome and identify the risk factor and prognostic indicators of mortality. Of the 14 patients, 5 died and 9 survived. Mean age was 54 years (range from 41-61): it was 53 years in the group of survivors and 62 years in deceased group. There was a significant difference in leukocyte count between patients who survived (range 4900-17000/mm) and those died (range 20.300-31000/mm3). Mean hospital stay was about 19 days (range 2-57 days).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The interval from the onset of clinical symptoms to the initial surgical intervention seems to be the most important prognostic factor with a significant impact on outcome. Despite extensive therapeutic efforts, Fournier's gangrene remains a surgical emergency and early recognition with prompt radical debridement is the mainstays of management.", "meshes": ["Adult", "Female", "Fournier Gangrene", "Humans", "Male", "Middle Aged", "Survival Rate"], "year": "2009"}
{"id": "pubmedqa_26133538", "dataset": "pubmedqa", "question": "Context: Abdominal bloating is reported by a majority of irritable bowel syndrome (IBS) patients. Excess colonic fermentation may cause gaseous symptoms. Several foodstuffs contain oligosaccharides with an α-galactosidic linkage that is resistant to mammalian hydrolases. Assisted hydrolysis by exogenous α-galactosidase enzyme (AG) could offer a way of controlling IBS symptoms by reducing colonic fermentation and gas production. The aim of this study was to assess the effect of AG on symptom severity and quality of life in IBS patients with abdominal bloating or flatulence. A total of 125 subjects with IBS received AG or placebo at meals for 12 weeks. IBS-Symptom Severity Score (IBS-SSS) and quality of life (QoL) were assessed at baseline, during the treatment and at 4-week follow-up. AG showed a trend toward a more prominent decrease in IBS-SSS. The responder rate at week 16 was higher for the AG group. No difference was detected in QoL between AG and placebo groups. A total of 25 patients (18 in AG group and 7 in placebo group, p = 0.016) withdrew from the study. Abdominal pain and diarrhea were more often reported as reason for withdrawal in AG group.\n\nQuestion: Does oral α-galactosidase relieve irritable bowel symptoms?", "question_only": "Does oral α-galactosidase relieve irritable bowel symptoms?", "context": "Abdominal bloating is reported by a majority of irritable bowel syndrome (IBS) patients. Excess colonic fermentation may cause gaseous symptoms. Several foodstuffs contain oligosaccharides with an α-galactosidic linkage that is resistant to mammalian hydrolases. Assisted hydrolysis by exogenous α-galactosidase enzyme (AG) could offer a way of controlling IBS symptoms by reducing colonic fermentation and gas production. The aim of this study was to assess the effect of AG on symptom severity and quality of life in IBS patients with abdominal bloating or flatulence. A total of 125 subjects with IBS received AG or placebo at meals for 12 weeks. IBS-Symptom Severity Score (IBS-SSS) and quality of life (QoL) were assessed at baseline, during the treatment and at 4-week follow-up. AG showed a trend toward a more prominent decrease in IBS-SSS. The responder rate at week 16 was higher for the AG group. No difference was detected in QoL between AG and placebo groups. A total of 25 patients (18 in AG group and 7 in placebo group, p = 0.016) withdrew from the study. Abdominal pain and diarrhea were more often reported as reason for withdrawal in AG group.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "We found no evidence to support the use of AG routinely in IBS patients. Improvement of clinical response at 4-week follow-up may suggest a long-term effect of unknown mechanism, but could also be attributed to non-responder drop out. Gastrointestinal (GI) side effects may be a coincidence in this study, but irritation of GI tract by AG administration cannot be excluded.", "meshes": ["Abdominal Pain", "Administration, Oral", "Adult", "Diarrhea", "Female", "Finland", "Flatulence", "Gastrointestinal Agents", "Humans", "Irritable Bowel Syndrome", "Male", "Middle Aged", "Quality of Life", "Severity of Illness Index", "Treatment Outcome", "alpha-Galactosidase"], "year": "2016"}
{"id": "pubmedqa_19694846", "dataset": "pubmedqa", "question": "Context: Although there is evidence for the influential role of transformational leadership on health outcomes, researchers have used either attitude outcomes (e.g. job satisfaction) or softer health measures, such as general well-being. Specific measures of well-being such as sleep quality have not been used, despite its association with working conditions. A longitudinal design was used to collect data from Danish healthcare workers at time 1 in 2005 (n = 447) and 18 months later at time 2 in 2007 (n = 274). Structural equation modelling was used to investigate the relationships between transformational leadership, self-efficacy and sleep quality at both time points independently (cross-sectionally) and longitudinally. For all constructs, time 2 measures were influenced by the baseline level. Direct relationships between transformational leadership and sleep quality were found. This relationship was negative cross-sectionally at both time points, but positive between baseline and follow-up. The relationship between leadership and employees' sleep quality was not mediated by employees' self-efficacy.\n\nQuestion: Does self-efficacy mediate the relationship between transformational leadership behaviours and healthcare workers' sleep quality?", "question_only": "Does self-efficacy mediate the relationship between transformational leadership behaviours and healthcare workers' sleep quality?", "context": "Although there is evidence for the influential role of transformational leadership on health outcomes, researchers have used either attitude outcomes (e.g. job satisfaction) or softer health measures, such as general well-being. Specific measures of well-being such as sleep quality have not been used, despite its association with working conditions. A longitudinal design was used to collect data from Danish healthcare workers at time 1 in 2005 (n = 447) and 18 months later at time 2 in 2007 (n = 274). Structural equation modelling was used to investigate the relationships between transformational leadership, self-efficacy and sleep quality at both time points independently (cross-sectionally) and longitudinally. For all constructs, time 2 measures were influenced by the baseline level. Direct relationships between transformational leadership and sleep quality were found. This relationship was negative cross-sectionally at both time points, but positive between baseline and follow-up. The relationship between leadership and employees' sleep quality was not mediated by employees' self-efficacy.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Our results indicate that training managers in transformational leadership behaviours may have a positive impact on healthcare workers' health over time. However, more research is needed to examine the mechanisms by which transformational leadership brings about improved sleep quality; self-efficacy was not found to be the explanation.", "meshes": ["Adult", "Denmark", "Female", "Humans", "Job Satisfaction", "Leadership", "Longitudinal Studies", "Male", "Models, Theoretical", "Nurses' Aides", "Nursing Staff, Hospital", "Self Efficacy", "Sleep", "Sleep Initiation and Maintenance Disorders", "Surveys and Questionnaires"], "year": "2009"}
{"id": "pubmedqa_25443385", "dataset": "pubmedqa", "question": "Context: Virtual planning and guided surgery with or without prebent or milled plates are becoming more and more common for mandibular reconstruction with fibular free flaps (FFFs). Although this excellent surgical option is being used more widely, the question of the additional cost of planning and cutting-guide production has to be discussed. In capped payment systems such additional costs have to be offset by other savings if there are no special provisions for extra funding. Our study was designed to determine whether using virtual planning and guided surgery resulted in time saved during surgery and whether this time gain resulted in self-funding of such planning through the time saved. All consecutive cases of FFF surgery were evaluated during a 2-year period. Institutional data were used to determine the price of 1 minute of operative time. The time for fibula molding, plate adaptation, and insetting was recorded. During the defined period, we performed 20 mandibular reconstructions using FFFs, 9 with virtual planning and guided surgery and 11 freehand cases. One minute of operative time was calculated to cost US $47.50. Multiplying this number by the time saved, we found that the additional cost of virtual planning was reduced from US $5,098 to US $1,231.50 with a prebent plate and from US $6,980 to US $3,113.50 for a milled plate.\n\nQuestion: Are virtual planning and guided surgery for head and neck reconstruction economically viable?", "question_only": "Are virtual planning and guided surgery for head and neck reconstruction economically viable?", "context": "Virtual planning and guided surgery with or without prebent or milled plates are becoming more and more common for mandibular reconstruction with fibular free flaps (FFFs). Although this excellent surgical option is being used more widely, the question of the additional cost of planning and cutting-guide production has to be discussed. In capped payment systems such additional costs have to be offset by other savings if there are no special provisions for extra funding. Our study was designed to determine whether using virtual planning and guided surgery resulted in time saved during surgery and whether this time gain resulted in self-funding of such planning through the time saved. All consecutive cases of FFF surgery were evaluated during a 2-year period. Institutional data were used to determine the price of 1 minute of operative time. The time for fibula molding, plate adaptation, and insetting was recorded. During the defined period, we performed 20 mandibular reconstructions using FFFs, 9 with virtual planning and guided surgery and 11 freehand cases. One minute of operative time was calculated to cost US $47.50. Multiplying this number by the time saved, we found that the additional cost of virtual planning was reduced from US $5,098 to US $1,231.50 with a prebent plate and from US $6,980 to US $3,113.50 for a milled plate.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Even in capped health care systems, virtual planning and guided surgery including prebent or milled plates are financially viable.", "meshes": ["Aged", "Angiography", "Bone Plates", "Bone Transplantation", "Carcinoma, Squamous Cell", "Computer Simulation", "Cost Savings", "Diagnosis-Related Groups", "Female", "Fibula", "Free Tissue Flaps", "Health Care Costs", "Hospital Costs", "Humans", "Imaging, Three-Dimensional", "Male", "Mandibular Neoplasms", "Mandibular Reconstruction", "Middle Aged", "Models, Anatomic", "Operative Time", "Patient Care Planning", "Prospective Studies", "Surgery, Computer-Assisted", "Switzerland", "Tomography, X-Ray Computed", "Transplant Donor Site", "User-Computer Interface"], "year": "2015"}
{"id": "pubmedqa_20594006", "dataset": "pubmedqa", "question": "Context: Tethering of the spinal cord is thought to increase the chance of neurological injury when scoliosis correction is undertaken. All patients with myelomeningocele (MM) are radiographically tethered, and untethering procedures carry significant morbidity risks including worsening neurological function and wound complications. No guidelines exist as regards untethering in patients with MM prior to scoliosis correction surgery. The authors' aim in this study was to evaluate their experience in patients with MM who were not untethered before scoliosis correction. Seventeen patients with MM were retrospectively identified and 1) had no evidence of a clinically symptomatic tethered cord, 2) had undergone spinal fusion for scoliosis correction, and 3) had not been untethered for at least 1 year prior to surgery. The minimum follow-up after fusion was 2 years. Charts and radiographs were reviewed for neurological or shunt complications in the perioperative period. The average age of the patients was 12.4 years, and the following neurological levels were affected: T-12 and above, 7 patients; L-1/L-2, 6 patients; L-3, 2 patients; and L-4, 2 patients. All were radiographically tethered as confirmed on MR imaging. Fourteen of the patients (82%) had a ventriculoperitoneal shunt. The mean Cobb angle was corrected from 82 degrees to 35 degrees , for a 57% correction. All patients underwent neuromonitoring of their upper extremities, and some underwent lower extremity monitoring as well. Postoperatively, no patient experienced a new cranial nerve palsy, shunt malfunction, change in urological function, or upper extremity weakness/sensory loss. One patient had transient lower extremity weakness, which returned to baseline within 1 month of surgery.\n\nQuestion: A patient with myelomeningocele: is untethering necessary prior to scoliosis correction?", "question_only": "A patient with myelomeningocele: is untethering necessary prior to scoliosis correction?", "context": "Tethering of the spinal cord is thought to increase the chance of neurological injury when scoliosis correction is undertaken. All patients with myelomeningocele (MM) are radiographically tethered, and untethering procedures carry significant morbidity risks including worsening neurological function and wound complications. No guidelines exist as regards untethering in patients with MM prior to scoliosis correction surgery. The authors' aim in this study was to evaluate their experience in patients with MM who were not untethered before scoliosis correction. Seventeen patients with MM were retrospectively identified and 1) had no evidence of a clinically symptomatic tethered cord, 2) had undergone spinal fusion for scoliosis correction, and 3) had not been untethered for at least 1 year prior to surgery. The minimum follow-up after fusion was 2 years. Charts and radiographs were reviewed for neurological or shunt complications in the perioperative period. The average age of the patients was 12.4 years, and the following neurological levels were affected: T-12 and above, 7 patients; L-1/L-2, 6 patients; L-3, 2 patients; and L-4, 2 patients. All were radiographically tethered as confirmed on MR imaging. Fourteen of the patients (82%) had a ventriculoperitoneal shunt. The mean Cobb angle was corrected from 82 degrees to 35 degrees , for a 57% correction. All patients underwent neuromonitoring of their upper extremities, and some underwent lower extremity monitoring as well. Postoperatively, no patient experienced a new cranial nerve palsy, shunt malfunction, change in urological function, or upper extremity weakness/sensory loss. One patient had transient lower extremity weakness, which returned to baseline within 1 month of surgery.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The study results suggested that spinal cord untethering may be unnecessary in patients with MM who are undergoing scoliosis corrective surgery and do not present with clinical symptoms of a tethered cord, even though tethering is radiographically demonstrated.", "meshes": ["Adolescent", "Child", "Female", "Humans", "Lumbar Vertebrae", "Magnetic Resonance Imaging", "Male", "Meningomyelocele", "Neural Tube Defects", "Neurosurgical Procedures", "Radiography", "Risk Factors", "Scoliosis", "Spinal Cord", "Spinal Fusion", "Thoracic Vertebrae", "Treatment Outcome", "Unnecessary Procedures"], "year": "2010"}
{"id": "pubmedqa_24973051", "dataset": "pubmedqa", "question": "Context: Establishing a core curriculum for undergraduate Emergency Medicine (EM) education is crucial to development of the specialty. The Clerkship Directors in Emergency Medicine (CDEM) National Curriculum Task Force recommended that all students in a 4(th)-year EM clerkship be exposed to 10 emergent clinical conditions. To evaluate the feasibility of encountering recommended core conditions in a clinical setting during a 4(th)-year EM clerkship. Students from three institutions participated in this ongoing, prospective observation study. Students' patient logs were collected during 4-week EM clerkships between July 2011 and June 2012. De-identified logs were reviewed and the number of patient encounters for each of the CDEM-identified emergent conditions was recorded. The percentage of students who saw each of the core complaints was calculated, as was the average number of core complaints seen by each. Data from 130 students at three institutions were captured; 15.4% of students saw all 10 conditions during their rotation, and 76.9% saw at least eight. The average number of conditions seen per student was 8.4 (range of 7.0-8.6). The percentage of students who saw each condition varied, ranging from 100% (chest pain and abdominal pain) to 31% (cardiac arrest).\n\nQuestion: Medical student education in emergency medicine: do students meet the national standards for clinical encounters of selected core conditions?", "question_only": "Medical student education in emergency medicine: do students meet the national standards for clinical encounters of selected core conditions?", "context": "Establishing a core curriculum for undergraduate Emergency Medicine (EM) education is crucial to development of the specialty. The Clerkship Directors in Emergency Medicine (CDEM) National Curriculum Task Force recommended that all students in a 4(th)-year EM clerkship be exposed to 10 emergent clinical conditions. To evaluate the feasibility of encountering recommended core conditions in a clinical setting during a 4(th)-year EM clerkship. Students from three institutions participated in this ongoing, prospective observation study. Students' patient logs were collected during 4-week EM clerkships between July 2011 and June 2012. De-identified logs were reviewed and the number of patient encounters for each of the CDEM-identified emergent conditions was recorded. The percentage of students who saw each of the core complaints was calculated, as was the average number of core complaints seen by each. Data from 130 students at three institutions were captured; 15.4% of students saw all 10 conditions during their rotation, and 76.9% saw at least eight. The average number of conditions seen per student was 8.4 (range of 7.0-8.6). The percentage of students who saw each condition varied, ranging from 100% (chest pain and abdominal pain) to 31% (cardiac arrest).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Most students do not encounter all 10 conditions during patient encounters throughout a 4-week EM rotation, although most have exposure to at least eight. Certain conditions are far less likely than others to be encountered, and may need to be taught in a nonclinical setting.", "meshes": ["Clinical Clerkship", "Curriculum", "Education, Medical, Undergraduate", "Emergency Medicine", "Feasibility Studies", "Humans", "Prospective Studies"], "year": "2014"}
{"id": "pubmedqa_21739621", "dataset": "pubmedqa", "question": "Context: To examine longitudinal patterns in body mass index (BMI) over 14 years and its association with knee pain in the Chingford Study. We studied a total of 594 women with BMI data from clinic visits at years (Y) 1, 5, 10, and 15. Knee pain at Y15 was assessed by questionnaire. Associations between BMI over 14 years and knee pain at Y15 were examined using logistic regression. BMI significantly increased from Y1 to Y15 (P<0.0005) with medians (interquartile ranges) of 24.5 kg/m(2)  (22.5-27.2 kg/m(2) ) and 26.5 kg/m(2)  (23.9-30.1 kg/m(2) ), respectively. At Y15, 45.1% of subjects had knee pain. A greater BMI at Y1 (odds ratio [OR] 1.34, 95% confidence interval [95% CI]1.05-1.69), at Y15 (OR 1.34, 95% CI 1.10-1.61), and change in BMI over 15 years (OR 1.40, 95% CI 1.00-1.93) were significant predictors of knee pain at Y15 (P<0.05). BMI change was associated with bilateral (OR 1.61, 95% CI 1.05-1.76, P = 0.024) but not unilateral knee pain (OR 1.22, 95% CI 0.73-1.76, P = 0.298). The association between BMI change and knee pain was independent of radiographic knee osteoarthritis (OA). The strength of association between BMI and knee pain at Y15 was similar during followup measurements.\n\nQuestion: Does obesity predict knee pain over fourteen years in women, independently of radiographic changes?", "question_only": "Does obesity predict knee pain over fourteen years in women, independently of radiographic changes?", "context": "To examine longitudinal patterns in body mass index (BMI) over 14 years and its association with knee pain in the Chingford Study. We studied a total of 594 women with BMI data from clinic visits at years (Y) 1, 5, 10, and 15. Knee pain at Y15 was assessed by questionnaire. Associations between BMI over 14 years and knee pain at Y15 were examined using logistic regression. BMI significantly increased from Y1 to Y15 (P<0.0005) with medians (interquartile ranges) of 24.5 kg/m(2)  (22.5-27.2 kg/m(2) ) and 26.5 kg/m(2)  (23.9-30.1 kg/m(2) ), respectively. At Y15, 45.1% of subjects had knee pain. A greater BMI at Y1 (odds ratio [OR] 1.34, 95% confidence interval [95% CI]1.05-1.69), at Y15 (OR 1.34, 95% CI 1.10-1.61), and change in BMI over 15 years (OR 1.40, 95% CI 1.00-1.93) were significant predictors of knee pain at Y15 (P<0.05). BMI change was associated with bilateral (OR 1.61, 95% CI 1.05-1.76, P = 0.024) but not unilateral knee pain (OR 1.22, 95% CI 0.73-1.76, P = 0.298). The association between BMI change and knee pain was independent of radiographic knee osteoarthritis (OA). The strength of association between BMI and knee pain at Y15 was similar during followup measurements.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Over 14 years, a higher BMI predicts knee pain at Y15 in women, independently of radiographic knee OA. When adjusted, the association was significant in bilateral, not unilateral, knee pain, suggesting alternative pathologic mechanisms may exist. The longitudinal effect of BMI on knee pain at Y15 is equally important at any time point, which may assist reducing the population burden of knee pain.", "meshes": ["Aged", "Arthralgia", "Body Mass Index", "England", "Female", "Humans", "Knee Joint", "Logistic Models", "Longitudinal Studies", "Middle Aged", "Obesity", "Odds Ratio", "Osteoarthritis, Knee", "Pain Measurement", "Prospective Studies", "Radiography", "Risk Assessment", "Risk Factors", "Surveys and Questionnaires", "Time Factors"], "year": "2011"}
{"id": "pubmedqa_23422012", "dataset": "pubmedqa", "question": "Context: Vancomycin is the primary treatment for infections caused by methicilin-resistant Staphylococcus aureus (MRSA). The association of vancomycin treatment failures with increased vancomycin minimum inhibitory concentration (MIC) is a well-recognized problem. A number of single-centre studies have identified progressive increases in glycopeptide MICs for S. aureus strains over recent years - a phenomenon known as vancomycin MIC creep. It is unknown if this is a worldwide phenomenon or if it is localized to specific centers. The aim of this study was to evaluate the trend of vancomycin MIC for isolates of MRSA over a 3-year period in a tertiary university hospital in Portugal. MRSA isolates from samples of patients admitted from January 2007 to December 2009 were assessed. Etest method was used to determine the respective vancomycin MIC. Only one isolate per patient was included in the final analysis. A total of 93 MRSA isolates were studied. The vancomycin MICs were 0.75, 1, 1.5 and 2 mg/L for 1 (1.1%), 19 (20.4%), 38 (40.9%), 35 (37.6%) isolates, respectively. During the 3 year period, we observed a significant fluctuation in the rate of MRSA with a vancomycin MIC > 1 mg/L (2007: 86.2%; 2008: 93.3%; 2009: 58.8%, p = 0.002). No MRSA isolate presented a MIC > 2 mg/L.\n\nQuestion: Is vancomycin MIC creep a worldwide phenomenon?", "question_only": "Is vancomycin MIC creep a worldwide phenomenon?", "context": "Vancomycin is the primary treatment for infections caused by methicilin-resistant Staphylococcus aureus (MRSA). The association of vancomycin treatment failures with increased vancomycin minimum inhibitory concentration (MIC) is a well-recognized problem. A number of single-centre studies have identified progressive increases in glycopeptide MICs for S. aureus strains over recent years - a phenomenon known as vancomycin MIC creep. It is unknown if this is a worldwide phenomenon or if it is localized to specific centers. The aim of this study was to evaluate the trend of vancomycin MIC for isolates of MRSA over a 3-year period in a tertiary university hospital in Portugal. MRSA isolates from samples of patients admitted from January 2007 to December 2009 were assessed. Etest method was used to determine the respective vancomycin MIC. Only one isolate per patient was included in the final analysis. A total of 93 MRSA isolates were studied. The vancomycin MICs were 0.75, 1, 1.5 and 2 mg/L for 1 (1.1%), 19 (20.4%), 38 (40.9%), 35 (37.6%) isolates, respectively. During the 3 year period, we observed a significant fluctuation in the rate of MRSA with a vancomycin MIC > 1 mg/L (2007: 86.2%; 2008: 93.3%; 2009: 58.8%, p = 0.002). No MRSA isolate presented a MIC > 2 mg/L.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "We were unable to find in our institution data compatible to the presence of vancomycin MIC creep during the study period. This phenomenon seems not to be generalized; as a result each institution should systematically monitor MRSA vancomycin MIC over time.", "meshes": ["Anti-Bacterial Agents", "Hospitals, University", "Humans", "Microbial Sensitivity Tests", "Staphylococcus aureus", "Vancomycin"], "year": "2013"}
{"id": "pubmedqa_23412195", "dataset": "pubmedqa", "question": "Context: This study was designed to compare clinical effectiveness of operative with nonoperative treatment for displaced midshaft clavicular fractures (DMCF). We systematically searched electronic databases (MEDILINE, EMBASE, CLINICAL, OVID, BIOSIS and Cochrane registry of controlled clinical trials) to identify randomized controlled trials (RCTs) in which operative treatment was compared with nonoperative treatment for DMCF from 1980 to 2012. The methodologic quality of trials was assessed. Data from chosen studies were pooled with using of fixed-effects and random-effects models with mean differences and risk ratios for continuous and dichotomous variables, respectively. Four RCTs with a total of 321 patients were screened for the present study. Results showed that the operative treatment was superior to the nonoperative treatment regarding the rate of nonunion [95 % confidence interval (CI) (0.05, 0.43), P = 0.0004], malunion [95 % CI (0.06, 0.34), P < 0.00001] and overall complication [95 % CI (0.43-0.76), P = 0.0001]. Subgroup analyses of complications revealed that significant differences were existed in the incidence of neurologic symptoms [95 % CI (0.20, 0.74), P = 0.004] and dissatisfaction with appearance [95 % CI (0.19, 0.65), P = 0.001]. Lack of consistent and standardized assessment data, insufficiency analysis that carried out showed improved functional outcomes (P < 0.05) in operative treatment.\n\nQuestion: Should displaced midshaft clavicular fractures be treated surgically?", "question_only": "Should displaced midshaft clavicular fractures be treated surgically?", "context": "This study was designed to compare clinical effectiveness of operative with nonoperative treatment for displaced midshaft clavicular fractures (DMCF). We systematically searched electronic databases (MEDILINE, EMBASE, CLINICAL, OVID, BIOSIS and Cochrane registry of controlled clinical trials) to identify randomized controlled trials (RCTs) in which operative treatment was compared with nonoperative treatment for DMCF from 1980 to 2012. The methodologic quality of trials was assessed. Data from chosen studies were pooled with using of fixed-effects and random-effects models with mean differences and risk ratios for continuous and dichotomous variables, respectively. Four RCTs with a total of 321 patients were screened for the present study. Results showed that the operative treatment was superior to the nonoperative treatment regarding the rate of nonunion [95 % confidence interval (CI) (0.05, 0.43), P = 0.0004], malunion [95 % CI (0.06, 0.34), P < 0.00001] and overall complication [95 % CI (0.43-0.76), P = 0.0001]. Subgroup analyses of complications revealed that significant differences were existed in the incidence of neurologic symptoms [95 % CI (0.20, 0.74), P = 0.004] and dissatisfaction with appearance [95 % CI (0.19, 0.65), P = 0.001]. Lack of consistent and standardized assessment data, insufficiency analysis that carried out showed improved functional outcomes (P < 0.05) in operative treatment.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The available evidence suggests that the operative treatment for DMCF is associated with a lower rate of nonunion, malunion and complication than nonoperative treatment. This study supports traditional primary operative treatment for DMCF in active adults.", "meshes": ["Clavicle", "Databases, Factual", "Fracture Fixation, Internal", "Fractures, Bone", "Humans", "Postoperative Complications", "Randomized Controlled Trials as Topic", "Recovery of Function", "Treatment Outcome"], "year": "2013"}
{"id": "pubmedqa_10381996", "dataset": "pubmedqa", "question": "Context: To determine whether the use of empiric chest radiography (CXR) is of significant value in detecting clinically unsuspected acute chest syndrome (ACS) in febrile patients with sickle cell disease (SCD). Patients with SCD presenting to the emergency department and hematology clinic with temperature greater than or equal to 38 degrees C were prospectively evaluated using a physician-completed questionnaire. The questionnaire included inquiries into the patient's physical signs and symptoms and the physician's clinical impression for the presence of ACS. The questionnaire was completed before obtaining CXR results in all patients. Seventy-three patients with SCD with 96 febrile events were evaluated over a 1-year period. Twenty-four percent (23/96) of the patients had CXR evidence of ACS. On the basis of the questionnaire data, 61% (14/23) of ACS cases were not clinically suspected by the evaluating physician before obtaining CXR. Comparing the patients with and without ACS revealed that, with the exception of splinting (4/23 [17%] versus 0/73 [0%]), no symptom or physical examination finding helped to identify which patients had ACS. Fifty-seven percent of patients with ACS had completely normal findings on physical examination. The presentation of patients with clinically detected versus clinically unsuspected ACS also did not differ significantly. Length of hospitalization, oxygen use, and need for transfusion were the same in both the unsuspected and detected ACS groups. Overall physician sensitivity for predicting ACS was only 39%, and diagnostic accuracy did not improve significantly with increasing levels of pediatric training.\n\nQuestion: Clinician assessment for acute chest syndrome in febrile patients with sickle cell disease: is it accurate enough?", "question_only": "Clinician assessment for acute chest syndrome in febrile patients with sickle cell disease: is it accurate enough?", "context": "To determine whether the use of empiric chest radiography (CXR) is of significant value in detecting clinically unsuspected acute chest syndrome (ACS) in febrile patients with sickle cell disease (SCD). Patients with SCD presenting to the emergency department and hematology clinic with temperature greater than or equal to 38 degrees C were prospectively evaluated using a physician-completed questionnaire. The questionnaire included inquiries into the patient's physical signs and symptoms and the physician's clinical impression for the presence of ACS. The questionnaire was completed before obtaining CXR results in all patients. Seventy-three patients with SCD with 96 febrile events were evaluated over a 1-year period. Twenty-four percent (23/96) of the patients had CXR evidence of ACS. On the basis of the questionnaire data, 61% (14/23) of ACS cases were not clinically suspected by the evaluating physician before obtaining CXR. Comparing the patients with and without ACS revealed that, with the exception of splinting (4/23 [17%] versus 0/73 [0%]), no symptom or physical examination finding helped to identify which patients had ACS. Fifty-seven percent of patients with ACS had completely normal findings on physical examination. The presentation of patients with clinically detected versus clinically unsuspected ACS also did not differ significantly. Length of hospitalization, oxygen use, and need for transfusion were the same in both the unsuspected and detected ACS groups. Overall physician sensitivity for predicting ACS was only 39%, and diagnostic accuracy did not improve significantly with increasing levels of pediatric training.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "ACS is common in patients with SCD who present with fever and was grossly underestimated by evaluating physicians. History and physical examination appear to be of little value in defining which febrile patients require CXR. In view of the mortality and morbidity associated with ACS, empiric CXR should be considered when evaluating a febrile patient with SCD.", "meshes": ["Acute Disease", "Adolescent", "Anemia, Sickle Cell", "Blood Transfusion", "Child", "Child, Preschool", "Diagnosis, Differential", "Emergency Treatment", "Female", "Fever", "Humans", "Infant", "Length of Stay", "Male", "Oxygen Inhalation Therapy", "Physical Examination", "Pneumonia", "Prospective Studies", "Radiography", "Reproducibility of Results", "Single-Blind Method", "Surveys and Questionnaires", "Syndrome"], "year": "1999"}
{"id": "pubmedqa_19409117", "dataset": "pubmedqa", "question": "Context: To evaluate the efficacy of extracorporeal shock wave lithotripsy (SWL) on lower calyceal calculi in relation to the renal anatomical factors and determine which of these factors can be used to select patients who will benefit from SWL. We analyzed retrospectively 78 patients with single radiopaque lower calyceal stones treated with SWL. The patients were evaluated 3 months after lithotripsy with a simple abdominal X-ray and a kidney ultrasound scan. The success of the treatment, removal of all fragments, was correlated with renal anatomical factors measured in the pre-treatment intravenous urography: infundibulopelvic angle, lower infundibulum width, lower infundibulum length, ratio length/width, infundibulum height, and number of minor calyces in the lower calyceal group. Three months after SWL treatment, 39 patients were stone-free (NR group) and 39 had residual fragments (R group). Both groups presented no differences in relation to infundibulopelvic angle, width and length of the lower calyceal infundibulum, length/width ratio of the lower infundibulum or number of lower calyces. Height of the infundibulum, described as the distance between the line passing through the lowest part of the calyx containing the calculus and the highest point of the lower lip of renal pelvis, was the only parameter in which significant differences (p = 0.002) were found between the NR and R groups.\n\nQuestion: Can infundibular height predict the clearance of lower pole calyceal stone after extracorporeal shockwave lithotripsy?", "question_only": "Can infundibular height predict the clearance of lower pole calyceal stone after extracorporeal shockwave lithotripsy?", "context": "To evaluate the efficacy of extracorporeal shock wave lithotripsy (SWL) on lower calyceal calculi in relation to the renal anatomical factors and determine which of these factors can be used to select patients who will benefit from SWL. We analyzed retrospectively 78 patients with single radiopaque lower calyceal stones treated with SWL. The patients were evaluated 3 months after lithotripsy with a simple abdominal X-ray and a kidney ultrasound scan. The success of the treatment, removal of all fragments, was correlated with renal anatomical factors measured in the pre-treatment intravenous urography: infundibulopelvic angle, lower infundibulum width, lower infundibulum length, ratio length/width, infundibulum height, and number of minor calyces in the lower calyceal group. Three months after SWL treatment, 39 patients were stone-free (NR group) and 39 had residual fragments (R group). Both groups presented no differences in relation to infundibulopelvic angle, width and length of the lower calyceal infundibulum, length/width ratio of the lower infundibulum or number of lower calyces. Height of the infundibulum, described as the distance between the line passing through the lowest part of the calyx containing the calculus and the highest point of the lower lip of renal pelvis, was the only parameter in which significant differences (p = 0.002) were found between the NR and R groups.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Lower Infundibular height could be a good measurement tool for deciding which patients with lower calyceal lithiasis would benefit from SWL treatment. Height of less than 22 mm suggests a good outcome from lithotripsy.", "meshes": ["Female", "Humans", "Kidney Calculi", "Kidney Calices", "Lithotripsy", "Logistic Models", "Male", "Middle Aged", "ROC Curve", "Retrospective Studies", "Treatment Outcome"], "year": null}
{"id": "pubmedqa_23588461", "dataset": "pubmedqa", "question": "Context: Ascitis and undernutrition are frequent complications of cirrhosis, however ascitis volume and anthropometric assessment are not routinely documented or considered in prognostic evaluation. In a homogeneous cohort followed during two years these variables were scrutinized, aiming to ascertain relevance for longterm outcome. Population (N = 25, all males with alcoholic cirrhosis) was recruited among patients hospitalized for uncomplicated ascitis. Exclusion criteria were refractory or tense ascitis, cancer, spontaneous bacterial peritonitis, bleeding varices and critical illness. Measurements included ultrasonographically estimated ascitis volume, dry body mass index/BMI , upper arm anthropometrics, hematologic counts and liver function tests. Population (age 48.3 ± 11.3 years, BMI 21.1 ± 3.5 kg/m², serum albumin 2.5 ± 0.8 g/dL) was mostly in the Child-Pugh C category (77.8%) but clinically stable. During the follow-up period of 22.6 ± 3.8 months, additional hospitalizations numbered 1.7 ± 1.0 and more than one quarter succumbed. Admission ascitis volume corresponded to 7.1 ± 3.6 L and dry BMI to 18.3 ± 3.5 kg/m². Child Pugh index was relevant for both mortality and rehospitalization. Nevertheless, similar matches for mortality were documented with ascitis volume and dry BMI, and arm circumference below the 5th percentile was highly significantly associated with rehospitalization.\n\nQuestion: Should ascitis volume and anthropometric measurements be estimated in hospitalized alcoholic cirrotics?", "question_only": "Should ascitis volume and anthropometric measurements be estimated in hospitalized alcoholic cirrotics?", "context": "Ascitis and undernutrition are frequent complications of cirrhosis, however ascitis volume and anthropometric assessment are not routinely documented or considered in prognostic evaluation. In a homogeneous cohort followed during two years these variables were scrutinized, aiming to ascertain relevance for longterm outcome. Population (N = 25, all males with alcoholic cirrhosis) was recruited among patients hospitalized for uncomplicated ascitis. Exclusion criteria were refractory or tense ascitis, cancer, spontaneous bacterial peritonitis, bleeding varices and critical illness. Measurements included ultrasonographically estimated ascitis volume, dry body mass index/BMI , upper arm anthropometrics, hematologic counts and liver function tests. Population (age 48.3 ± 11.3 years, BMI 21.1 ± 3.5 kg/m², serum albumin 2.5 ± 0.8 g/dL) was mostly in the Child-Pugh C category (77.8%) but clinically stable. During the follow-up period of 22.6 ± 3.8 months, additional hospitalizations numbered 1.7 ± 1.0 and more than one quarter succumbed. Admission ascitis volume corresponded to 7.1 ± 3.6 L and dry BMI to 18.3 ± 3.5 kg/m². Child Pugh index was relevant for both mortality and rehospitalization. Nevertheless, similar matches for mortality were documented with ascitis volume and dry BMI, and arm circumference below the 5th percentile was highly significantly associated with rehospitalization.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "A greater association than hitherto acknowledged, between ascitis volume and anthropometric measurements from one side, and long-term rehospitalization and mortality from the other, was demonstrated in male stable alcoholic cirrhotics. Further studies with alcoholic and other modalities of cirrhosis including women are recommended.", "meshes": ["Adult", "Anthropometry", "Ascites", "Ascitic Fluid", "Blood Chemical Analysis", "Cohort Studies", "Female", "Hospitalization", "Humans", "Liver Cirrhosis, Alcoholic", "Male", "Middle Aged", "Recurrence", "Treatment Outcome"], "year": null}
{"id": "pubmedqa_7664228", "dataset": "pubmedqa", "question": "Context: To determine whether decreasing lengths of stay over time for selected diagnostic categories were associated with increased hospital readmission rates and mean number of physician visits after discharge. Retrospective descriptive study. The seven large (125 beds or more) acute care hospitals in Winnipeg. Manitoba residents admitted to any one of the seven hospitals because acute myocardial infarction (AMI), bronchitis or asthma, transurethral prostatectomy (TURP) and uterine or adnexal procedures for nonmalignant disease during the fiscal years 1989-90 to 1992-93. Patients from out of province, those who died in hospital, those with excessively long stays (more than 60 days) and those who were transferred to or from another institution were excluded. Length of hospital stay, and rate of readmission within 30 days after discharge for all four categories and mean number of physician visits within 30 days after discharge for two categories (AMI and bronchitis or asthma. The length of stay decreased significantly over the 4 years for all of the four categories, the smallest change being observed for patients with AMI (11.1%) and the largest for those with bronchitis or asthma (22.0%). The readmission rates for AMI, bronchitis or asthma, and TURP showed no consistent change over the 4 years. The readmission rate for uterine or adnexal procedures increased significantly between the first and second year (chi 2 = 4.28, p = 0.04) but then remained constant over the next 3 years. The mean number of physician visits increased slightly for AMI in the first year (1.92 to 2.01) and then remained virtually the same. It decreased slightly for bronchitis or asthma over the 4 years. There was no significant correlation between length of stay and readmission rates for individual hospitals in 1992-93 in any of the four categories. Also, no correlation was observed between length of stay and mean number of physician visits for individual hospitals in 1992-93 in the categories AMI and bronchitis or asthma.\n\nQuestion: Discharging patients earlier from Winnipeg hospitals: does it adversely affect quality of care?", "question_only": "Discharging patients earlier from Winnipeg hospitals: does it adversely affect quality of care?", "context": "To determine whether decreasing lengths of stay over time for selected diagnostic categories were associated with increased hospital readmission rates and mean number of physician visits after discharge. Retrospective descriptive study. The seven large (125 beds or more) acute care hospitals in Winnipeg. Manitoba residents admitted to any one of the seven hospitals because acute myocardial infarction (AMI), bronchitis or asthma, transurethral prostatectomy (TURP) and uterine or adnexal procedures for nonmalignant disease during the fiscal years 1989-90 to 1992-93. Patients from out of province, those who died in hospital, those with excessively long stays (more than 60 days) and those who were transferred to or from another institution were excluded. Length of hospital stay, and rate of readmission within 30 days after discharge for all four categories and mean number of physician visits within 30 days after discharge for two categories (AMI and bronchitis or asthma. The length of stay decreased significantly over the 4 years for all of the four categories, the smallest change being observed for patients with AMI (11.1%) and the largest for those with bronchitis or asthma (22.0%). The readmission rates for AMI, bronchitis or asthma, and TURP showed no consistent change over the 4 years. The readmission rate for uterine or adnexal procedures increased significantly between the first and second year (chi 2 = 4.28, p = 0.04) but then remained constant over the next 3 years. The mean number of physician visits increased slightly for AMI in the first year (1.92 to 2.01) and then remained virtually the same. It decreased slightly for bronchitis or asthma over the 4 years. There was no significant correlation between length of stay and readmission rates for individual hospitals in 1992-93 in any of the four categories. Also, no correlation was observed between length of stay and mean number of physician visits for individual hospitals in 1992-93 in the categories AMI and bronchitis or asthma.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Improving hospital efficiency by shortening length of stay does not appear to result in increased rates of readmission or numbers of physician visits within 30 days after discharge from hospital. Research is needed to identify optimal lengths of stay and expected readmission rates.", "meshes": ["Diagnosis-Related Groups", "Humans", "Length of Stay", "Manitoba", "Office Visits", "Patient Readmission", "Quality of Health Care", "Retrospective Studies"], "year": "1995"}
{"id": "pubmedqa_10605400", "dataset": "pubmedqa", "question": "Context: As part of an MRC funded study into primary care oral anticoagulation management, INR measurements obtained in general practice were validated against values on the same samples obtained in hospital laboratories. A prospective comparative trial was undertaken between three hospital laboratories and nine general practices. All patients attending general practice based anticoagulant clinics had parallel INR estimations performed in general practice and in a hospital laboratory. 405 tests were performed. Comparison between results obtained in the practices and those in the reference hospital laboratory (gold standard), which used the same method of testing for INR, showed a correlation coefficient of 0.96. Correlation coefficients comparing the results with the various standard laboratory techniques ranged from 0.86 to 0.92. It was estimated that up to 53% of tests would have resulted in clinically significant differences (change in warfarin dose) depending upon the site and method of testing. The practice derived results showed a positive bias ranging from 0.28 to 1.55, depending upon the site and method of testing.\n\nQuestion: Is the international normalised ratio (INR) reliable?", "question_only": "Is the international normalised ratio (INR) reliable?", "context": "As part of an MRC funded study into primary care oral anticoagulation management, INR measurements obtained in general practice were validated against values on the same samples obtained in hospital laboratories. A prospective comparative trial was undertaken between three hospital laboratories and nine general practices. All patients attending general practice based anticoagulant clinics had parallel INR estimations performed in general practice and in a hospital laboratory. 405 tests were performed. Comparison between results obtained in the practices and those in the reference hospital laboratory (gold standard), which used the same method of testing for INR, showed a correlation coefficient of 0.96. Correlation coefficients comparing the results with the various standard laboratory techniques ranged from 0.86 to 0.92. It was estimated that up to 53% of tests would have resulted in clinically significant differences (change in warfarin dose) depending upon the site and method of testing. The practice derived results showed a positive bias ranging from 0.28 to 1.55, depending upon the site and method of testing.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "No technical problems associated with INR testing within primary care were uncovered. Discrepant INR results are as problematic in hospital settings as they are in primary care. These data highlight the failings of the INR to standardise when different techniques and reagents are used, an issue which needs to be resolved. For primary care to become more involved in therapeutic oral anticoagulation monitoring, close links are needed between hospital laboratories and practices, particularly with regard to training and quality assurance.", "meshes": ["Anticoagulants", "Family Practice", "Humans", "International Normalized Ratio", "Laboratories, Hospital", "Monitoring, Physiologic", "Point-of-Care Systems", "Prospective Studies", "Reproducibility of Results", "Warfarin"], "year": "1999"}
{"id": "pubmedqa_23224030", "dataset": "pubmedqa", "question": "Context: Individuals with type 1 diabetes have a high risk of developing cardiovascular diseases, and it has been reported that they consume a high atherogenic diet. We examined how nutrient intake and adherence to current European nutritional recommendations evolved in a large cohort of European individuals with type 1 diabetes over a period of 7 years.SUBJECTS/ We analysed data from the EURODIAB Prospective Complications Study, a European multicentre prospective cohort study. Standardized 3-day dietary records were employed in individuals with type 1 diabetes. One thousand one hundred and two patients (553 men, 549 women, baseline age 33 ± 10 years, duration 15 ± 9 years) had complete nutritional data available at baseline and after 7 years. We calculated mean differences in reported nutrients over time and adjusted these for age, gender, HbA1c and BMI with ANOVA models. Compared to baseline, there were minor changes in nutrients. Reported protein (-0.35% energy (en), fat (-1.07% en), saturated fat (-0.25% en) and cholesterol (-7.42 mg/1000 kcal) intakes were lower, whereas carbohydrate (+1.23% en) and fibre (+0.46 g/1000 kcal) intakes were higher at the 7-year follow-up. European recommendations for adequate nutrient intakes were followed in individuals with type 1 diabetes for protein (76% at baseline and 78% at follow-up), moderately for fat (34, 40%), carbohydrate (34, 41%) and cholesterol (39, 47%), but poorly for fibre (1.4, 2.4%) and saturated fat (11, 13%).\n\nQuestion: Do European people with type 1 diabetes consume a high atherogenic diet?", "question_only": "Do European people with type 1 diabetes consume a high atherogenic diet?", "context": "Individuals with type 1 diabetes have a high risk of developing cardiovascular diseases, and it has been reported that they consume a high atherogenic diet. We examined how nutrient intake and adherence to current European nutritional recommendations evolved in a large cohort of European individuals with type 1 diabetes over a period of 7 years.SUBJECTS/ We analysed data from the EURODIAB Prospective Complications Study, a European multicentre prospective cohort study. Standardized 3-day dietary records were employed in individuals with type 1 diabetes. One thousand one hundred and two patients (553 men, 549 women, baseline age 33 ± 10 years, duration 15 ± 9 years) had complete nutritional data available at baseline and after 7 years. We calculated mean differences in reported nutrients over time and adjusted these for age, gender, HbA1c and BMI with ANOVA models. Compared to baseline, there were minor changes in nutrients. Reported protein (-0.35% energy (en), fat (-1.07% en), saturated fat (-0.25% en) and cholesterol (-7.42 mg/1000 kcal) intakes were lower, whereas carbohydrate (+1.23% en) and fibre (+0.46 g/1000 kcal) intakes were higher at the 7-year follow-up. European recommendations for adequate nutrient intakes were followed in individuals with type 1 diabetes for protein (76% at baseline and 78% at follow-up), moderately for fat (34, 40%), carbohydrate (34, 41%) and cholesterol (39, 47%), but poorly for fibre (1.4, 2.4%) and saturated fat (11, 13%).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "European individuals with type 1 diabetes consume a high atherogenic diet as few patients met recommendations for dietary fibre and saturated fat. This study showed minor changes in dietary nutrients and energy intakes over a period of 7 years. Nutrition education needs particular focus on strategies to increase dietary fibre and reduce saturated fat to exploit their potential benefit.", "meshes": ["Adolescent", "Adult", "Body Mass Index", "Body Weight", "Cardiovascular Diseases", "Cholesterol", "Cholesterol, Dietary", "Diabetes Mellitus, Type 1", "Diet Records", "Diet, Atherogenic", "Dietary Carbohydrates", "Dietary Fiber", "Dietary Proteins", "Energy Intake", "European Continental Ancestry Group", "Female", "Follow-Up Studies", "Humans", "Insulin", "Male", "Middle Aged", "Motor Activity", "Nutrition Assessment", "Nutritional Status", "Prospective Studies", "Recommended Dietary Allowances", "Young Adult"], "year": "2013"}
{"id": "pubmedqa_24298614", "dataset": "pubmedqa", "question": "Context: The clinical and prognostic value of the previous node classification of TNM staging in early gastric cancer (EGC) has been less definitive. The aim was to assess the suitability of the revised node staging for prediction of clinical behavior of EGC. Between 2005 and 2008, 1,845 patients were diagnosed with EGC and underwent surgery at Severance Hospitals. Clinicopathological characteristics were analyzed with comparisons between sixth and seventh TNM staging. When comparing IB with IIA upstaged based on seventh staging, poor differentiation, signet ring cell, diffuse, undifferentiated types, perineural invasion (PNI), larger size and younger age, were more significantly associated with IIA. Clinicopathological factors were compared between N0/N1 and N2/N3 based on both staging. In mucosal cancer, younger age, diffuse and undifferentiated types were more significantly associated with N2/N3 based on seventh staging. In submucosal cancer, larger size, poor differentiation, signet ring cell, diffuse, undifferentiated types, PNI and deeper submucosal invasion, were more significantly associated with N2/N3 based on seventh staging.\n\nQuestion: Is the 7th TNM edition suitable for biological predictor in early gastric cancer?", "question_only": "Is the 7th TNM edition suitable for biological predictor in early gastric cancer?", "context": "The clinical and prognostic value of the previous node classification of TNM staging in early gastric cancer (EGC) has been less definitive. The aim was to assess the suitability of the revised node staging for prediction of clinical behavior of EGC. Between 2005 and 2008, 1,845 patients were diagnosed with EGC and underwent surgery at Severance Hospitals. Clinicopathological characteristics were analyzed with comparisons between sixth and seventh TNM staging. When comparing IB with IIA upstaged based on seventh staging, poor differentiation, signet ring cell, diffuse, undifferentiated types, perineural invasion (PNI), larger size and younger age, were more significantly associated with IIA. Clinicopathological factors were compared between N0/N1 and N2/N3 based on both staging. In mucosal cancer, younger age, diffuse and undifferentiated types were more significantly associated with N2/N3 based on seventh staging. In submucosal cancer, larger size, poor differentiation, signet ring cell, diffuse, undifferentiated types, PNI and deeper submucosal invasion, were more significantly associated with N2/N3 based on seventh staging.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Upstaging in EGC based on the revised TNM staging reflects more aggressive biological behavior of cancer. The new TNM staging system may be informative in prediction of biological behavior of EGC as well as prognosis and survival.", "meshes": ["Adult", "Aged", "Female", "Gastric Mucosa", "Humans", "Male", "Middle Aged", "Neoplasm Invasiveness", "Neoplasm Staging", "Stomach Neoplasms"], "year": null}
{"id": "pubmedqa_18507507", "dataset": "pubmedqa", "question": "Context: Specialty pharmaceuticals have evolved beyond their status as niche drugs designed to treat rare conditions and are now poised to become the standard of care in a wide variety of common chronic illnesses. Due in part to the cost of these therapies, payers are increasingly demanding evidence of their value. Determining the value of these medications is hampered by a lack of robust pharmacoeconomic data. To outline emerging strategies and case study examples for the medical and pharmacy benefits management of specialty pharmaceuticals. The promise of specialty pharmaceuticals: increased life expectancy, improved quality of life, enhanced workplace productivity, decreased burden of disease, and reduced health care spending comes at a significant cost. These agents require special handling, administration, patient education, clinical support, and risk mitigation. Additionally, specialty drugs require distribution systems that ensure appropriate patient selection and data collection. With the specialty pharmaceutical pipeline overflowing with new medicines and an aging population increasingly relying on these novel treatments to treat common diseases, the challenge of managing the costs associated with these agents can be daunting. Aided by sophisticated pharmacoeconomic models to assess value, the cost impacts of these specialty drugs can be appropriately controlled.\n\nQuestion: The promise of specialty pharmaceuticals: are they worth the price?", "question_only": "The promise of specialty pharmaceuticals: are they worth the price?", "context": "Specialty pharmaceuticals have evolved beyond their status as niche drugs designed to treat rare conditions and are now poised to become the standard of care in a wide variety of common chronic illnesses. Due in part to the cost of these therapies, payers are increasingly demanding evidence of their value. Determining the value of these medications is hampered by a lack of robust pharmacoeconomic data. To outline emerging strategies and case study examples for the medical and pharmacy benefits management of specialty pharmaceuticals. The promise of specialty pharmaceuticals: increased life expectancy, improved quality of life, enhanced workplace productivity, decreased burden of disease, and reduced health care spending comes at a significant cost. These agents require special handling, administration, patient education, clinical support, and risk mitigation. Additionally, specialty drugs require distribution systems that ensure appropriate patient selection and data collection. With the specialty pharmaceutical pipeline overflowing with new medicines and an aging population increasingly relying on these novel treatments to treat common diseases, the challenge of managing the costs associated with these agents can be daunting. Aided by sophisticated pharmacoeconomic models to assess value, the cost impacts of these specialty drugs can be appropriately controlled.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Current evidence suggests that when used in targeted patient populations, specialty pharmaceuticals may represent a good health care value.", "meshes": ["Chronic Disease", "Drug Costs", "Humans", "Managed Care Programs", "Pharmaceutical Preparations", "Rare Diseases", "United States"], "year": "2008"}
{"id": "pubmedqa_23497210", "dataset": "pubmedqa", "question": "Context: Although record linkage of routinely collected health datasets is a valuable research resource, most datasets are established for administrative purposes and not for health outcomes research. In order for meaningful results to be extrapolated to specific populations, the limitations of the data and linkage methodology need to be investigated and clarified. It is the objective of this study to investigate the differences in ascertainment which may arise between a hospital admission dataset and a dispensing claims dataset, using major depression in pregnancy as an example. The safe use of antidepressants in pregnancy is an ongoing issue for clinicians with around 10% of pregnant women suffer from depression. As the birth admission will be the first admission to hospital during their pregnancy for most women, their use of antidepressants, or their depressive condition, may not be revealed to the attending hospital clinicians. This may result in adverse outcomes for the mother and infant. Population-based de-identified data were provided from the Western Australian Data Linkage System linking the administrative health records of women with a delivery to related records from the Midwives' Notification System, the Hospital Morbidity Data System and the national Pharmaceutical Benefits Scheme dataset. The women with depression during their pregnancy were ascertained in two ways: women with dispensing records relating to dispensed antidepressant medicines with an WHO ATC code to the 3rd level, pharmacological subgroup, 'N06A Antidepressants'; and, women with any hospital admission during pregnancy, including the birth admission, if a comorbidity was recorded relating to depression. From 2002 to 2005, there were 96698 births in WA. At least one antidepressant was dispensed to 4485 (4.6%) pregnant women. There were 3010 (3.1%) women with a comorbidity related to depression recorded on their delivery admission, or other admission to hospital during pregnancy. There were a total of 7495 pregnancies identified by either set of records. Using data linkage, we determined that these records represented 6596 individual pregnancies. Only 899 pregnancies were found in both groups (13.6% of all cases). 80% of women dispensed an antidepressant did not have depression recorded as a comorbidity on their hospital records. A simple capture-recapture calculation suggests the prevalence of depression in this population of pregnant women to be around 16%.\n\nQuestion: Are women with major depression in pregnancy identifiable in population health data?", "question_only": "Are women with major depression in pregnancy identifiable in population health data?", "context": "Although record linkage of routinely collected health datasets is a valuable research resource, most datasets are established for administrative purposes and not for health outcomes research. In order for meaningful results to be extrapolated to specific populations, the limitations of the data and linkage methodology need to be investigated and clarified. It is the objective of this study to investigate the differences in ascertainment which may arise between a hospital admission dataset and a dispensing claims dataset, using major depression in pregnancy as an example. The safe use of antidepressants in pregnancy is an ongoing issue for clinicians with around 10% of pregnant women suffer from depression. As the birth admission will be the first admission to hospital during their pregnancy for most women, their use of antidepressants, or their depressive condition, may not be revealed to the attending hospital clinicians. This may result in adverse outcomes for the mother and infant. Population-based de-identified data were provided from the Western Australian Data Linkage System linking the administrative health records of women with a delivery to related records from the Midwives' Notification System, the Hospital Morbidity Data System and the national Pharmaceutical Benefits Scheme dataset. The women with depression during their pregnancy were ascertained in two ways: women with dispensing records relating to dispensed antidepressant medicines with an WHO ATC code to the 3rd level, pharmacological subgroup, 'N06A Antidepressants'; and, women with any hospital admission during pregnancy, including the birth admission, if a comorbidity was recorded relating to depression. From 2002 to 2005, there were 96698 births in WA. At least one antidepressant was dispensed to 4485 (4.6%) pregnant women. There were 3010 (3.1%) women with a comorbidity related to depression recorded on their delivery admission, or other admission to hospital during pregnancy. There were a total of 7495 pregnancies identified by either set of records. Using data linkage, we determined that these records represented 6596 individual pregnancies. Only 899 pregnancies were found in both groups (13.6% of all cases). 80% of women dispensed an antidepressant did not have depression recorded as a comorbidity on their hospital records. A simple capture-recapture calculation suggests the prevalence of depression in this population of pregnant women to be around 16%.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "No single data source is likely to provide a complete health profile for an individual. For women with depression in pregnancy and dispensed antidepressants, the hospital admission data do not adequately capture all cases.", "meshes": ["Adult", "Antidepressive Agents", "Australia", "Databases, Factual", "Depressive Disorder, Major", "Female", "Hospital Records", "Humans", "Longitudinal Studies", "Medical Record Linkage", "Medical Records Systems, Computerized", "Pregnancy", "Pregnancy Complications", "Prevalence"], "year": "2013"}
{"id": "pubmedqa_18158048", "dataset": "pubmedqa", "question": "Context: There is controversy surrounding the optimal management of the testicular remnant associated with the vanishing testes syndrome. Some urologists advocate the need for surgical exploration, whereas others believe this is unnecessary. These differing opinions are based on the variable reports of viable germ cell elements found within the testicular remnants. To better understand the pathology associated with this syndrome and the need for surgical management, we reviewed our experience regarding the incidence of viable germ cell elements within the testicular remnant. An institutional review board-approved, retrospective review was performed of all consecutive patients undergoing exploration for a nonpalpable testis at Eastern Virginia Medical School and Geisinger Medical Center between 1994 and 2006. Patients who were found to have spermatic vessels and a vas deferens exiting a closed internal inguinal ring were included in this analysis. Fifty-six patients underwent removal of the testicular remnant. Patient age ranged from 11 to 216 months. In 8 of the specimens (14%), we identified viable germ cell elements. In an additional 4 patients (7%), we identified seminiferous tubules without germ cell elements.\n\nQuestion: Histologic evaluation of the testicular remnant associated with the vanishing testes syndrome: is surgical management necessary?", "question_only": "Histologic evaluation of the testicular remnant associated with the vanishing testes syndrome: is surgical management necessary?", "context": "There is controversy surrounding the optimal management of the testicular remnant associated with the vanishing testes syndrome. Some urologists advocate the need for surgical exploration, whereas others believe this is unnecessary. These differing opinions are based on the variable reports of viable germ cell elements found within the testicular remnants. To better understand the pathology associated with this syndrome and the need for surgical management, we reviewed our experience regarding the incidence of viable germ cell elements within the testicular remnant. An institutional review board-approved, retrospective review was performed of all consecutive patients undergoing exploration for a nonpalpable testis at Eastern Virginia Medical School and Geisinger Medical Center between 1994 and 2006. Patients who were found to have spermatic vessels and a vas deferens exiting a closed internal inguinal ring were included in this analysis. Fifty-six patients underwent removal of the testicular remnant. Patient age ranged from 11 to 216 months. In 8 of the specimens (14%), we identified viable germ cell elements. In an additional 4 patients (7%), we identified seminiferous tubules without germ cell elements.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "In our review, we identified that a significant number of testicular remnants associated with the vanishing testes syndrome can harbor viable germ cell elements or seminiferous tubules. The exact fate of these residual elements remains unknown; however, there may exist the potential for malignant transformation. Given the potential for malignant degeneration, we believe that these remnants should be removed.", "meshes": ["Child, Preschool", "Cryptorchidism", "Germ Cells", "Humans", "Infant", "Male", "Seminiferous Tubules", "Testis"], "year": "2007"}
{"id": "pubmedqa_21198823", "dataset": "pubmedqa", "question": "Context: Elevation of cardiac troponin (cTn) is considered specific for myocardial damage. Elevated cTn and echocardiogrpahic documentation of wall motion abnormalities (WMAs) that were recorded after extreme physical effort raise the question whether dobutamine stress echo (DSE), can also induce elevation of troponin. we prospective enrolled stable patients (age>18 years) referred to DSE. The exam was performed under standardized conditions. Blood samples for cTnI were obtained at baseline and 18-24 hours after the test. We aimed to compare between the clinical and echocardiographic features of patients with elevated cTnI and those without cTnI elevations. Fifty-seven consecutive patients were included. The average age was 64.4 ± 10.7, 73% of the patients were males, and nearly half of the patients were known to have ischemic heart disease. Two of the patients were excluded due to technical difficulty. No signs of ischemia were recorded in 25 (45.4%). Among the patients with established ischemia on DSE, 12 (22%) had mild ischemia, 13 (23.6%) had moderate and 5 (9%) had severe ischemia. Angiography was performed in 13 (26%) of the patients, of which 7 had PCI and one was referred to bypass surgery. None of the patients had elevated cTnI 18-24 hours after the DSE.\n\nQuestion: Can dobutamine stress echocardiography induce cardiac troponin elevation?", "question_only": "Can dobutamine stress echocardiography induce cardiac troponin elevation?", "context": "Elevation of cardiac troponin (cTn) is considered specific for myocardial damage. Elevated cTn and echocardiogrpahic documentation of wall motion abnormalities (WMAs) that were recorded after extreme physical effort raise the question whether dobutamine stress echo (DSE), can also induce elevation of troponin. we prospective enrolled stable patients (age>18 years) referred to DSE. The exam was performed under standardized conditions. Blood samples for cTnI were obtained at baseline and 18-24 hours after the test. We aimed to compare between the clinical and echocardiographic features of patients with elevated cTnI and those without cTnI elevations. Fifty-seven consecutive patients were included. The average age was 64.4 ± 10.7, 73% of the patients were males, and nearly half of the patients were known to have ischemic heart disease. Two of the patients were excluded due to technical difficulty. No signs of ischemia were recorded in 25 (45.4%). Among the patients with established ischemia on DSE, 12 (22%) had mild ischemia, 13 (23.6%) had moderate and 5 (9%) had severe ischemia. Angiography was performed in 13 (26%) of the patients, of which 7 had PCI and one was referred to bypass surgery. None of the patients had elevated cTnI 18-24 hours after the DSE.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Our results indicate that there is no elevation of cTn despite the occurrence of significant WMAs on DSE. We conclude that cTnI cannot be used as an additional diagnostic tool during pharmacological stress test performed to evaluate the presence and severity of ischemia.", "meshes": ["Dobutamine", "Echocardiography", "Exercise Test", "Female", "Humans", "Male", "Middle Aged", "Myocardial Infarction", "Troponin T", "Vasodilator Agents"], "year": "2011"}
{"id": "pubmedqa_22303473", "dataset": "pubmedqa", "question": "Context: Family caregivers of dementia patients are at increased risk of developing depression or anxiety. A multi-component program designed to mobilize support of family networks demonstrated effectiveness in decreasing depressive symptoms in caregivers. However, the impact of an intervention consisting solely of family meetings on depression and anxiety has not yet been evaluated. This study examines the preventive effects of family meetings for primary caregivers of community-dwelling dementia patients. A randomized multicenter trial was conducted among 192 primary caregivers of community dwelling dementia patients. Caregivers did not meet the diagnostic criteria for depressive or anxiety disorder at baseline. Participants were randomized to the family meetings intervention (n = 96) or usual care (n = 96) condition. The intervention consisted of two individual sessions and four family meetings which occurred once every 2 to 3 months for a year. Outcome measures after 12 months were the incidence of a clinical depressive or anxiety disorder and change in depressive and anxiety symptoms (primary outcomes), caregiver burden and quality of life (secondary outcomes). Intention-to-treat as well as per protocol analyses were performed. A substantial number of caregivers (72/192) developed a depressive or anxiety disorder within 12 months. The intervention was not superior to usual care either in reducing the risk of disorder onset (adjusted IRR 0.98; 95% CI 0.69 to 1.38) or in reducing depressive (randomization-by-time interaction coefficient = -1.40; 95% CI -3.91 to 1.10) or anxiety symptoms (randomization-by-time interaction coefficient = -0.55; 95% CI -1.59 to 0.49). The intervention did not reduce caregiver burden or their health related quality of life.\n\nQuestion: Does a family meetings intervention prevent depression and anxiety in family caregivers of dementia patients?", "question_only": "Does a family meetings intervention prevent depression and anxiety in family caregivers of dementia patients?", "context": "Family caregivers of dementia patients are at increased risk of developing depression or anxiety. A multi-component program designed to mobilize support of family networks demonstrated effectiveness in decreasing depressive symptoms in caregivers. However, the impact of an intervention consisting solely of family meetings on depression and anxiety has not yet been evaluated. This study examines the preventive effects of family meetings for primary caregivers of community-dwelling dementia patients. A randomized multicenter trial was conducted among 192 primary caregivers of community dwelling dementia patients. Caregivers did not meet the diagnostic criteria for depressive or anxiety disorder at baseline. Participants were randomized to the family meetings intervention (n = 96) or usual care (n = 96) condition. The intervention consisted of two individual sessions and four family meetings which occurred once every 2 to 3 months for a year. Outcome measures after 12 months were the incidence of a clinical depressive or anxiety disorder and change in depressive and anxiety symptoms (primary outcomes), caregiver burden and quality of life (secondary outcomes). Intention-to-treat as well as per protocol analyses were performed. A substantial number of caregivers (72/192) developed a depressive or anxiety disorder within 12 months. The intervention was not superior to usual care either in reducing the risk of disorder onset (adjusted IRR 0.98; 95% CI 0.69 to 1.38) or in reducing depressive (randomization-by-time interaction coefficient = -1.40; 95% CI -3.91 to 1.10) or anxiety symptoms (randomization-by-time interaction coefficient = -0.55; 95% CI -1.59 to 0.49). The intervention did not reduce caregiver burden or their health related quality of life.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "This study did not demonstrate preventive effects of family meetings on the mental health of family caregivers. Further research should determine whether this intervention might be more beneficial if provided in a more concentrated dose, when applied for therapeutic purposes or targeted towards subgroups of caregivers.", "meshes": ["Aged", "Anxiety", "Caregivers", "Delivery of Health Care", "Dementia", "Demography", "Depression", "Female", "Follow-Up Studies", "Humans", "Longitudinal Studies", "Male", "Social Support", "Treatment Outcome"], "year": "2012"}
{"id": "pubmedqa_16776337", "dataset": "pubmedqa", "question": "Context: A retrospective analysis of a contemporary series of patients with pituitary apoplexy was performed to ascertain whether the histopathological features influence the clinical presentation or the outcome. A retrospective analysis was performed in 59 patients treated for pituitary apoplexy at the University of Virginia Health System, Charlottesville, Virginia, or Groote Schuur Hospital, University of Cape Town, South Africa. The patients were divided into two groups according to the histological features of their disease: one group with infarction alone, comprising 22 patients; and the other with hemorrhagic infarction and/or frank hemorrhage, comprising 37 patients. The presenting symptoms, clinical features, endocrinological status, and outcome were compared between the two groups.\n\nQuestion: Pituitary apoplexy: do histological features influence the clinical presentation and outcome?", "question_only": "Pituitary apoplexy: do histological features influence the clinical presentation and outcome?", "context": "A retrospective analysis of a contemporary series of patients with pituitary apoplexy was performed to ascertain whether the histopathological features influence the clinical presentation or the outcome. A retrospective analysis was performed in 59 patients treated for pituitary apoplexy at the University of Virginia Health System, Charlottesville, Virginia, or Groote Schuur Hospital, University of Cape Town, South Africa. The patients were divided into two groups according to the histological features of their disease: one group with infarction alone, comprising 22 patients; and the other with hemorrhagic infarction and/or frank hemorrhage, comprising 37 patients. The presenting symptoms, clinical features, endocrinological status, and outcome were compared between the two groups.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The patients who presented with histological features of pituitary tumor infarction alone had less severe clinical features on presentation, a longer course prior to presentation, and a better outcome than those presenting with hemorrhagic infarction or frank hemorrhage. The endocrine replacement requirements were similar in both groups.", "meshes": ["Adenoma", "Female", "Follow-Up Studies", "Hemorrhage", "Humans", "Infarction", "Male", "Middle Aged", "Pituitary Apoplexy", "Pituitary Gland", "Pituitary Neoplasms", "Retrospective Studies", "Treatment Outcome"], "year": "2006"}
{"id": "pubmedqa_26818046", "dataset": "pubmedqa", "question": "Context: To examine the clinical effect (efficacy and tolerability) of high doses of zonisamide (ZNS) (>500 mg/d) in adult patients with pharmacoresistant epilepsy. Between 2006 and 2013, all epileptic outpatients treated with high doses of ZNS were selected. Safety and efficacy were assessed based on patient and caregiver reports. Serum levels of ZNS and other concomitant antiepileptic drugs were evaluated if available. Nine patients (5 female): 8 focal/1 generalized pharmacoresistant epilepsy. Mean age: 34 years. Most frequent seizure type: complex partial seizures; other seizure types: generalized tonic-clonic, tonic, myoclonia. Zonisamide in polytherapy in all (100%), administered in tritherapy in 3 (33%) of 9 patients; mean dose: 633 (600-700) mg/d; efficacy (>50% seizure reduction) was observed in 5 (55%) of 9 patients. Five of 9 patients are still taking high doses of ZNS (more than 1 year). Adverse events were observed in 3 (37%) of 8 patients. Good tolerance to high doses of other antiepileptic drugs had been observed in 6 (66%) of 9 patients. Plasma levels of ZNS were only available in 2 patients; both were in the therapeutic range (34.95, 30.91) (10-40 mg/L).\n\nQuestion: Could Adult European Pharmacoresistant Epilepsy Patients Be Treated With Higher Doses of Zonisamide?", "question_only": "Could Adult European Pharmacoresistant Epilepsy Patients Be Treated With Higher Doses of Zonisamide?", "context": "To examine the clinical effect (efficacy and tolerability) of high doses of zonisamide (ZNS) (>500 mg/d) in adult patients with pharmacoresistant epilepsy. Between 2006 and 2013, all epileptic outpatients treated with high doses of ZNS were selected. Safety and efficacy were assessed based on patient and caregiver reports. Serum levels of ZNS and other concomitant antiepileptic drugs were evaluated if available. Nine patients (5 female): 8 focal/1 generalized pharmacoresistant epilepsy. Mean age: 34 years. Most frequent seizure type: complex partial seizures; other seizure types: generalized tonic-clonic, tonic, myoclonia. Zonisamide in polytherapy in all (100%), administered in tritherapy in 3 (33%) of 9 patients; mean dose: 633 (600-700) mg/d; efficacy (>50% seizure reduction) was observed in 5 (55%) of 9 patients. Five of 9 patients are still taking high doses of ZNS (more than 1 year). Adverse events were observed in 3 (37%) of 8 patients. Good tolerance to high doses of other antiepileptic drugs had been observed in 6 (66%) of 9 patients. Plasma levels of ZNS were only available in 2 patients; both were in the therapeutic range (34.95, 30.91) (10-40 mg/L).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "High doses of ZNS are effective and safe in pharmacoresistant epileptic patients. Therapeutic drug monitoring of ZNS may be considered at therapeutic failure.", "meshes": ["Adult", "Anticonvulsants", "Dose-Response Relationship, Drug", "Drug Resistant Epilepsy", "Europe", "Female", "Humans", "Isoxazoles", "Male", "Middle Aged", "Retrospective Studies", "Treatment Outcome", "Young Adult"], "year": null}
{"id": "pubmedqa_21420186", "dataset": "pubmedqa", "question": "Context: Sporadic data present in literature report how preterm birth and low birth weight are risk factors for the development of cardiovascular diseases in later life. High levels of asymmetric dimethylarginine (ADMA), a strong inhibitor of nitric oxide synthesis, are associated with the future development of adverse cardiovascular events and cardiac death. 1) to verify the presence of a statistically significant difference between ADMA levels in young adults born preterm at extremely low birth weight (<1000 g; ex-ELBW) and those of a control group of healthy adults born at term (C) and 2) to seek correlations between ADMA levels in ex-ELBW and anthropometric and clinical parameters (gender, chronological age, gestational age, birth weight, and duration of stay in Neonatal Intensive Care Unit). Thirty-two ex-ELBW subjects (11 males [M] and 21 females [F], aged 17-29years, mean age 22.2 ± 2.3 years) were compared with 25 C (7 M and 18F). ADMA levels were assessed by high-performance liquid chromatography with highly sensitive laser fluorescent detection. ADMA levels were reduced in ex-ELBW subjects compared to C (0.606+0.095 vs 0.562+0.101 μmol/L, p<0.05), and significantly correlated inversely with gestational age (r=-0.61, p<0.00001) and birth weight (r=-0.57, p<0.0002).\n\nQuestion: Could ADMA levels in young adults born preterm predict an early endothelial dysfunction?", "question_only": "Could ADMA levels in young adults born preterm predict an early endothelial dysfunction?", "context": "Sporadic data present in literature report how preterm birth and low birth weight are risk factors for the development of cardiovascular diseases in later life. High levels of asymmetric dimethylarginine (ADMA), a strong inhibitor of nitric oxide synthesis, are associated with the future development of adverse cardiovascular events and cardiac death. 1) to verify the presence of a statistically significant difference between ADMA levels in young adults born preterm at extremely low birth weight (<1000 g; ex-ELBW) and those of a control group of healthy adults born at term (C) and 2) to seek correlations between ADMA levels in ex-ELBW and anthropometric and clinical parameters (gender, chronological age, gestational age, birth weight, and duration of stay in Neonatal Intensive Care Unit). Thirty-two ex-ELBW subjects (11 males [M] and 21 females [F], aged 17-29years, mean age 22.2 ± 2.3 years) were compared with 25 C (7 M and 18F). ADMA levels were assessed by high-performance liquid chromatography with highly sensitive laser fluorescent detection. ADMA levels were reduced in ex-ELBW subjects compared to C (0.606+0.095 vs 0.562+0.101 μmol/L, p<0.05), and significantly correlated inversely with gestational age (r=-0.61, p<0.00001) and birth weight (r=-0.57, p<0.0002).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Our findings reveal a significant decrease in ADMA levels of ex-ELBW subjects compared to C, underlining a probable correlation with preterm birth and low birth weight. Taken together, these results may underlie the onset of early circulatory dysfunction predictive of increased cardiovascular risk.", "meshes": ["Adolescent", "Adult", "Arginine", "Early Diagnosis", "Endothelium, Vascular", "Female", "Gestational Age", "Humans", "Infant, Extremely Low Birth Weight", "Infant, Low Birth Weight", "Infant, Newborn", "Male", "Predictive Value of Tests", "Premature Birth", "Vascular Diseases", "Young Adult"], "year": "2012"}
{"id": "pubmedqa_23052500", "dataset": "pubmedqa", "question": "Context: Staging laparoscopy (SL) is not regularly performed for patients with hepatocellular carcinoma (HCC). It may change treatment strategy, preventing unnecessary open exploration. An additional advantage of SL is possible biopsy of the nontumorous liver to assess fibrosis/cirrhosis. This study aimed to determine whether SL for patients with HCC still is useful. Patients with HCC who underwent SL between January 1999 and December 2011 were analyzed. Their demographics, preoperative imaging studies, surgical findings, and histology were assessed. The 56 patients (34 men and 22 women; mean age, 60 ± 14 years) in this study underwent SL for assessment of extensive disease or metastases. For two patients, SL was unsuccessful because of intraabdominal adhesions. For four patients (7.1 %), SL showed unresectability because of metastases (n = 1), tumor progression (n = 1), or severe cirrhosis in the contralateral lobe (n = 2). An additional five patients did not undergo laparotomy due to disease progression detected on imaging after SL. Exploratory laparotomy for the remaining 47 patients showed 6 (13 %) additional unresectable tumors due to advanced tumor (n = 5) or nodal metastases (n = 1). Consequently, the yield of SL was 7 % (95 % confidence interval (CI), 3-17 %), and the accuracy was 27 % (95 % CI, 11-52 %). A biopsy of the contralateral liver was performed for 45 patients who underwent SL, leading to changes in management for 4 patients (17 %) with cirrhosis.\n\nQuestion: Staging laparoscopy in patients with hepatocellular carcinoma: is it useful?", "question_only": "Staging laparoscopy in patients with hepatocellular carcinoma: is it useful?", "context": "Staging laparoscopy (SL) is not regularly performed for patients with hepatocellular carcinoma (HCC). It may change treatment strategy, preventing unnecessary open exploration. An additional advantage of SL is possible biopsy of the nontumorous liver to assess fibrosis/cirrhosis. This study aimed to determine whether SL for patients with HCC still is useful. Patients with HCC who underwent SL between January 1999 and December 2011 were analyzed. Their demographics, preoperative imaging studies, surgical findings, and histology were assessed. The 56 patients (34 men and 22 women; mean age, 60 ± 14 years) in this study underwent SL for assessment of extensive disease or metastases. For two patients, SL was unsuccessful because of intraabdominal adhesions. For four patients (7.1 %), SL showed unresectability because of metastases (n = 1), tumor progression (n = 1), or severe cirrhosis in the contralateral lobe (n = 2). An additional five patients did not undergo laparotomy due to disease progression detected on imaging after SL. Exploratory laparotomy for the remaining 47 patients showed 6 (13 %) additional unresectable tumors due to advanced tumor (n = 5) or nodal metastases (n = 1). Consequently, the yield of SL was 7 % (95 % confidence interval (CI), 3-17 %), and the accuracy was 27 % (95 % CI, 11-52 %). A biopsy of the contralateral liver was performed for 45 patients who underwent SL, leading to changes in management for 4 patients (17 %) with cirrhosis.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The overall yield of SL for HCC was 7 %, and the accuracy was 27 %. When accurate imaging methods are available and additional percutaneous liver biopsy is implemented as a standard procedure in the preoperative workup of patients with HCC, the benefit of SL will become even less.", "meshes": ["Carcinoma, Hepatocellular", "Female", "Humans", "Laparoscopy", "Laparotomy", "Liver Cirrhosis", "Liver Neoplasms", "Magnetic Resonance Imaging", "Male", "Middle Aged", "Neoplasm Metastasis", "Neoplasm Recurrence, Local", "Neoplasm Staging", "Tomography, X-Ray Computed"], "year": "2013"}
{"id": "pubmedqa_12836106", "dataset": "pubmedqa", "question": "Context: Injury severity score (ISS), Glasgow coma score (GCS), and revised trauma score (RTS) are the most frequently used methods to evaluate the severity of injury in blunt trauma patients. ISS is too complicated to assess easily and GCS and RTS are easy to assess but somewhat subjective. White blood cell count (WBC) is an easy, quick and objective test. This study was performed to evaluate the significance of the WBC count at presentation in the blunt trauma patients. 713 blunt trauma patients, who were admitted to the Uludag University Medical Center Emergency Department between 01.04.2000-31.12.2000, were retrospectively evaluated in terms of ISS, GCS, RTS and white blood cell count at presentation. Statistical analysis revealed that WBC was correlated positively with ISS, but negatively with GCS and RTS.\n\nQuestion: Does the leukocyte count correlate with the severity of injury?", "question_only": "Does the leukocyte count correlate with the severity of injury?", "context": "Injury severity score (ISS), Glasgow coma score (GCS), and revised trauma score (RTS) are the most frequently used methods to evaluate the severity of injury in blunt trauma patients. ISS is too complicated to assess easily and GCS and RTS are easy to assess but somewhat subjective. White blood cell count (WBC) is an easy, quick and objective test. This study was performed to evaluate the significance of the WBC count at presentation in the blunt trauma patients. 713 blunt trauma patients, who were admitted to the Uludag University Medical Center Emergency Department between 01.04.2000-31.12.2000, were retrospectively evaluated in terms of ISS, GCS, RTS and white blood cell count at presentation. Statistical analysis revealed that WBC was correlated positively with ISS, but negatively with GCS and RTS.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The leukocyte count at presentation can be used as an adjunct in the evaluation of the severity of injury in blunt trauma patients.", "meshes": ["Glasgow Coma Scale", "Humans", "Injury Severity Score", "Leukocyte Count", "Predictive Value of Tests", "Trauma Severity Indices", "Wounds, Nonpenetrating"], "year": "2003"}
{"id": "pubmedqa_23361217", "dataset": "pubmedqa", "question": "Context: There are a number of factors responsible for the longevity of unicompartmental knee replacements (UKR). These include the magnitude of postoperative alignment and the type of material used. The effect of component design and material on postoperative alignment, however, has not been explored. We retrospectively reviewed 89 patients who underwent UKR with robotic guidance. Patients were divided into two groups, according to whether they had received an all-polyethylene inlay component (Inlay group) or a metal-backed onlay component (Onlay group). We explored the magnitude of mechanical alignment correction obtained in both groups. Mean postoperative mechanical alignment was significantly closer to neutral in the Onlay group (mean=2.8°; 95% CI=2.4°, 3.2°) compared to the Inlay group (mean=3.9°; 95% CI=3.4°, 4.4°) (R2=0.65; P=0.003), adjusting for gender, BMI, age, side and preoperative mechanical alignment (Fig. 2). Further exploration revealed that the thickness of the tibial polyethyelene insert had a significant effect on postoperative alignment when added to the model (R2=0.68; P=0.01).\n\nQuestion: Does the type of tibial component affect mechanical alignment in unicompartmental knee replacement?", "question_only": "Does the type of tibial component affect mechanical alignment in unicompartmental knee replacement?", "context": "There are a number of factors responsible for the longevity of unicompartmental knee replacements (UKR). These include the magnitude of postoperative alignment and the type of material used. The effect of component design and material on postoperative alignment, however, has not been explored. We retrospectively reviewed 89 patients who underwent UKR with robotic guidance. Patients were divided into two groups, according to whether they had received an all-polyethylene inlay component (Inlay group) or a metal-backed onlay component (Onlay group). We explored the magnitude of mechanical alignment correction obtained in both groups. Mean postoperative mechanical alignment was significantly closer to neutral in the Onlay group (mean=2.8°; 95% CI=2.4°, 3.2°) compared to the Inlay group (mean=3.9°; 95% CI=3.4°, 4.4°) (R2=0.65; P=0.003), adjusting for gender, BMI, age, side and preoperative mechanical alignment (Fig. 2). Further exploration revealed that the thickness of the tibial polyethyelene insert had a significant effect on postoperative alignment when added to the model (R2=0.68; P=0.01).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Patients who received a metal-backed Onlay tibial component obtained better postoperative mechanical alignment compared to those who received all-polyethylene Inlay prostheses. The thicker overall construct of Onlay prostheses appears to be an important determinant of postoperative alignment. Considering their higher survivorship rates and improved postoperative mechanical alignment, Onlay prostheses should be the first option when performing medial UKR.", "meshes": ["Arthroplasty, Replacement, Knee", "Biocompatible Materials", "Biomechanical Phenomena", "Body Mass Index", "Confidence Intervals", "Female", "Humans", "Knee Prosthesis", "Male", "Medical Audit", "Middle Aged", "Polyethylene", "Prosthesis Failure", "Retrospective Studies", "Tibia", "United States"], "year": "2013"}
{"id": "pubmedqa_22382608", "dataset": "pubmedqa", "question": "Context: The differential diagnosis between essential tremor (ET) and Parkinson's disease (PD) may be, in some cases, very difficult on clinical grounds alone. In addition, it is accepted that a small percentage of ET patients presenting symptoms and signs of possible PD may progress finally to a typical pattern of parkinsonism. Ioflupane, N-u-fluoropropyl-2a-carbomethoxy-3a-(4-iodophenyl) nortropane, also called FP-CIT, labelled with (123)I (commercially known as DaTSCAN) has been proven to be useful in the differential diagnosis between PD and ET and to confirm dopaminergic degeneration in patients with parkinsonism. The aim of this study is to identify dopaminergic degeneration in patients with PD and distinguish them from others with ET using semi-quantitative SPECT (123)I-Ioflupane (DaTSCAN) data in comparison with normal volunteers (NV), in addition with the respective ones of patients referred as suffering from ET, as well as, of patients with a PD diagnosis at an initial stage with a unilateral presentation of motor signs. Twenty-eight patients suffering from ET (10 males plus 18 females) and 28 NV (12 males and 16 females) were enroled in this study. In addition, 33 patients (11 males and 22 females) with an established diagnosis of PD with unilateral limb involvement (12 left hemi-body and 21 right hemi-body) were included for comparison with ET. We used DaTSCAN to obtain SPECT images and measure the radiopharmaceutical uptake in the striatum (S), as well as the caudate nucleus (CN) and putamen (P) in all individuals. Qualitative (Visual) interpretation of the SPECT data did not find any difference in the uptake of the radiopharmaceutical at the level of the S, CN and P between NV and ET patients. Reduced accumulation of the radiopharmaceutical uptake was found in the P of all PD patients. Semiquantitative analysis revealed significant differences between NV and ET patients in the striatum, reduced in the latter. There was also a significant reduction in the tracer accumulation in the left putamen of patients with right hemi-parkinsonism compared to ET and NV. Patients with left hemi-parkinsonism, demonstrated reduced radioligand uptake in the right putamen in comparison with ET and NV. Clinical follow-up of 20 patients with ET at (so many months afterwards) revealed no significant change in clinical presentation, particularly no signs of PD. Follow-up DaTSCAN performed in 10 of them (so many months afterwards) was negative in all but one. This one had an equivocal baseline study which deteriorated 12 months later.\n\nQuestion: SPECT study with I-123-Ioflupane (DaTSCAN) in patients with essential tremor. Is there any correlation with Parkinson's disease?", "question_only": "SPECT study with I-123-Ioflupane (DaTSCAN) in patients with essential tremor. Is there any correlation with Parkinson's disease?", "context": "The differential diagnosis between essential tremor (ET) and Parkinson's disease (PD) may be, in some cases, very difficult on clinical grounds alone. In addition, it is accepted that a small percentage of ET patients presenting symptoms and signs of possible PD may progress finally to a typical pattern of parkinsonism. Ioflupane, N-u-fluoropropyl-2a-carbomethoxy-3a-(4-iodophenyl) nortropane, also called FP-CIT, labelled with (123)I (commercially known as DaTSCAN) has been proven to be useful in the differential diagnosis between PD and ET and to confirm dopaminergic degeneration in patients with parkinsonism. The aim of this study is to identify dopaminergic degeneration in patients with PD and distinguish them from others with ET using semi-quantitative SPECT (123)I-Ioflupane (DaTSCAN) data in comparison with normal volunteers (NV), in addition with the respective ones of patients referred as suffering from ET, as well as, of patients with a PD diagnosis at an initial stage with a unilateral presentation of motor signs. Twenty-eight patients suffering from ET (10 males plus 18 females) and 28 NV (12 males and 16 females) were enroled in this study. In addition, 33 patients (11 males and 22 females) with an established diagnosis of PD with unilateral limb involvement (12 left hemi-body and 21 right hemi-body) were included for comparison with ET. We used DaTSCAN to obtain SPECT images and measure the radiopharmaceutical uptake in the striatum (S), as well as the caudate nucleus (CN) and putamen (P) in all individuals. Qualitative (Visual) interpretation of the SPECT data did not find any difference in the uptake of the radiopharmaceutical at the level of the S, CN and P between NV and ET patients. Reduced accumulation of the radiopharmaceutical uptake was found in the P of all PD patients. Semiquantitative analysis revealed significant differences between NV and ET patients in the striatum, reduced in the latter. There was also a significant reduction in the tracer accumulation in the left putamen of patients with right hemi-parkinsonism compared to ET and NV. Patients with left hemi-parkinsonism, demonstrated reduced radioligand uptake in the right putamen in comparison with ET and NV. Clinical follow-up of 20 patients with ET at (so many months afterwards) revealed no significant change in clinical presentation, particularly no signs of PD. Follow-up DaTSCAN performed in 10 of them (so many months afterwards) was negative in all but one. This one had an equivocal baseline study which deteriorated 12 months later.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Our results do not support the hypothesis of a link between essential tremor and Parkinson's disease. However, it appears that ET patients have a small degree of striatal dopaminergic degeneration. If this is due to alterations in the nigrostriatl pathway or of other origin it is not clear. Follow-up studies of essential tremor patients are warranted to assess progression of disease and to understand better the possible cause for striatal dopaminergic degeneration.", "meshes": ["Case-Control Studies", "Diagnosis, Differential", "Dopamine", "Essential Tremor", "Female", "Humans", "Male", "Motor Activity", "Nortropanes", "Parkinson Disease", "Tomography, Emission-Computed, Single-Photon"], "year": "2012"}
{"id": "pubmedqa_9363529", "dataset": "pubmedqa", "question": "Context: To evaluate psychological distress as a predictor of disability due to common chronic disorders. A 10-year follow-up study was carried out among a representative cohort (N = 8655) of 18-64 year old Finnish farmers, who had participated in a health survey in 1979 and were able to work at baseline. A record linkage with the nationwide register of the Social Insurance Institution was made to identify disability pensions granted between 1980 and 1990 in the cohort. The medical certificates of 1004 (11.6%) prematurely retired farmers were reviewed to confirm and classify disabling conditions. A sum score based on self-reports of 11 symptoms at the baseline was used as a measure of psychological distress. After adjustment for age, sex, smoking and body mass index, the cause-specific relative risks (RR) (95% confidence intervals [CI]) of disability in the highest quartile of the psychological distress score as compared with the lowest quartile were for myocardial infarction 2.34 (95% CI: 1.17-4.69), for depression 2.50 (95% CI: 1.09-5.72), for neck-shoulder disorders 1.98 (95% CI: 1.26-3.11), for unspecified low-back disorders 1.76 (95% CI: 1.24-2.49), for knee osteoarthritis 1.55 (95% CI: 0.91-2.63) and for trip osteoarthritis 0.89 (95% CI: 0.42-1.85). The corresponding RR for overall disability was 1.76 (95% CI: 1.44-2.14) in the highest quartile of psychological distress score as compared with the lowest quartile.\n\nQuestion: Does psychological distress predict disability?", "question_only": "Does psychological distress predict disability?", "context": "To evaluate psychological distress as a predictor of disability due to common chronic disorders. A 10-year follow-up study was carried out among a representative cohort (N = 8655) of 18-64 year old Finnish farmers, who had participated in a health survey in 1979 and were able to work at baseline. A record linkage with the nationwide register of the Social Insurance Institution was made to identify disability pensions granted between 1980 and 1990 in the cohort. The medical certificates of 1004 (11.6%) prematurely retired farmers were reviewed to confirm and classify disabling conditions. A sum score based on self-reports of 11 symptoms at the baseline was used as a measure of psychological distress. After adjustment for age, sex, smoking and body mass index, the cause-specific relative risks (RR) (95% confidence intervals [CI]) of disability in the highest quartile of the psychological distress score as compared with the lowest quartile were for myocardial infarction 2.34 (95% CI: 1.17-4.69), for depression 2.50 (95% CI: 1.09-5.72), for neck-shoulder disorders 1.98 (95% CI: 1.26-3.11), for unspecified low-back disorders 1.76 (95% CI: 1.24-2.49), for knee osteoarthritis 1.55 (95% CI: 0.91-2.63) and for trip osteoarthritis 0.89 (95% CI: 0.42-1.85). The corresponding RR for overall disability was 1.76 (95% CI: 1.44-2.14) in the highest quartile of psychological distress score as compared with the lowest quartile.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Psychological distress is an independent risk factor for disability. Its predictive significance varies between disorders leading to functional deterioration. The association mechanisms are likely to vary from one disorder to another.", "meshes": ["Adolescent", "Adult", "Age Distribution", "Agriculture", "Cardiovascular Diseases", "Cohort Studies", "Disabled Persons", "Female", "Finland", "Health Surveys", "Humans", "Incidence", "Male", "Middle Aged", "Musculoskeletal Diseases", "Predictive Value of Tests", "Proportional Hazards Models", "Registries", "Risk Factors", "Sex Distribution", "Stress, Psychological"], "year": "1997"}
{"id": "pubmedqa_17919952", "dataset": "pubmedqa", "question": "Context: (i) To examine the association between self-reported mechanical factors and chronic oro-facial pain. (ii) To test the hypothesis that this relationship could be explained by: (a) reporting of psychological factors, (b) common association of self-reported mechanical factors with other unexplained syndromes. A population based cross-sectional study of 4200 randomly selected adults registered with a General Medical Practice in North West, England. The study examined the association of chronic oro-facial pain with a variety of self-reported mechanical factors: teeth grinding, facial trauma, missing teeth and the feeling that the teeth did not fit together properly. Information was also collected on demographic factors, psychological factors and the reporting of other frequently unexplained syndromes. An adjusted response rate of 72% was achieved. Only two mechanical factors: teeth grinding (odds ratio (OR) 2.0, 95% CI 1.3-3.0) and facial trauma (OR 2.0; 95% CI 1.3-2.9) were independently associated with chronic oro-facial pain after adjusting for psychological factors. However, these factors were also commonly associated with the reporting of other frequently unexplained syndromes: teeth grinding (odds ratio (OR) 1.8, 95% CI 1.5-2.2), facial trauma (OR 2.1; 95% CI 1.7-2.6).\n\nQuestion: Are reports of mechanical dysfunction in chronic oro-facial pain related to somatisation?", "question_only": "Are reports of mechanical dysfunction in chronic oro-facial pain related to somatisation?", "context": "(i) To examine the association between self-reported mechanical factors and chronic oro-facial pain. (ii) To test the hypothesis that this relationship could be explained by: (a) reporting of psychological factors, (b) common association of self-reported mechanical factors with other unexplained syndromes. A population based cross-sectional study of 4200 randomly selected adults registered with a General Medical Practice in North West, England. The study examined the association of chronic oro-facial pain with a variety of self-reported mechanical factors: teeth grinding, facial trauma, missing teeth and the feeling that the teeth did not fit together properly. Information was also collected on demographic factors, psychological factors and the reporting of other frequently unexplained syndromes. An adjusted response rate of 72% was achieved. Only two mechanical factors: teeth grinding (odds ratio (OR) 2.0, 95% CI 1.3-3.0) and facial trauma (OR 2.0; 95% CI 1.3-2.9) were independently associated with chronic oro-facial pain after adjusting for psychological factors. However, these factors were also commonly associated with the reporting of other frequently unexplained syndromes: teeth grinding (odds ratio (OR) 1.8, 95% CI 1.5-2.2), facial trauma (OR 2.1; 95% CI 1.7-2.6).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Self-reported mechanical factors associated with chronic oro-facial pain are confounded, in part, by psychological factors and are equally common across other frequently unexplained syndromes. They may represent another feature of somatisation. Therefore the use of extensive invasive therapy such as occlusal adjustments and surgery to change mechanical factors may not be justified in many cases.", "meshes": ["Adolescent", "Adult", "Aged", "Bruxism", "Chronic Disease", "Cross-Sectional Studies", "Facial Injuries", "Facial Pain", "Female", "Humans", "Male", "Middle Aged", "Mouth, Edentulous", "Multivariate Analysis", "Prevalence", "Stress, Mechanical", "Surveys and Questionnaires"], "year": "2008"}
{"id": "pubmedqa_14631523", "dataset": "pubmedqa", "question": "Context: The objectives were to identify prognostic factors for the survival of children with cerebellar astrocytoma, and to evaluate the reproducibility and prognostic value of histological sub-classification and grading. Children aged 0-14 years treated in Denmark for a cerebellar astrocytoma in the period 1960-1984 were included and followed until January 2001 or until their death. The histological specimens from each patient were reviewed for revised grading and classification according to three different classification schemes: the WHO, the Kernohan and the Daumas-Duport grading systems. The overall survival rate was 81% after a follow-up time of 15-40 years. The significant positive prognostic factors for survival were \"surgically gross-total removal\" of the tumour at surgery and location of the tumour in the cerebellum proper as opposed to location in the fourth ventricle. No difference in survival time was demonstrated when we compared pilocytic astrocytoma and fibrillary astrocytoma. Moreover, we found that the Kernohan and the WHO classification systems had no predictive value and that the Daumas-Duport system is unsuitable as a prognostic tool for low-grade posterior fossa astrocytomas.\n\nQuestion: Sub-classification of low-grade cerebellar astrocytoma: is it clinically meaningful?", "question_only": "Sub-classification of low-grade cerebellar astrocytoma: is it clinically meaningful?", "context": "The objectives were to identify prognostic factors for the survival of children with cerebellar astrocytoma, and to evaluate the reproducibility and prognostic value of histological sub-classification and grading. Children aged 0-14 years treated in Denmark for a cerebellar astrocytoma in the period 1960-1984 were included and followed until January 2001 or until their death. The histological specimens from each patient were reviewed for revised grading and classification according to three different classification schemes: the WHO, the Kernohan and the Daumas-Duport grading systems. The overall survival rate was 81% after a follow-up time of 15-40 years. The significant positive prognostic factors for survival were \"surgically gross-total removal\" of the tumour at surgery and location of the tumour in the cerebellum proper as opposed to location in the fourth ventricle. No difference in survival time was demonstrated when we compared pilocytic astrocytoma and fibrillary astrocytoma. Moreover, we found that the Kernohan and the WHO classification systems had no predictive value and that the Daumas-Duport system is unsuitable as a prognostic tool for low-grade posterior fossa astrocytomas.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Discordant observations due to interobserver variability make histological sub-classification of low-grade cerebellar astrocytomas in children insufficient for predicting prognosis and biological behaviour. Similar survival rates in a population of paediatric low-grade cerebellar astrocytomas of grades I and II indicate that tumour grade has no prognostic significance within this group of patients. \"Surgically gross-total removal\", especially if the tumour is located in the fourth ventricle is of the highest importance for long-term survival. Histological sub-classification of the tumours has no predictive value.", "meshes": ["Adolescent", "Astrocytoma", "Cerebellar Neoplasms", "Child", "Child, Preschool", "Female", "Follow-Up Studies", "Histological Techniques", "Humans", "Infant", "Infant, Newborn", "Male", "Neurologic Examination", "Predictive Value of Tests", "Prognosis", "Retrospective Studies", "Survival Rate", "Time Factors", "World Health Organization", "alpha-Crystallin B Chain"], "year": "2003"}
{"id": "pubmedqa_16147837", "dataset": "pubmedqa", "question": "Context: To compare maternal and neonatal outcomes among grandmultiparous women to those of multiparous women 30 years or older. A database of the vast majority of maternal and newborn hospital discharge records linked to birth/death certificates was queried to obtain information on all multiparous women with a singleton delivery in the state of California from January 1, 1997 through December 31, 1998. Maternal and neonatal pregnancy outcomes of grandmultiparous women were compared to multiparous women who were 30 years or older at the time of their last birth. The study population included 25,512 grandmultiparous and 265,060 multiparous women 30 years or older as controls. Grandmultiparous women were predominantly Hispanic (56%). After controlling for potential confounding factors, grandmultiparous women were at significantly higher risk for abruptio placentae (odds ratio OR: 1.3; 95% confidence intervals CI: 1.2-1.5), preterm delivery (OR: 1.3; 95% CI: 1.2-1.4), fetal macrosomia (OR: 1.5; 95% CI: 1.4-1.6), neonatal death (OR: 1.5; 95% CI: 1.3-1.8), postpartum hemorrhage (OR: 1.2; 95% CI: 1.1-1.3) and blood transfusion (OR: 1.5; 95% CI: 1.3-1.8).\n\nQuestion: Is grandmultiparity an independent risk factor for adverse perinatal outcomes?", "question_only": "Is grandmultiparity an independent risk factor for adverse perinatal outcomes?", "context": "To compare maternal and neonatal outcomes among grandmultiparous women to those of multiparous women 30 years or older. A database of the vast majority of maternal and newborn hospital discharge records linked to birth/death certificates was queried to obtain information on all multiparous women with a singleton delivery in the state of California from January 1, 1997 through December 31, 1998. Maternal and neonatal pregnancy outcomes of grandmultiparous women were compared to multiparous women who were 30 years or older at the time of their last birth. The study population included 25,512 grandmultiparous and 265,060 multiparous women 30 years or older as controls. Grandmultiparous women were predominantly Hispanic (56%). After controlling for potential confounding factors, grandmultiparous women were at significantly higher risk for abruptio placentae (odds ratio OR: 1.3; 95% confidence intervals CI: 1.2-1.5), preterm delivery (OR: 1.3; 95% CI: 1.2-1.4), fetal macrosomia (OR: 1.5; 95% CI: 1.4-1.6), neonatal death (OR: 1.5; 95% CI: 1.3-1.8), postpartum hemorrhage (OR: 1.2; 95% CI: 1.1-1.3) and blood transfusion (OR: 1.5; 95% CI: 1.3-1.8).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Grandmultiparous women had increased maternal and neonatal morbidity, and neonatal mortality even after controlling for confounders, suggesting a need for closer observation than regular multiparous patients during labor and delivery.", "meshes": ["Female", "Humans", "Parity", "Pregnancy", "Pregnancy Complications", "Pregnancy Outcome", "Retrospective Studies", "Risk Factors"], "year": "2005"}
{"id": "pubmedqa_24153338", "dataset": "pubmedqa", "question": "Context: With the advancement of an aging society in the world, an increasing number of elderly patients have been hospitalized due to aneurysmal subarachnoid hemorrhage (aSAH). There is no study that compares the elderly cases of aSAH who receive the definitive treatment with those who treated conservatively. The aim of this study was to investigate the feasibility of the definitive surgery for the acute subarachnoid cases aged 80 or older. We reviewed 500 consecutive cases with acute aSAH with surgical indication for aneurysm repair. Inoperable cases such as dead-on-arrival and the cases with both pupils dilated were excluded. We compared the cases aged 80 or older that received clipping or coil embolization with the controls that the family selected conservative treatment. 69 cases were included in this study (ranged 80-98, male:female=9:60). 56 cases (81.2%) had an aneurysm in the anterior circulation. 23 cases received clipping, 20 cases coil embolization and 26 cases treated conservatively. The cases with aneurysm repair showed significantly better clinical outcome than the controls, while World Federation of Neurological Surgeons (WFNS) grade on admission and premorbid modified Rankin Scale showed no difference between them.\n\nQuestion: Is aneurysm repair justified for the patients aged 80 or older after aneurysmal subarachnoid hemorrhage?", "question_only": "Is aneurysm repair justified for the patients aged 80 or older after aneurysmal subarachnoid hemorrhage?", "context": "With the advancement of an aging society in the world, an increasing number of elderly patients have been hospitalized due to aneurysmal subarachnoid hemorrhage (aSAH). There is no study that compares the elderly cases of aSAH who receive the definitive treatment with those who treated conservatively. The aim of this study was to investigate the feasibility of the definitive surgery for the acute subarachnoid cases aged 80 or older. We reviewed 500 consecutive cases with acute aSAH with surgical indication for aneurysm repair. Inoperable cases such as dead-on-arrival and the cases with both pupils dilated were excluded. We compared the cases aged 80 or older that received clipping or coil embolization with the controls that the family selected conservative treatment. 69 cases were included in this study (ranged 80-98, male:female=9:60). 56 cases (81.2%) had an aneurysm in the anterior circulation. 23 cases received clipping, 20 cases coil embolization and 26 cases treated conservatively. The cases with aneurysm repair showed significantly better clinical outcome than the controls, while World Federation of Neurological Surgeons (WFNS) grade on admission and premorbid modified Rankin Scale showed no difference between them.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Better prognosis was obtained when ruptured aneurysm was repaired in the elderly than it was treated conservatively. From the results of this study, we should not hesitate to offer the definitive surgery for the elderly with aSAH.", "meshes": ["Aged, 80 and over", "Aneurysm, Ruptured", "Cerebrovascular Circulation", "Cohort Studies", "Embolization, Therapeutic", "Female", "Humans", "Male", "Neurosurgical Procedures", "Prognosis", "Retrospective Studies", "Subarachnoid Hemorrhage", "Treatment Outcome"], "year": "2014"}
{"id": "pubmedqa_12607120", "dataset": "pubmedqa", "question": "Context: Anastomotic leakage is the most threatening early complication in sphincter-preserving rectal cancer surgery. While the oncological consequences have been well examined, only few data exist about the functional outcome. We investigated continence function in 150 patients after curative sphincter-preserving rectal cancer surgery. Functional results were compared in 22 patients with a clinically relevant anastomotic leakage, confirmed radiologically or endoscopically, and 128 patients with uneventful recovery. Evaluation of continence function was based on the Cleveland Clinic Continence Score and was examined in all patients with anastomotic leakage and in 111 patients without complications 107+/-46 weeks postoperatively. Additionally, 14 patients with anastomotic leakage and 58 patients with uneventful recovery underwent anorectal manometry 26+/-15 weeks postoperatively. The continence score in patients after anastomotic leakage did not differ significantly from that in patients without complications. Sphincter function was similar. Maximum tolerable volume and rectal compliance were slightly but not significantly worse after leakage.\n\nQuestion: Does anastomotic leakage affect functional outcome after rectal resection for cancer?", "question_only": "Does anastomotic leakage affect functional outcome after rectal resection for cancer?", "context": "Anastomotic leakage is the most threatening early complication in sphincter-preserving rectal cancer surgery. While the oncological consequences have been well examined, only few data exist about the functional outcome. We investigated continence function in 150 patients after curative sphincter-preserving rectal cancer surgery. Functional results were compared in 22 patients with a clinically relevant anastomotic leakage, confirmed radiologically or endoscopically, and 128 patients with uneventful recovery. Evaluation of continence function was based on the Cleveland Clinic Continence Score and was examined in all patients with anastomotic leakage and in 111 patients without complications 107+/-46 weeks postoperatively. Additionally, 14 patients with anastomotic leakage and 58 patients with uneventful recovery underwent anorectal manometry 26+/-15 weeks postoperatively. The continence score in patients after anastomotic leakage did not differ significantly from that in patients without complications. Sphincter function was similar. Maximum tolerable volume and rectal compliance were slightly but not significantly worse after leakage.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Continence function remained undisturbed after anastomotic leakage due to rectal resection", "meshes": ["Anastomosis, Surgical", "Fecal Incontinence", "Female", "Humans", "Male", "Manometry", "Middle Aged", "Postoperative Complications", "Rectal Neoplasms", "Rectum", "Statistics, Nonparametric", "Surgical Wound Dehiscence", "Surveys and Questionnaires", "Treatment Failure"], "year": "2003"}
{"id": "pubmedqa_12920330", "dataset": "pubmedqa", "question": "Context: Evidence suggests substantial comorbidity between symptoms of somatization and depression in clinical as well as nonclinical populations. However, as most existing research has been retrospective or cross-sectional in design, very little is known about the specific nature of this relationship. In particular, it is unclear whether somatic complaints may heighten the risk for the subsequent development of depressive symptoms. We report findings on the link between symptoms of somatization (assessed using the SCL-90-R) and depression 5 years later (assessed using the CES-D) in an initially healthy cohort of community adults, based on prospective data from the RENO Diet-Heart Study. Gender-stratified multiple regression analyses revealed that baseline CES-D scores were the best predictors of subsequent depressive symptoms for men and women. Baseline scores on the SCL-90-R somatization subscale significantly predicted subsequent self-reported symptoms of depressed mood 5 years later, but only in women. However, somatic complaints were a somewhat less powerful predictor than income and age.\n\nQuestion: Do somatic complaints predict subsequent symptoms of depression?", "question_only": "Do somatic complaints predict subsequent symptoms of depression?", "context": "Evidence suggests substantial comorbidity between symptoms of somatization and depression in clinical as well as nonclinical populations. However, as most existing research has been retrospective or cross-sectional in design, very little is known about the specific nature of this relationship. In particular, it is unclear whether somatic complaints may heighten the risk for the subsequent development of depressive symptoms. We report findings on the link between symptoms of somatization (assessed using the SCL-90-R) and depression 5 years later (assessed using the CES-D) in an initially healthy cohort of community adults, based on prospective data from the RENO Diet-Heart Study. Gender-stratified multiple regression analyses revealed that baseline CES-D scores were the best predictors of subsequent depressive symptoms for men and women. Baseline scores on the SCL-90-R somatization subscale significantly predicted subsequent self-reported symptoms of depressed mood 5 years later, but only in women. However, somatic complaints were a somewhat less powerful predictor than income and age.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Our findings suggest that somatic complaints may represent one, but not necessarily the most important, risk factor for the subsequent development of depressive symptoms in women in nonclinical populations. The results also highlight the importance of including social variables in studies on women's depression as well as conducting additional research to further examine predictors of depressive symptoms in men.", "meshes": ["Adult", "Aged", "Comorbidity", "Demography", "Depression", "Female", "Humans", "Male", "Middle Aged", "Primary Health Care", "Prospective Studies", "Somatoform Disorders"], "year": null}
{"id": "pubmedqa_14697414", "dataset": "pubmedqa", "question": "Context: To analyze, retrospectively, the patterns and behavior of metastatic lesions in prostate cancer patients treated with external beam radiotherapy and to investigate whether patients with<or =5 lesions had an improved outcome relative to patients with>5 lesions. The treatment and outcome of 369 eligible patients with Stage T1-T3aN0-NXM0 prostate cancer were analyzed during a minimal 10-year follow-up period. All patients were treated with curative intent to a mean dose of 65 Gy. The full history of any metastatic disease was documented for each subject, including the initial site of involvement, any progression over time, and patient survival. The overall survival rate for the 369 patients was 75% at 5 years and 45% at 10 years. The overall survival rate of patients who never developed metastases was 90% and 81% at 5 and 10 years, respectively. However, among the 74 patients (20%) who developed metastases, the survival rate at both 5 and 10 years was significantly reduced (p<0.0001). The overall survival rate for patients who developed bone metastases was 58% and 27% at 5 and 10 years, respectively, and patients with bone metastases to the pelvis fared worse compared with those with vertebral metastases. With regard to the metastatic number, patients with<or =5 metastatic lesions had superior survival rates relative to those with>5 lesions (73% and 36% at 5 and 10 years vs. 45% and 18% at 5 and 10 years, respectively; p = 0.02). In addition, both the metastasis-free survival rate and the interval measured from the date of the initial diagnosis of prostate cancer to the development of bone metastasis were statistically superior for patients with<or =5 lesions compared with patients with>5 lesions (p = 0.01 and 0.02, respectively). However, the survival rate and the interval from the date of diagnosis of bone metastasis to the time of death for patients in both groups were not significantly different, statistically (p = 0.17 and 0.27, respectively).\n\nQuestion: Is there a favorable subset of patients with prostate cancer who develop oligometastases?", "question_only": "Is there a favorable subset of patients with prostate cancer who develop oligometastases?", "context": "To analyze, retrospectively, the patterns and behavior of metastatic lesions in prostate cancer patients treated with external beam radiotherapy and to investigate whether patients with<or =5 lesions had an improved outcome relative to patients with>5 lesions. The treatment and outcome of 369 eligible patients with Stage T1-T3aN0-NXM0 prostate cancer were analyzed during a minimal 10-year follow-up period. All patients were treated with curative intent to a mean dose of 65 Gy. The full history of any metastatic disease was documented for each subject, including the initial site of involvement, any progression over time, and patient survival. The overall survival rate for the 369 patients was 75% at 5 years and 45% at 10 years. The overall survival rate of patients who never developed metastases was 90% and 81% at 5 and 10 years, respectively. However, among the 74 patients (20%) who developed metastases, the survival rate at both 5 and 10 years was significantly reduced (p<0.0001). The overall survival rate for patients who developed bone metastases was 58% and 27% at 5 and 10 years, respectively, and patients with bone metastases to the pelvis fared worse compared with those with vertebral metastases. With regard to the metastatic number, patients with<or =5 metastatic lesions had superior survival rates relative to those with>5 lesions (73% and 36% at 5 and 10 years vs. 45% and 18% at 5 and 10 years, respectively; p = 0.02). In addition, both the metastasis-free survival rate and the interval measured from the date of the initial diagnosis of prostate cancer to the development of bone metastasis were statistically superior for patients with<or =5 lesions compared with patients with>5 lesions (p = 0.01 and 0.02, respectively). However, the survival rate and the interval from the date of diagnosis of bone metastasis to the time of death for patients in both groups were not significantly different, statistically (p = 0.17 and 0.27, respectively).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Patients with<or =5 metastatic sites had significantly better survival rates than patients with>5 lesions. Because existing sites of metastatic disease may be the primary sites of origin for additional metastases, our findings suggest that early detection and aggressive treatment of patients with a small number of metastatic lesions is worth testing as an approach to improving long-term survival.", "meshes": ["Aged", "Aged, 80 and over", "Androgen Antagonists", "Antineoplastic Agents, Hormonal", "Bone Neoplasms", "Brain Neoplasms", "Humans", "Liver Neoplasms", "Lung Neoplasms", "Male", "Middle Aged", "Neoplasm Staging", "Orchiectomy", "Prostatic Neoplasms", "Retrospective Studies", "Survival Rate", "Treatment Outcome"], "year": "2004"}
{"id": "pubmedqa_26965932", "dataset": "pubmedqa", "question": "Context: This study sought to investigate the ischemic and bleeding outcomes of patients fulfilling high bleeding risk (HBR) criteria who were randomized to zotarolimus-eluting Endeavor Sprint stent (E-ZES) or bare-metal stent (BMS) implantation followed by an abbreviated dual antiplatelet therapy (DAPT) duration for stable or unstable coronary artery disease. DES instead of BMS use remains controversial in HBR patients, in whom long-term DAPT poses safety concerns. The ZEUS (Zotarolimus-Eluting Endeavor Sprint Stent in Uncertain DES Candidates) is a multinational, randomized single-blinded trial that randomized among others, in a stratified manner, 828 patients fulfilling pre-defined clinical or biochemical HBR criteria-including advanced age, indication to oral anticoagulants or other pro-hemorrhagic medications, history of bleeding and known anemia-to receive E-ZES or BMS followed by a protocol-mandated 30-day DAPT regimen. The primary endpoint of the study was the 12-month major adverse cardiovascular event rate, consisting of death, myocardial infarction, or target vessel revascularization. Compared with patients without, those with 1 or more HBR criteria had worse outcomes, owing to higher ischemic and bleeding risks. Among HBR patients, major adverse cardiovascular events occurred in 22.6% of the E-ZES and 29% of the BMS patients (hazard ratio: 0.75; 95% confidence interval: 0.57 to 0.98; p = 0.033), driven by lower myocardial infarction (3.5% vs. 10.4%; p<0.001) and target vessel revascularization (5.9% vs. 11.4%; p = 0.005) rates in the E-ZES arm. The composite of definite or probable stent thrombosis was significantly reduced in E-ZES recipients, whereas bleeding events did not differ between stent groups.\n\nQuestion: Is Bare-Metal Stent Implantation Still Justifiable in High Bleeding Risk Patients Undergoing Percutaneous Coronary Intervention?", "question_only": "Is Bare-Metal Stent Implantation Still Justifiable in High Bleeding Risk Patients Undergoing Percutaneous Coronary Intervention?", "context": "This study sought to investigate the ischemic and bleeding outcomes of patients fulfilling high bleeding risk (HBR) criteria who were randomized to zotarolimus-eluting Endeavor Sprint stent (E-ZES) or bare-metal stent (BMS) implantation followed by an abbreviated dual antiplatelet therapy (DAPT) duration for stable or unstable coronary artery disease. DES instead of BMS use remains controversial in HBR patients, in whom long-term DAPT poses safety concerns. The ZEUS (Zotarolimus-Eluting Endeavor Sprint Stent in Uncertain DES Candidates) is a multinational, randomized single-blinded trial that randomized among others, in a stratified manner, 828 patients fulfilling pre-defined clinical or biochemical HBR criteria-including advanced age, indication to oral anticoagulants or other pro-hemorrhagic medications, history of bleeding and known anemia-to receive E-ZES or BMS followed by a protocol-mandated 30-day DAPT regimen. The primary endpoint of the study was the 12-month major adverse cardiovascular event rate, consisting of death, myocardial infarction, or target vessel revascularization. Compared with patients without, those with 1 or more HBR criteria had worse outcomes, owing to higher ischemic and bleeding risks. Among HBR patients, major adverse cardiovascular events occurred in 22.6% of the E-ZES and 29% of the BMS patients (hazard ratio: 0.75; 95% confidence interval: 0.57 to 0.98; p = 0.033), driven by lower myocardial infarction (3.5% vs. 10.4%; p<0.001) and target vessel revascularization (5.9% vs. 11.4%; p = 0.005) rates in the E-ZES arm. The composite of definite or probable stent thrombosis was significantly reduced in E-ZES recipients, whereas bleeding events did not differ between stent groups.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Among HBR patients with stable or unstable coronary artery disease, E-ZES implantation provides superior efficacy and safety as compared with conventional BMS. (Zotarolimus-Eluting Endeavor Sprint Stent in Uncertain DES Candidates [ZEUS]; NCT01385319).", "meshes": ["Aged", "Aged, 80 and over", "Cardiovascular Agents", "Coronary Artery Disease", "Drug Therapy, Combination", "Drug-Eluting Stents", "Female", "Hemorrhage", "Humans", "Male", "Metals", "Myocardial Infarction", "Patient Selection", "Percutaneous Coronary Intervention", "Platelet Aggregation Inhibitors", "Prosthesis Design", "Risk Assessment", "Risk Factors", "Single-Blind Method", "Sirolimus", "Stents", "Time Factors", "Treatment Outcome"], "year": "2016"}
{"id": "pubmedqa_10798511", "dataset": "pubmedqa", "question": "Context: Physical examination to detect abdominal injuries has been considered unreliable in alcohol-intoxicated trauma patients. Computed tomography (CT) plays the primary role in these abdominal evaluations. We reviewed medical records of all blunt trauma patients admitted to our trauma service from January 1, 1992, to March 31, 1998. Study patients had a blood alcohol level>or =80 mg/dL, Glasgow Coma Scale (GCS) score of 15, and unremarkable abdominal examination. Of 324 patients studied, 317 (98%) had CT scans negative for abdominal injury. Abdominal injuries were identified in 7 patients (2%), with only 2 (0.6%) requiring abdominal exploration. A significant association was found between major chest injury and abdominal injury.\n\nQuestion: Blunt trauma in intoxicated patients: is computed tomography of the abdomen always necessary?", "question_only": "Blunt trauma in intoxicated patients: is computed tomography of the abdomen always necessary?", "context": "Physical examination to detect abdominal injuries has been considered unreliable in alcohol-intoxicated trauma patients. Computed tomography (CT) plays the primary role in these abdominal evaluations. We reviewed medical records of all blunt trauma patients admitted to our trauma service from January 1, 1992, to March 31, 1998. Study patients had a blood alcohol level>or =80 mg/dL, Glasgow Coma Scale (GCS) score of 15, and unremarkable abdominal examination. Of 324 patients studied, 317 (98%) had CT scans negative for abdominal injury. Abdominal injuries were identified in 7 patients (2%), with only 2 (0.6%) requiring abdominal exploration. A significant association was found between major chest injury and abdominal injury.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The incidence of abdominal injury in intoxicated, hemodynamically stable, blunt trauma patients with a normal abdominal examination and normal mentation is low. Physical examination and attention to clinical risk factors allow accurate abdominal evaluation without CT.", "meshes": ["Abdominal Injuries", "Adolescent", "Adult", "Aged", "Aged, 80 and over", "Alcoholic Intoxication", "Female", "Humans", "Male", "Middle Aged", "Physical Examination", "Radiography", "Risk Factors", "Trauma Centers"], "year": "2000"}
{"id": "pubmedqa_23860049", "dataset": "pubmedqa", "question": "Context: To evaluate the role of clinical assessment with selective use of imaging studies in the management of suspected acute appendicitis in children. Medical records of children referred to Emergency Room in 2010 for suspected appendicitis were retrospectively reviewed. Diagnostic investigations divided by age and sex were related to pathological findings. Negative appendectomy and complication rates were calculated. 923 children needed surgical assessment : In 75.7% of them surgical indication was excluded and 24.3% were admitted to surgical ward for observation. Appendectomy was eventually performed in 137 patients (61.9%), 82.4% of them without any preoperative imaging while 17.6% underwent selective studies, mainly abdominal ultrasonography (14.6%). Imaging was requested twice as frequently in not operated admitted children (39.3%) than in the operated ones (17.5%, P<0.001). Overall complicated appendicitis rate (peritonitis and abscess) resulted 26.4% and negative appendectomy rate 8.8%. Females older than 10 years presented histologically not-confirmed appendicitis in 22.2% of cases, while the younger ones presented more frequently complicated appendicitis (29.3%).\n\nQuestion: Do we need imaging to diagnose appendicitis in children?", "question_only": "Do we need imaging to diagnose appendicitis in children?", "context": "To evaluate the role of clinical assessment with selective use of imaging studies in the management of suspected acute appendicitis in children. Medical records of children referred to Emergency Room in 2010 for suspected appendicitis were retrospectively reviewed. Diagnostic investigations divided by age and sex were related to pathological findings. Negative appendectomy and complication rates were calculated. 923 children needed surgical assessment : In 75.7% of them surgical indication was excluded and 24.3% were admitted to surgical ward for observation. Appendectomy was eventually performed in 137 patients (61.9%), 82.4% of them without any preoperative imaging while 17.6% underwent selective studies, mainly abdominal ultrasonography (14.6%). Imaging was requested twice as frequently in not operated admitted children (39.3%) than in the operated ones (17.5%, P<0.001). Overall complicated appendicitis rate (peritonitis and abscess) resulted 26.4% and negative appendectomy rate 8.8%. Females older than 10 years presented histologically not-confirmed appendicitis in 22.2% of cases, while the younger ones presented more frequently complicated appendicitis (29.3%).", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Clinical assessment is the key to diagnose appendicitis. Nevertheless, in girls older than 10 years, selected use of imaging should be implemented to avoid unnecessary appendectomies. Imaging of choice in equivocal cases should be ultrasonography.", "meshes": ["Acute Disease", "Adolescent", "Appendectomy", "Appendicitis", "Child", "Child, Preschool", "Diagnosis, Differential", "Diagnostic Errors", "Diagnostic Imaging", "Early Diagnosis", "Female", "Follow-Up Studies", "Humans", "Infant", "Infant, Newborn", "Male", "Retrospective Studies"], "year": null}
{"id": "pubmedqa_20828836", "dataset": "pubmedqa", "question": "Context: To determine the perinatal predictors of discordant screening outcomes based on a two-stage screening protocol with transient-evoked otoacoustic emissions (TEOAE) and automated auditory brainstem response (AABR). A cross-sectional study of infants tested with TEOAE and AABR under a hospital-based universal newborn hearing screening program in Lagos, Nigeria. Maternal and infant factors associated with discordant TEOAE and AABR outcomes were determined with multivariable logistic regression analyses adjusting for potential confounding factors. Of the 4718 infants enrolled under the program 1745 (36.9%) completed both TEOAE and AABR. Of this group, 1060 (60.7%) passed both TEOAE and AABR (\"true-negatives\"); 92 (5.3%) failed both TEOAE and AABR (\"true-positive\"); 571 (32.7%) failed TEOAE but passed AABR (\"false-positives\") while 22 (1.3%) passed TEOAE but failed AABR (\"false-negatives\"). Infants with false-positives were likely to be admitted into well-baby nursery (p=0.001), belong to mothers who attended antenatal care (p=0.010) or who delivered vaginally (p<0.001) compared to infants with true-negatives while infants with true-positives were also more likely to be delivered vaginally (p=0.002) or admitted into well-baby nursery (p=0.035) compared to infants with false-negatives. Infants with true-positives were significantly more likely to be delivered vaginally (p<0.001) and have severe hyperbilirubinemia (p=0.045) compared with infants with true-negatives. No association was observed between false-negatives and true-negatives. Antenatal care status, mode of delivery and nursery type were useful predictors of discordant outcomes among all infants undergoing screening (c-statistic=0.73).\n\nQuestion: Is discordance in TEOAE and AABR outcomes predictable in newborns?", "question_only": "Is discordance in TEOAE and AABR outcomes predictable in newborns?", "context": "To determine the perinatal predictors of discordant screening outcomes based on a two-stage screening protocol with transient-evoked otoacoustic emissions (TEOAE) and automated auditory brainstem response (AABR). A cross-sectional study of infants tested with TEOAE and AABR under a hospital-based universal newborn hearing screening program in Lagos, Nigeria. Maternal and infant factors associated with discordant TEOAE and AABR outcomes were determined with multivariable logistic regression analyses adjusting for potential confounding factors. Of the 4718 infants enrolled under the program 1745 (36.9%) completed both TEOAE and AABR. Of this group, 1060 (60.7%) passed both TEOAE and AABR (\"true-negatives\"); 92 (5.3%) failed both TEOAE and AABR (\"true-positive\"); 571 (32.7%) failed TEOAE but passed AABR (\"false-positives\") while 22 (1.3%) passed TEOAE but failed AABR (\"false-negatives\"). Infants with false-positives were likely to be admitted into well-baby nursery (p=0.001), belong to mothers who attended antenatal care (p=0.010) or who delivered vaginally (p<0.001) compared to infants with true-negatives while infants with true-positives were also more likely to be delivered vaginally (p=0.002) or admitted into well-baby nursery (p=0.035) compared to infants with false-negatives. Infants with true-positives were significantly more likely to be delivered vaginally (p<0.001) and have severe hyperbilirubinemia (p=0.045) compared with infants with true-negatives. No association was observed between false-negatives and true-negatives. Antenatal care status, mode of delivery and nursery type were useful predictors of discordant outcomes among all infants undergoing screening (c-statistic=0.73).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Given the available screening technologies, discordant TEOAE and AABR may be inevitable for some categories of hearing loss among apparently healthy newborns whose mothers received prenatal care. The potential limitations of perinatal morbidities as basis of targeted screening for such cases therefore merit further consideration.", "meshes": ["Adult", "Cross-Sectional Studies", "Delivery, Obstetric", "Evoked Potentials, Auditory, Brain Stem", "False Negative Reactions", "False Positive Reactions", "Female", "Hearing Loss", "Humans", "Hyperbilirubinemia, Neonatal", "Infant, Newborn", "Logistic Models", "Male", "Neonatal Screening", "Nigeria", "Nurseries, Hospital", "Otoacoustic Emissions, Spontaneous", "Prenatal Care", "Retrospective Studies"], "year": "2010"}
{"id": "pubmedqa_11483547", "dataset": "pubmedqa", "question": "Context: To determine the incidence and severity of acute side effects from the use of polyvalent antivenin in victims of rattlesnake bites. We retrospectively reviewed the records of all patients who presented with rattlesnake bites to a university teaching hospital during an 11-year period. From patient medical records, we extracted demographic data, clinical measurements, and outcomes during emergency department evaluation and subsequent hospitalization. Data regarding serum sickness were not collected. Primary outcome variables were the occurrence of immediate hypersensitivity reaction to antivenin, the type of reaction, permanent disability at hospital discharge, and mortality. We identified a total of 73 patients with rattlesnake bites during the study period. Bite envenomation was graded as nonenvenomated, 7 patients (10%); mild, 23 patients (32%); moderate, 32 patients (44%); and severe, 11 patients (15%). We identified 65 patients who received antivenin. Antivenin doses ranged from 1 to 30 vials per patient (mean, 12.0 +/- 6.0), for a total of 777 vials. In 43 patients (66%), 10 or more vials of antivenin were given. The mean number of vials of antivenin given to each snakebite grade were as follows: mild, 8.4 (+/-4.0); moderate, 11.8 (+/-5.7); and severe, 18.7 (+/-6.3). No deaths, amputations, or permanent disability from snakebite occurred in the patients receiving antivenin. Acute side effects of antivenin-occurring within the first 6 hours after administration-were seen in 12 patients (18%; 95% confidence interval, 10%-30%). Acute side effects consisted solely of urticaria in all but 1 patient (2%; 95% confidence interval, 0%-8%). This patient had a history of previous antivenin reaction and required a short course of intravenous epinephrine for blood pressure support. No other complications occurred.\n\nQuestion: Does the aggressive use of polyvalent antivenin for rattlesnake bites result in serious acute side effects?", "question_only": "Does the aggressive use of polyvalent antivenin for rattlesnake bites result in serious acute side effects?", "context": "To determine the incidence and severity of acute side effects from the use of polyvalent antivenin in victims of rattlesnake bites. We retrospectively reviewed the records of all patients who presented with rattlesnake bites to a university teaching hospital during an 11-year period. From patient medical records, we extracted demographic data, clinical measurements, and outcomes during emergency department evaluation and subsequent hospitalization. Data regarding serum sickness were not collected. Primary outcome variables were the occurrence of immediate hypersensitivity reaction to antivenin, the type of reaction, permanent disability at hospital discharge, and mortality. We identified a total of 73 patients with rattlesnake bites during the study period. Bite envenomation was graded as nonenvenomated, 7 patients (10%); mild, 23 patients (32%); moderate, 32 patients (44%); and severe, 11 patients (15%). We identified 65 patients who received antivenin. Antivenin doses ranged from 1 to 30 vials per patient (mean, 12.0 +/- 6.0), for a total of 777 vials. In 43 patients (66%), 10 or more vials of antivenin were given. The mean number of vials of antivenin given to each snakebite grade were as follows: mild, 8.4 (+/-4.0); moderate, 11.8 (+/-5.7); and severe, 18.7 (+/-6.3). No deaths, amputations, or permanent disability from snakebite occurred in the patients receiving antivenin. Acute side effects of antivenin-occurring within the first 6 hours after administration-were seen in 12 patients (18%; 95% confidence interval, 10%-30%). Acute side effects consisted solely of urticaria in all but 1 patient (2%; 95% confidence interval, 0%-8%). This patient had a history of previous antivenin reaction and required a short course of intravenous epinephrine for blood pressure support. No other complications occurred.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The administration of polyvalent Crotalidae antivenin is safe. Acute hypersensitivity, when it occurs, consists solely in most cases of urticaria. Serious side effects are uncommon.", "meshes": ["Adolescent", "Adult", "Aged", "Animals", "Antivenins", "Crotalus", "Female", "Humans", "Infant", "Male", "Retrospective Studies", "Snake Bites", "Urticaria"], "year": "2001"}
{"id": "pubmedqa_17076091", "dataset": "pubmedqa", "question": "Context: We sought to determine whether patients with obstructive sleep apnea (OSA) had an objective change in aerobic fitness during cycle ergometry compared to a normal population. The most accurate test of aerobic fitness is measurement of maximum oxygen consumption (VO2max) with cycle ergometry. We performed a retrospective cohort analysis (247 patients with OSA) of VO2max from annual cycle ergometry tests compared to a large control group (normative data from 1.4 million US Air Force tests) in a tertiary care setting. Overall, individuals with OSA had increased VO2max when compared to the normalized US Air Force data (p<.001). Patients with an apnea-hypopnea index of greater than 20 demonstrated a decreased VO2max as compared to normalized values (p<.001). No differences in VO2max were observed after either medical or surgical therapy for OSA.\n\nQuestion: Does obstructive sleep apnea affect aerobic fitness?", "question_only": "Does obstructive sleep apnea affect aerobic fitness?", "context": "We sought to determine whether patients with obstructive sleep apnea (OSA) had an objective change in aerobic fitness during cycle ergometry compared to a normal population. The most accurate test of aerobic fitness is measurement of maximum oxygen consumption (VO2max) with cycle ergometry. We performed a retrospective cohort analysis (247 patients with OSA) of VO2max from annual cycle ergometry tests compared to a large control group (normative data from 1.4 million US Air Force tests) in a tertiary care setting. Overall, individuals with OSA had increased VO2max when compared to the normalized US Air Force data (p<.001). Patients with an apnea-hypopnea index of greater than 20 demonstrated a decreased VO2max as compared to normalized values (p<.001). No differences in VO2max were observed after either medical or surgical therapy for OSA.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Overall, in a US Air Force population, OSA does not predict a decrease in aerobic fitness as measured by cycle ergometry. However, patients with an apnea-hypopnea index of greater than 20 have a statistically significant decrease in aerobic fitness compared to the normal population. This study demonstrates the effects of OSA on aerobic fitness. Further correlation of fitness testing results with OSA severity and treatment is needed.", "meshes": ["Adolescent", "Adult", "Cohort Studies", "Continuous Positive Airway Pressure", "Exercise Test", "Female", "Humans", "Male", "Middle Aged", "Military Personnel", "Oxygen Consumption", "Physical Fitness", "Retrospective Studies", "Sleep Apnea, Obstructive", "United States"], "year": "2006"}
{"id": "pubmedqa_8921484", "dataset": "pubmedqa", "question": "Context: After 34 weeks gestation, summary measures of location for birthweight (e.g means and centiles) increase more slowly for Australian Aborigines than for whites. A similar pattern has been observed for blacks in the US. This study tests whether the reported pattern is due to differential misclassification of gestational age. Simulation was used to measure the potential effect of differential misclassification of gestational age. Reported gestational age data were obtained from Queensland Perinatal Data Collection (QPDC). Estimates of the true distributions of gestational age were obtained by assuming various (plausible) types of misclassification and applying these to the reported distributions. Previous studies and data from the QPDC were used to help specify the birthweight distributions used in the simulations. At full term, the parameters of the birthweight distributions were robust to gestational age misclassification. At preterm, the 10th centiles were robust to misclassification. In contrast, the 90th centiles were sensitive to even minor misclassification. Extreme types of misclassification were required to remove the divergence in median birthweights for Aborigines and whites.\n\nQuestion: Does gestational age misclassification explain the difference in birthweights for Australian aborigines and whites?", "question_only": "Does gestational age misclassification explain the difference in birthweights for Australian aborigines and whites?", "context": "After 34 weeks gestation, summary measures of location for birthweight (e.g means and centiles) increase more slowly for Australian Aborigines than for whites. A similar pattern has been observed for blacks in the US. This study tests whether the reported pattern is due to differential misclassification of gestational age. Simulation was used to measure the potential effect of differential misclassification of gestational age. Reported gestational age data were obtained from Queensland Perinatal Data Collection (QPDC). Estimates of the true distributions of gestational age were obtained by assuming various (plausible) types of misclassification and applying these to the reported distributions. Previous studies and data from the QPDC were used to help specify the birthweight distributions used in the simulations. At full term, the parameters of the birthweight distributions were robust to gestational age misclassification. At preterm, the 10th centiles were robust to misclassification. In contrast, the 90th centiles were sensitive to even minor misclassification. Extreme types of misclassification were required to remove the divergence in median birthweights for Aborigines and whites.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Gestational age misclassification is an unlikely explanation for the reported divergence in average birth-weights for Aborigines and whites. The results might help with the interpretation of other between-population comparisons.", "meshes": ["Adult", "Australia", "Birth Weight", "Classification", "Computer Simulation", "European Continental Ancestry Group", "Female", "Gestational Age", "Humans", "Male", "Oceanic Ancestry Group", "Pregnancy"], "year": "1996"}
{"id": "pubmedqa_16968876", "dataset": "pubmedqa", "question": "Context: The aim of this prognostic factor analysis was to investigate if a patient's self-reported health-related quality of life (HRQOL) provided independent prognostic information for survival in non-small cell lung cancer (NSCLC) patients. Pretreatment HRQOL was measured in 391 advanced NSCLC patients using the EORTC QLQ-C30 and the EORTC Lung Cancer module (QLQ-LC13). The Cox proportional hazards regression model was used for both univariate and multivariate analyses of survival. In addition, a bootstrap validation technique was used to assess the stability of the outcomes. The final multivariate Cox regression model retained four parameters as independent prognostic factors for survival: male gender with a hazard ratio (HR) = 1.32 (95% CI 1.03-1.69; P = 0.03); performance status (0 to 1 versus 2) with HR = 1.63 (95% CI 1.04-2.54; P = 0.032); patient's self-reported score of pain with HR= 1.11 (95% CI 1.07-1.16; P<0.001) and dysphagia with HR = 1.12 (95% CI 1.04-1.21; P = 0.003). A 10-point shift worse in the scale measuring pain and dysphagia translated into an 11% and 12% increased in the likelihood of death respectively. A risk group categorization was also developed.\n\nQuestion: Is a patient's self-reported health-related quality of life a prognostic factor for survival in non-small-cell lung cancer patients?", "question_only": "Is a patient's self-reported health-related quality of life a prognostic factor for survival in non-small-cell lung cancer patients?", "context": "The aim of this prognostic factor analysis was to investigate if a patient's self-reported health-related quality of life (HRQOL) provided independent prognostic information for survival in non-small cell lung cancer (NSCLC) patients. Pretreatment HRQOL was measured in 391 advanced NSCLC patients using the EORTC QLQ-C30 and the EORTC Lung Cancer module (QLQ-LC13). The Cox proportional hazards regression model was used for both univariate and multivariate analyses of survival. In addition, a bootstrap validation technique was used to assess the stability of the outcomes. The final multivariate Cox regression model retained four parameters as independent prognostic factors for survival: male gender with a hazard ratio (HR) = 1.32 (95% CI 1.03-1.69; P = 0.03); performance status (0 to 1 versus 2) with HR = 1.63 (95% CI 1.04-2.54; P = 0.032); patient's self-reported score of pain with HR= 1.11 (95% CI 1.07-1.16; P<0.001) and dysphagia with HR = 1.12 (95% CI 1.04-1.21; P = 0.003). A 10-point shift worse in the scale measuring pain and dysphagia translated into an 11% and 12% increased in the likelihood of death respectively. A risk group categorization was also developed.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "The results suggest that patients' self-reported HRQOL provide independent prognostic information for survival. This finding supports the collection of such data in routine clinical practice.", "meshes": ["Adult", "Aged", "Carcinoma, Non-Small-Cell Lung", "Europe", "Female", "Health Status", "Humans", "Male", "Middle Aged", "Multivariate Analysis", "Prognosis", "Quality of Life", "Regression Analysis", "Survival Analysis"], "year": "2006"}
{"id": "pubmedqa_15631914", "dataset": "pubmedqa", "question": "Context: Clinically positive axillary nodes are widely considered a contraindication to sentinel lymph node (SLN) biopsy in breast cancer, yet no data support this mandate. In fact, data from the era of axillary lymph node dissection (ALND) suggest that clinical examination of the axilla is falsely positive in as many as 30% of cases. Here we report the results of SLN biopsy in a selected group of breast cancer patients with palpable axillary nodes classified as either moderately or highly suspicious for metastasis. Among 2,027 consecutive SLN biopsy procedures performed by two experienced surgeons, clinically suspicious axillary nodes were identified in 106, and categorized as group 1 (asymmetric enlargement of the ipsilateral axillary nodes moderately suspicious for metastasis, n = 62) and group 2 (clinically positive axillary nodes highly suspicious for metastasis, n = 44). Clinical examination of the axilla was inaccurate in 41% of patients (43 of 106) overall, and was falsely positive in 53% of patients (33 of 62) with moderately suspicious nodes and 23% of patients (10 of 44) with highly suspicious nodes. False-positive results were less frequent with larger tumor size (p = 0.002) and higher histologic grade (p = 0.002), but were not associated with age, body mass index, or a previous surgical biopsy.\n\nQuestion: Is the clinically positive axilla in breast cancer really a contraindication to sentinel lymph node biopsy?", "question_only": "Is the clinically positive axilla in breast cancer really a contraindication to sentinel lymph node biopsy?", "context": "Clinically positive axillary nodes are widely considered a contraindication to sentinel lymph node (SLN) biopsy in breast cancer, yet no data support this mandate. In fact, data from the era of axillary lymph node dissection (ALND) suggest that clinical examination of the axilla is falsely positive in as many as 30% of cases. Here we report the results of SLN biopsy in a selected group of breast cancer patients with palpable axillary nodes classified as either moderately or highly suspicious for metastasis. Among 2,027 consecutive SLN biopsy procedures performed by two experienced surgeons, clinically suspicious axillary nodes were identified in 106, and categorized as group 1 (asymmetric enlargement of the ipsilateral axillary nodes moderately suspicious for metastasis, n = 62) and group 2 (clinically positive axillary nodes highly suspicious for metastasis, n = 44). Clinical examination of the axilla was inaccurate in 41% of patients (43 of 106) overall, and was falsely positive in 53% of patients (33 of 62) with moderately suspicious nodes and 23% of patients (10 of 44) with highly suspicious nodes. False-positive results were less frequent with larger tumor size (p = 0.002) and higher histologic grade (p = 0.002), but were not associated with age, body mass index, or a previous surgical biopsy.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Clinical axillary examination in breast cancer is subject to false-positive results, and is by itself insufficient justification for axillary lymph node dissection. If other means of preoperative assessment such as palpation- or image-guided fine needle aspiration are negative or indeterminate, then SLN biopsy deserves wider consideration as an alternative to routine axillary lymph node dissection in the clinically node-positive setting.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Axilla", "Breast Neoplasms", "False Positive Reactions", "Female", "Humans", "Lymph Nodes", "Lymphatic Metastasis", "Middle Aged", "Predictive Value of Tests", "Reproducibility of Results", "Sentinel Lymph Node Biopsy"], "year": "2005"}
{"id": "pubmedqa_26578404", "dataset": "pubmedqa", "question": "Context: Breathlessness is one of the most distressing symptoms experienced by patients with advanced cancer and noncancer diagnoses alike. Often, severity of breathlessness increases quickly, calling for rapid symptom control. Oral, buccal, and parenteral routes of provider-controlled drug administration have been described. It is unclear whether patient-controlled therapy (PCT) systems would be an additional treatment option. To investigate whether intravenous opioid PCT can be an effective therapeutic method to reduce breathlessness in patients with advanced disease. Secondary aims were to study the feasibility and acceptance of opioid PCT in patients with refractory breathlessness. This was a pilot observational study with 18 inpatients with advanced disease and refractory breathlessness receiving opioid PCT. Breathlessness was measured on a self-reported numeric rating scale. Richmond Agitation Sedation Scale scores, Palliative Performance Scale scores, vital signs, and a self-developed patient satisfaction questionnaire were used for measuring secondary outcomes. Descriptive and interference analyses (Friedman test) and post hoc analyses (Wilcoxon tests and Bonferroni corrections) were performed. Eighteen of 815 patients (advanced cancer; median age = 57.5 years [range 36-81]; 77.8% female) received breathlessness symptom control with opioid PCT; daily morphine equivalent dose at Day 1 was median = 20.3 mg (5.0-49.6 mg); Day 2: 13.0 mg (1.0-78.5 mg); Day 3: 16.0 mg (8.3-47.0 mg). Numeric rating scale of current breathlessness decreased (baseline: median = 5 [range 1-10]; Day 1: median = 4 [range 0-8], P < 0.01; Day 2: median = 4 [range 0-5], P < 0.01). Physiological parameters were stable over time. On Day 3, 12/12 patients confirmed that this mode of application provided relief of breathlessness.\n\nQuestion: Patient-Controlled Therapy of Breathlessness in Palliative Care: A New Therapeutic Concept for Opioid Administration?", "question_only": "Patient-Controlled Therapy of Breathlessness in Palliative Care: A New Therapeutic Concept for Opioid Administration?", "context": "Breathlessness is one of the most distressing symptoms experienced by patients with advanced cancer and noncancer diagnoses alike. Often, severity of breathlessness increases quickly, calling for rapid symptom control. Oral, buccal, and parenteral routes of provider-controlled drug administration have been described. It is unclear whether patient-controlled therapy (PCT) systems would be an additional treatment option. To investigate whether intravenous opioid PCT can be an effective therapeutic method to reduce breathlessness in patients with advanced disease. Secondary aims were to study the feasibility and acceptance of opioid PCT in patients with refractory breathlessness. This was a pilot observational study with 18 inpatients with advanced disease and refractory breathlessness receiving opioid PCT. Breathlessness was measured on a self-reported numeric rating scale. Richmond Agitation Sedation Scale scores, Palliative Performance Scale scores, vital signs, and a self-developed patient satisfaction questionnaire were used for measuring secondary outcomes. Descriptive and interference analyses (Friedman test) and post hoc analyses (Wilcoxon tests and Bonferroni corrections) were performed. Eighteen of 815 patients (advanced cancer; median age = 57.5 years [range 36-81]; 77.8% female) received breathlessness symptom control with opioid PCT; daily morphine equivalent dose at Day 1 was median = 20.3 mg (5.0-49.6 mg); Day 2: 13.0 mg (1.0-78.5 mg); Day 3: 16.0 mg (8.3-47.0 mg). Numeric rating scale of current breathlessness decreased (baseline: median = 5 [range 1-10]; Day 1: median = 4 [range 0-8], P < 0.01; Day 2: median = 4 [range 0-5], P < 0.01). Physiological parameters were stable over time. On Day 3, 12/12 patients confirmed that this mode of application provided relief of breathlessness.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Opioid PCT is a feasible and acceptable therapeutic method to reduce refractory breathlessness in palliative care patients.", "meshes": ["Administration, Intravenous", "Adult", "Aged", "Aged, 80 and over", "Analgesia, Patient-Controlled", "Analgesics, Opioid", "Dyspnea", "Feasibility Studies", "Female", "Humans", "Longitudinal Studies", "Male", "Middle Aged", "Neoplasms", "Palliative Care", "Patient Satisfaction", "Prospective Studies", "Self Report", "Severity of Illness Index"], "year": "2016"}
{"id": "pubmedqa_23337545", "dataset": "pubmedqa", "question": "Context: Acute fibrinous and organizing pneumonia (AFOP) is a recently described histologic pattern of diffuse pulmonary disease. In children, all cases reported to date have been fatal. In this study, we describe the first nonfatal AFOP in a child and review the literature. A 10-year-old boy developed very severe aplastic anemia (VSAA) after being admitted to our hospital with a fulminant hepatic failure of unknown origin. A chest computed tomography scan revealed multiple lung nodules and a biopsy of a pulmonary lesion showed all the signs of AFOP. Infectious workup remained negative. We started immunosuppressive therapy with antithymocyte globulin and cyclosporine to treat VSAA. Subsequent chest computed tomography scans showed a considerable diminution of the lung lesions but the VSAA did not improve until we performed hematopoietic stem cell transplantation 5 months later.\n\nQuestion: Is acute fibrinous and organizing pneumonia the expression of immune dysregulation?", "question_only": "Is acute fibrinous and organizing pneumonia the expression of immune dysregulation?", "context": "Acute fibrinous and organizing pneumonia (AFOP) is a recently described histologic pattern of diffuse pulmonary disease. In children, all cases reported to date have been fatal. In this study, we describe the first nonfatal AFOP in a child and review the literature. A 10-year-old boy developed very severe aplastic anemia (VSAA) after being admitted to our hospital with a fulminant hepatic failure of unknown origin. A chest computed tomography scan revealed multiple lung nodules and a biopsy of a pulmonary lesion showed all the signs of AFOP. Infectious workup remained negative. We started immunosuppressive therapy with antithymocyte globulin and cyclosporine to treat VSAA. Subsequent chest computed tomography scans showed a considerable diminution of the lung lesions but the VSAA did not improve until we performed hematopoietic stem cell transplantation 5 months later.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Aplastic anemia is associated with a variety of autoimmune syndromes. The sequence of events in our patient suggests that the hepatic failure, AFOP, and the VSAA may all have been part of an autoimmune syndrome. AFOP could be the result of immune dysregulation in this pediatric case with favorable outcome after immunosuppressive therapy and hematopoietic stem cell transplantation.", "meshes": ["Acute Disease", "Child", "Cryptogenic Organizing Pneumonia", "Hematopoietic Stem Cell Transplantation", "Humans", "Immune System Diseases", "Immunosuppressive Agents", "Male"], "year": "2013"}
{"id": "pubmedqa_22902073", "dataset": "pubmedqa", "question": "Context: The purpose of this study was to investigate whether knowledge of ultrasound-obtained estimated fetal weight (US-EFW) is a risk factor for cesarean delivery (CD). Retrospective cohort from a single center in 2009-2010 of singleton, term live births. CD rates were compared for women with and without US-EFW within 1 month of delivery and adjusted for potential confounders. Of the 2329 women in our cohort, 50.2% had US-EFW within 1 month of delivery. CD was significantly more common for women with US-EFW (15.7% vs 10.2%; P<.001); after we controlled for confounders, US-EFW remained an independent risk factor for CD (odds ratio, 1.44; 95% confidence interval, 1.1-1.9). The risk increased when US-EFW was>3500 g (odds ratio, 1.8; 95% confidence interval, 1.3-2.7).\n\nQuestion: Estimated fetal weight by ultrasound: a modifiable risk factor for cesarean delivery?", "question_only": "Estimated fetal weight by ultrasound: a modifiable risk factor for cesarean delivery?", "context": "The purpose of this study was to investigate whether knowledge of ultrasound-obtained estimated fetal weight (US-EFW) is a risk factor for cesarean delivery (CD). Retrospective cohort from a single center in 2009-2010 of singleton, term live births. CD rates were compared for women with and without US-EFW within 1 month of delivery and adjusted for potential confounders. Of the 2329 women in our cohort, 50.2% had US-EFW within 1 month of delivery. CD was significantly more common for women with US-EFW (15.7% vs 10.2%; P<.001); after we controlled for confounders, US-EFW remained an independent risk factor for CD (odds ratio, 1.44; 95% confidence interval, 1.1-1.9). The risk increased when US-EFW was>3500 g (odds ratio, 1.8; 95% confidence interval, 1.3-2.7).", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Knowledge of US-EFW, above and beyond the impact of fetal size itself, increases the risk of CD. Acquisition of US-EFW near term appears to be an independent and potentially modifiable risk factor for CD.", "meshes": ["Birth Weight", "Cesarean Section", "Female", "Fetal Weight", "Gestational Age", "Humans", "Predictive Value of Tests", "Pregnancy", "Retrospective Studies", "Risk Factors", "Ultrasonography, Prenatal"], "year": "2012"}
{"id": "pubmedqa_24748473", "dataset": "pubmedqa", "question": "Context: Laparoscopic sleeve gastrectomy (LSG) is currently being performed with increasing frequency worldwide. It offers an excellent weight loss and resolution of comorbidities in the short term with a very low incidence of complications. However, the ever present risk of a staple line leak is still a major concern. Since 2005, data from obese patients that undergo bariatric procedures in Germany are prospectively registered in an online database and analyzed at the Institute of Quality Assurance in Surgical Medicine. For the current analysis, all patients that had undergone primary sleeve gastrectomy for morbid obesity within a 7-year period were considered. Using the GBSR, data from 5.400 LSGs were considered for analysis. Staple line leak rate decreased during the study period from 6.5 to 1.4 %. Male gender, higher BMI, concomitant sleep apnea, conversion to laparotomy, longer operation time, use of both buttresses and oversewing, and the occurrence of intraoperative complications were associated with a significantly higher leakage rate. On multivariate analysis, operation time and year of procedure only had a significant impact on staple line leak rate.\n\nQuestion: Are there risk factors that increase the rate of staple line leakage in patients undergoing primary sleeve gastrectomy for morbid obesity?", "question_only": "Are there risk factors that increase the rate of staple line leakage in patients undergoing primary sleeve gastrectomy for morbid obesity?", "context": "Laparoscopic sleeve gastrectomy (LSG) is currently being performed with increasing frequency worldwide. It offers an excellent weight loss and resolution of comorbidities in the short term with a very low incidence of complications. However, the ever present risk of a staple line leak is still a major concern. Since 2005, data from obese patients that undergo bariatric procedures in Germany are prospectively registered in an online database and analyzed at the Institute of Quality Assurance in Surgical Medicine. For the current analysis, all patients that had undergone primary sleeve gastrectomy for morbid obesity within a 7-year period were considered. Using the GBSR, data from 5.400 LSGs were considered for analysis. Staple line leak rate decreased during the study period from 6.5 to 1.4 %. Male gender, higher BMI, concomitant sleep apnea, conversion to laparotomy, longer operation time, use of both buttresses and oversewing, and the occurrence of intraoperative complications were associated with a significantly higher leakage rate. On multivariate analysis, operation time and year of procedure only had a significant impact on staple line leak rate.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The results of the current study demonstrated that there are factors that increase the risk of a leakage which would enable surgeons to define risk groups, to more carefully select patients, and to offer a closer follow-up during the postoperative course with early recognition and adequate treatment. All future efforts should be focused on a further reduction of serious complications to make the LSG a widely accepted and safer procedure.", "meshes": ["Adolescent", "Adult", "Aged", "Bariatric Surgery", "Body Mass Index", "Comorbidity", "Female", "Gastrectomy", "Germany", "Humans", "Intraoperative Complications", "Laparoscopy", "Male", "Middle Aged", "Obesity, Morbid", "Operative Time", "Postoperative Complications", "Risk Factors", "Surgical Stapling", "Weight Loss", "Young Adult"], "year": "2014"}
{"id": "pubmedqa_15943725", "dataset": "pubmedqa", "question": "Context: Serum pancreatic lipase may improve the diagnosis of pancreatitis compared to serum amylase. Both enzymes have been measured simultaneously at our hospital allowing for a comparison of their diagnostic accuracy. Seventeen thousand five hundred and thirty-one measurements of either serum amylase and or serum pancreatic lipase were made on 10 931 patients treated at a metropolitan teaching hospital between January 2001 and May 2003. Of these, 8937 were initially treated in the Emergency Department. These results were collected in a database, which was linked by the patients' medical record number to the radiology and medical records. Patients with either an elevated lipase value or a discharge diagnosis of acute pancreatitis had their radiological diagnosis reviewed along with their biochemistry and histology record. The diagnosis of acute pancreatitis was made if there was radiological evidence of peripancreatic inflammation. One thousand eight hundred and twenty-five patients had either elevated serum amylase and or serum pancreatic lipase. The medical records coded for pancreatitis in a further 55 whose enzymes were not elevated. Three hundred and twenty of these had radiological evidence of acute pancreatitis. Receiver operator characteristic analysis of the initial sample from patients received in the Emergency Department showed improved diagnostic accuracy for serum pancreatic lipase (area under the curve (AUC) 0.948) compared with serum amylase (AUC, 0.906, P<0.05). A clinically useful cut-off point would be at the diagnostic threshold; 208 U/L (normal<190 U/L) for serum pancreatic lipase and 114 U/L (normal 27-100 U/L) for serum amylase where the sensitivity was 90.3 cf., 76.8% and the specificity was 93 cf., 92.6%. 18.8% of the acute pancreatitis patients did not have elevated serum amylase while only 2.9% did not have elevated serum pancreatic lipase on the first emergency department measurement.\n\nQuestion: Should serum pancreatic lipase replace serum amylase as a biomarker of acute pancreatitis?", "question_only": "Should serum pancreatic lipase replace serum amylase as a biomarker of acute pancreatitis?", "context": "Serum pancreatic lipase may improve the diagnosis of pancreatitis compared to serum amylase. Both enzymes have been measured simultaneously at our hospital allowing for a comparison of their diagnostic accuracy. Seventeen thousand five hundred and thirty-one measurements of either serum amylase and or serum pancreatic lipase were made on 10 931 patients treated at a metropolitan teaching hospital between January 2001 and May 2003. Of these, 8937 were initially treated in the Emergency Department. These results were collected in a database, which was linked by the patients' medical record number to the radiology and medical records. Patients with either an elevated lipase value or a discharge diagnosis of acute pancreatitis had their radiological diagnosis reviewed along with their biochemistry and histology record. The diagnosis of acute pancreatitis was made if there was radiological evidence of peripancreatic inflammation. One thousand eight hundred and twenty-five patients had either elevated serum amylase and or serum pancreatic lipase. The medical records coded for pancreatitis in a further 55 whose enzymes were not elevated. Three hundred and twenty of these had radiological evidence of acute pancreatitis. Receiver operator characteristic analysis of the initial sample from patients received in the Emergency Department showed improved diagnostic accuracy for serum pancreatic lipase (area under the curve (AUC) 0.948) compared with serum amylase (AUC, 0.906, P<0.05). A clinically useful cut-off point would be at the diagnostic threshold; 208 U/L (normal<190 U/L) for serum pancreatic lipase and 114 U/L (normal 27-100 U/L) for serum amylase where the sensitivity was 90.3 cf., 76.8% and the specificity was 93 cf., 92.6%. 18.8% of the acute pancreatitis patients did not have elevated serum amylase while only 2.9% did not have elevated serum pancreatic lipase on the first emergency department measurement.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "It is concluded that serum pancreatic lipase is a more accurate biomarker of acute pancreatitis than serum amylase.", "meshes": ["Acute Disease", "Amylases", "Biomarkers", "Humans", "Lipase", "Pancreas", "Pancreatitis", "Radiography", "Sensitivity and Specificity", "Time Factors"], "year": "2005"}
{"id": "pubmedqa_20871246", "dataset": "pubmedqa", "question": "Context: The objectives of this study were to evaluate the ability of the Young-Burgess classification system to predict mortality, transfusion requirements, and nonorthopaedic injuries in patients with pelvic ring fractures and to determine whether mortality rates after pelvic fractures have changed over time. Retrospective review. Level I trauma center. One thousand two hundred forty-eight patients with pelvic fractures during a 7-year period. None. Mortality at index admission, transfusion requirement during first 24 hours, and presence of nonorthopaedic injuries as a function of Young-Burgess pelvic classification type. Mortality compared with historic controls. Despite a relatively large sample size, the ability of the Young-Burgess system to predict mortality only approached statistical significance (P = 0.07, Kruskal-Wallis). The Young-Burgess system differentiated transfusion requirements--lateral compression Type 3 (LC3) and anteroposterior compression Types 2 (APC2) and 3 (APC3) fractures had higher transfusion requirements than did lateral compression Type 1 (LC1), anteroposterior compression Type 1 (APC1), and vertical shear (VS) (P<0.05)--but was not as useful at predicting head, chest, or abdomen injuries. Dividing fractures into stable and unstable types allowed the system to predict mortality rates, abdomen injury rates, and transfusion requirements. Overall mortality in the study group was 9.1%, unchanged from original Young-Burgess studies 15 years previously (P = 0.3).\n\nQuestion: Young-Burgess classification of pelvic ring fractures: does it predict mortality, transfusion requirements, and non-orthopaedic injuries?", "question_only": "Young-Burgess classification of pelvic ring fractures: does it predict mortality, transfusion requirements, and non-orthopaedic injuries?", "context": "The objectives of this study were to evaluate the ability of the Young-Burgess classification system to predict mortality, transfusion requirements, and nonorthopaedic injuries in patients with pelvic ring fractures and to determine whether mortality rates after pelvic fractures have changed over time. Retrospective review. Level I trauma center. One thousand two hundred forty-eight patients with pelvic fractures during a 7-year period. None. Mortality at index admission, transfusion requirement during first 24 hours, and presence of nonorthopaedic injuries as a function of Young-Burgess pelvic classification type. Mortality compared with historic controls. Despite a relatively large sample size, the ability of the Young-Burgess system to predict mortality only approached statistical significance (P = 0.07, Kruskal-Wallis). The Young-Burgess system differentiated transfusion requirements--lateral compression Type 3 (LC3) and anteroposterior compression Types 2 (APC2) and 3 (APC3) fractures had higher transfusion requirements than did lateral compression Type 1 (LC1), anteroposterior compression Type 1 (APC1), and vertical shear (VS) (P<0.05)--but was not as useful at predicting head, chest, or abdomen injuries. Dividing fractures into stable and unstable types allowed the system to predict mortality rates, abdomen injury rates, and transfusion requirements. Overall mortality in the study group was 9.1%, unchanged from original Young-Burgess studies 15 years previously (P = 0.3).", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "The Young-Burgess system is useful for predicting transfusion requirements. For the system to predict mortality or nonorthopaedic injuries, fractures must be divided into stable (APC1, LC1) and unstable (APC2, APC3, LC2, LC3, VS, combined mechanism of injury) types. LC1 injuries are very common and not always benign (overall mortality rate, 8.2%).", "meshes": ["Blood Transfusion", "Comorbidity", "Fractures, Compression", "Humans", "Maryland", "Pelvic Bones", "Predictive Value of Tests", "Retrospective Studies", "Survival Rate", "Tomography, X-Ray Computed", "Trauma Centers", "Trauma Severity Indices"], "year": "2010"}
{"id": "pubmedqa_11713724", "dataset": "pubmedqa", "question": "Context: Cancer of the buccal mucosa is an uncommon and aggressive neoplasm of the oral cavity. Less than 2% of patients treated for cancer of the oral cavity at Roswell Park Cancer Institute (RPCI) from 1971 to 1997 had primary buccal cancers. Because the majority of these patients did not undergo any adjuvant treatment, this group provided us with the opportunity to assess the relationship between margin status and local recurrence for both small (T1-T2) and large (T3-T4) tumors treated with surgery alone. The RPCI tumor registry database reported 104 patients who were treated for buccal carcinoma. A retrospective chart review identified 27 patients who met our criteria for a buccal mucosal primary tumor (epicenter of the mass in the buccal mucosa). There were 13 men and 14 women, ranging in age from 34 to 94 years (mean, 75). Data were collected regarding patient demographics, presenting symptoms, stage, treatment received, and outcome. All patients underwent surgical resection of their primary lesion; 21 (75%) had T1 or T2 tumors. The rate of local recurrence was 56% for the group as a whole. Patients with close or positive margins had a 66% local failure rate as compared with 52% when surgical margins were negative (greater than or equal to 5 mm from the resection margin after tissue fixation; P = ns). Among those in whom negative margins were achieved, patients with T1-T2 disease had a 40% local failure rate with surgical resection alone.\n\nQuestion: Cancer of the buccal mucosa: are margins and T-stage accurate predictors of local control?", "question_only": "Cancer of the buccal mucosa: are margins and T-stage accurate predictors of local control?", "context": "Cancer of the buccal mucosa is an uncommon and aggressive neoplasm of the oral cavity. Less than 2% of patients treated for cancer of the oral cavity at Roswell Park Cancer Institute (RPCI) from 1971 to 1997 had primary buccal cancers. Because the majority of these patients did not undergo any adjuvant treatment, this group provided us with the opportunity to assess the relationship between margin status and local recurrence for both small (T1-T2) and large (T3-T4) tumors treated with surgery alone. The RPCI tumor registry database reported 104 patients who were treated for buccal carcinoma. A retrospective chart review identified 27 patients who met our criteria for a buccal mucosal primary tumor (epicenter of the mass in the buccal mucosa). There were 13 men and 14 women, ranging in age from 34 to 94 years (mean, 75). Data were collected regarding patient demographics, presenting symptoms, stage, treatment received, and outcome. All patients underwent surgical resection of their primary lesion; 21 (75%) had T1 or T2 tumors. The rate of local recurrence was 56% for the group as a whole. Patients with close or positive margins had a 66% local failure rate as compared with 52% when surgical margins were negative (greater than or equal to 5 mm from the resection margin after tissue fixation; P = ns). Among those in whom negative margins were achieved, patients with T1-T2 disease had a 40% local failure rate with surgical resection alone.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Local excision of T1 and T2 buccal mucosa cancers with pathologically negative margins had a high rate of local recurrence in our series. Low T-stage and negative margins are not adequate predictors of local control. Even early buccal tumors may benefit from adjuvant therapy to enhance local control.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Biopsy, Needle", "Carcinoma, Squamous Cell", "Cheek", "Disease-Free Survival", "Female", "Humans", "Male", "Middle Aged", "Mouth Mucosa", "Mouth Neoplasms", "Neoplasm Staging", "Predictive Value of Tests", "Probability", "Prognosis", "Registries", "Retrospective Studies", "Sensitivity and Specificity", "Survival Rate"], "year": null}
{"id": "pubmedqa_23347337", "dataset": "pubmedqa", "question": "Context: To provide equality of cancer care to rural patients, Townsville Cancer Centre administers intensive chemotherapy regimens to rural patients with node-positive breast and metastatic colorectal cancers at the same doses as urban patients. Side-effects were usually managed by rural general practitioners locally.AIM: The aim is to determine the safety of this practice by comparing the profile of serious adverse events and dose intensities between urban and rural patients at the Townsville Cancer Centre. A retrospective audit was conducted in patients with metastatic colorectal and node-positive breast cancers during a 24-month period. Fisher's exact test was used for analysis. Rurality was determined as per rural, remote and metropolitan classification. Of the 121 patients included, 70 and 51 patients had breast and colon cancers respectively. The urban versus rural patient split among all patients, breast and colorectal cancer subgroups was 68 versus 53, 43 versus 27 and 25 versus 26 respectively. A total of 421 cycles was given with dose intensity of>95% for breast cancer in both groups (P>0.05). Rate of febrile neutropenia was 9.3% versus 7.4% (P = 0.56). For XELOX, rate of diarrhoea was 20% versus 19% (P = 0.66) and rate of vomiting was 20% versus 11% (P = 0.11). Only two patients were transferred to Townsville for admission. No toxic death occurred in either group.\n\nQuestion: Is intensive chemotherapy safe for rural cancer patients?", "question_only": "Is intensive chemotherapy safe for rural cancer patients?", "context": "To provide equality of cancer care to rural patients, Townsville Cancer Centre administers intensive chemotherapy regimens to rural patients with node-positive breast and metastatic colorectal cancers at the same doses as urban patients. Side-effects were usually managed by rural general practitioners locally.AIM: The aim is to determine the safety of this practice by comparing the profile of serious adverse events and dose intensities between urban and rural patients at the Townsville Cancer Centre. A retrospective audit was conducted in patients with metastatic colorectal and node-positive breast cancers during a 24-month period. Fisher's exact test was used for analysis. Rurality was determined as per rural, remote and metropolitan classification. Of the 121 patients included, 70 and 51 patients had breast and colon cancers respectively. The urban versus rural patient split among all patients, breast and colorectal cancer subgroups was 68 versus 53, 43 versus 27 and 25 versus 26 respectively. A total of 421 cycles was given with dose intensity of>95% for breast cancer in both groups (P>0.05). Rate of febrile neutropenia was 9.3% versus 7.4% (P = 0.56). For XELOX, rate of diarrhoea was 20% versus 19% (P = 0.66) and rate of vomiting was 20% versus 11% (P = 0.11). Only two patients were transferred to Townsville for admission. No toxic death occurred in either group.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "It appears safe to administer intensive chemotherapy regimens at standard doses to rural patients without increased morbidity or mortality. Support for general practitioners through phone or videoconferencing may reduce the safety concerns.", "meshes": ["Adult", "Aged", "Antineoplastic Agents", "Breast Neoplasms", "Colonic Neoplasms", "Diarrhea", "Female", "Humans", "Male", "Middle Aged", "Retrospective Studies", "Rural Population", "Vomiting"], "year": "2013"}
{"id": "pubmedqa_11833948", "dataset": "pubmedqa", "question": "Context: To detemine the relationship between delay in transfer to rehabilitation wards and outcome for patients aged over 75 years with fracture of the proximal femur. An observational study in a district general hospital of all patients admitted to hospital aged over 75 years with fracture of the proximal femur over 3 1/2 years. Outcome data collected included the number of patients discharged back to their usual residence and total hospital length of stay related to age, gender, usual residence and delay in transfer to a rehabilitation ward. 58% of 455 patients were transferred to a rehabilitation ward. For those patients who were transferred to a rehabilitation ward only age predicted discharge to a more dependent residence. The relative risk for discharge to a more dependent residence for people aged over 85 years compared to younger people was 1.47 (95% CI 1.15-1.88). Delay in transfer to rehabilitation was associated with a longer total hospital length of stay of 0.64 (95% CI 0.23-1.05) days per day of delay in transfer.\n\nQuestion: Does a delay in transfer to a rehabilitation unit for older people affect outcome after fracture of the proximal femur?", "question_only": "Does a delay in transfer to a rehabilitation unit for older people affect outcome after fracture of the proximal femur?", "context": "To detemine the relationship between delay in transfer to rehabilitation wards and outcome for patients aged over 75 years with fracture of the proximal femur. An observational study in a district general hospital of all patients admitted to hospital aged over 75 years with fracture of the proximal femur over 3 1/2 years. Outcome data collected included the number of patients discharged back to their usual residence and total hospital length of stay related to age, gender, usual residence and delay in transfer to a rehabilitation ward. 58% of 455 patients were transferred to a rehabilitation ward. For those patients who were transferred to a rehabilitation ward only age predicted discharge to a more dependent residence. The relative risk for discharge to a more dependent residence for people aged over 85 years compared to younger people was 1.47 (95% CI 1.15-1.88). Delay in transfer to rehabilitation was associated with a longer total hospital length of stay of 0.64 (95% CI 0.23-1.05) days per day of delay in transfer.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Delay in transfer to a rehabilitation ward was associated with a disproportionate increase in total hospital length of stay for patients aged over 75 with fracture of the proximal femur.", "meshes": ["Aged", "Aged, 80 and over", "Female", "Femoral Neck Fractures", "Hospital Units", "Humans", "Length of Stay", "Logistic Models", "Male", "Patient Transfer", "Rehabilitation Centers", "Risk Factors", "Time Factors"], "year": "2001"}
{"id": "pubmedqa_10757151", "dataset": "pubmedqa", "question": "Context: Ischemic preconditioning (IP) is initiated through one or several short bouts of ischemia and reperfusion which precede a prolonged ischemia. To test whether a reperfusion must precede the prolonged index ischemia, a series without reperfusion (intraischemic preconditioning: IIP) and a series with gradual onset of ischemia, i.e. ramp ischemia (RI), which is possibly related to the development of hibernation, was compared to conventional IP (CIP). Experiments were performed an 27 blood-perfused rabbit hearts (Langendorff apparatus) that were randomized into one of four series: (1) control (n = 7): 60 min normal flow - 60 min low flow (10%) ischemia - 60 min reperfusion. (2) CIP (n = 7): 4 times 5 min zero flow with 10 min reperfusion each - 60 min low flow (10%) - ischemia 60 min reperfusion. (3) IIP (n = 7): 50 min normal flow - 10 min no flow - 60min low flow (10%) ischemia -4 60min reperfusion. (4) RI (n=6): gradual reduction to 10% flow during 60min - 60min low flow (10%) ischemia - 60min reperfusion. At the end of each protocol, the infarcted area was assessed. The infarct area in control hearts was 6.7+/-1.4% (means+/-SEM) of LV total area, in CIP hearts 2.6+/-0.8%, in IIP hearts 3.1+/-0.5%, and in RI hearts 3.0+/-0.3% (all p<0.05 vs. control). The differences between the three protection protocols were statistically not significant, and no protective protocol reduced post-ischemic myocardial dysfunction.\n\nQuestion: Does ischemic preconditioning require reperfusion before index ischemia?", "question_only": "Does ischemic preconditioning require reperfusion before index ischemia?", "context": "Ischemic preconditioning (IP) is initiated through one or several short bouts of ischemia and reperfusion which precede a prolonged ischemia. To test whether a reperfusion must precede the prolonged index ischemia, a series without reperfusion (intraischemic preconditioning: IIP) and a series with gradual onset of ischemia, i.e. ramp ischemia (RI), which is possibly related to the development of hibernation, was compared to conventional IP (CIP). Experiments were performed an 27 blood-perfused rabbit hearts (Langendorff apparatus) that were randomized into one of four series: (1) control (n = 7): 60 min normal flow - 60 min low flow (10%) ischemia - 60 min reperfusion. (2) CIP (n = 7): 4 times 5 min zero flow with 10 min reperfusion each - 60 min low flow (10%) - ischemia 60 min reperfusion. (3) IIP (n = 7): 50 min normal flow - 10 min no flow - 60min low flow (10%) ischemia -4 60min reperfusion. (4) RI (n=6): gradual reduction to 10% flow during 60min - 60min low flow (10%) ischemia - 60min reperfusion. At the end of each protocol, the infarcted area was assessed. The infarct area in control hearts was 6.7+/-1.4% (means+/-SEM) of LV total area, in CIP hearts 2.6+/-0.8%, in IIP hearts 3.1+/-0.5%, and in RI hearts 3.0+/-0.3% (all p<0.05 vs. control). The differences between the three protection protocols were statistically not significant, and no protective protocol reduced post-ischemic myocardial dysfunction.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The preconditioning effect (infarct size reduction) appears not to depend on intermittent reperfusion. Thus, the protective mechanism of IP develops during the initial ischemia that precedes the index ischemia. Alternatively, low-flow ischemia is effectively a sort of reperfusion.", "meshes": ["Animals", "Disease Models, Animal", "Evaluation Studies as Topic", "Hemodynamics", "Ischemic Preconditioning, Myocardial", "Male", "Myocardial Infarction", "Myocardial Reperfusion", "Rabbits", "Random Allocation"], "year": "2000"}
{"id": "pubmedqa_12913347", "dataset": "pubmedqa", "question": "Context: Being unmarried is a well-known risk factor for poor pregnancy outcome such as preterm delivery and intrauterine growth restriction. The aim of this prospective study was to assess the prevalence and risk of bacterial vaginosis (BV) and selected bacteria isolated from the lower genital tract and to determine the socioeconomic and microbiological characteristics that might be responsible for poor pregnancy outcome observed among unmarried pregnant women. The study population comprised 196 pregnant women attending 10 randomly selected outpatient maternity units in the Lodz region, central Poland. Cervicovaginal samples were obtained between 8 and 16 weeks of gestation. Based on Spiegel's criteria, gram-stained vaginal smears were examined for BV and the BV-associated flora was sought by culture. To evaluate the risk factors, relative risk ratios were calculated using EPI INFO software. Among 196 pregnant women, 40 (20.4%) were unmarried. BV was diagnosed among 55 (28.1%) women studied. In the univariate analysis, unmarried pregnant women were characterized by younger age, primary educational level, poor economic situation and excessive smoking during pregnancy, as compared to married women. The unmarried status was a borderline risk factor for BV (OR = 1.83, 95% CI 0.94-4.9) after adjustment for age, smoking and education. An analysis of the microbiological culture from the lower genital tract revealed that unmarried pregnant women had a higher risk for several types of pathological microflora, as compared to married women. However, this finding was significant only for Mycoplasma hominis. The independent risk factors of M. hominis were the young age of the subject and a low concentration of Lactobacillus spp.\n\nQuestion: Do microbiological factors account for poor pregnancy outcome among unmarried pregnant women in Poland?", "question_only": "Do microbiological factors account for poor pregnancy outcome among unmarried pregnant women in Poland?", "context": "Being unmarried is a well-known risk factor for poor pregnancy outcome such as preterm delivery and intrauterine growth restriction. The aim of this prospective study was to assess the prevalence and risk of bacterial vaginosis (BV) and selected bacteria isolated from the lower genital tract and to determine the socioeconomic and microbiological characteristics that might be responsible for poor pregnancy outcome observed among unmarried pregnant women. The study population comprised 196 pregnant women attending 10 randomly selected outpatient maternity units in the Lodz region, central Poland. Cervicovaginal samples were obtained between 8 and 16 weeks of gestation. Based on Spiegel's criteria, gram-stained vaginal smears were examined for BV and the BV-associated flora was sought by culture. To evaluate the risk factors, relative risk ratios were calculated using EPI INFO software. Among 196 pregnant women, 40 (20.4%) were unmarried. BV was diagnosed among 55 (28.1%) women studied. In the univariate analysis, unmarried pregnant women were characterized by younger age, primary educational level, poor economic situation and excessive smoking during pregnancy, as compared to married women. The unmarried status was a borderline risk factor for BV (OR = 1.83, 95% CI 0.94-4.9) after adjustment for age, smoking and education. An analysis of the microbiological culture from the lower genital tract revealed that unmarried pregnant women had a higher risk for several types of pathological microflora, as compared to married women. However, this finding was significant only for Mycoplasma hominis. The independent risk factors of M. hominis were the young age of the subject and a low concentration of Lactobacillus spp.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The observed socioeconomic, demographic and microbiological differences between unmarried and married women could be responsible for the poor pregnancy outcome among unmarried pregnant women in Poland. Unmarried pregnant women should be covered by comprehensive medical care even before pregnancy. Further studies taking into account the role of psychological stress, patterns of sexual behavior and substance abuse during pregnancy could help identify the factors responsible for adverse pregnancy outcome among unmarried pregnant women.", "meshes": ["Adult", "Female", "Humans", "Lactobacillus", "Mycoplasma Infections", "Mycoplasma hominis", "Poland", "Pregnancy", "Pregnancy Complications, Infectious", "Pregnancy Outcome", "Prevalence", "Prospective Studies", "Random Allocation", "Risk Factors", "Single Person", "Vaginosis, Bacterial"], "year": null}
{"id": "pubmedqa_16809243", "dataset": "pubmedqa", "question": "Context: To investigate if fetal gender (1) affects the risk of having an emergency department (ED) visit for asthma; and (2) is associated with adverse pregnancy outcomes among women who had at least one visit to the ED for asthma during pregnancy. We linked two provincial administrative databases containing records on in-patient deliveries and ED visits. The study sample included women who delivered a live singleton baby between April 2003 and March 2004. Pregnant women who made at least one ED visit for asthma were counted as cases and the rest of the women as control subjects. We performed a multivariable analysis using logistic regression to model the risk of having an ED visit for asthma, with fetal gender being one of the predictors. In addition, a series of multivariable logistic regressions were also constructed separately for cases and controls for the following adverse delivery outcomes: low birth weight baby, preterm delivery, and delivery via Caesarian section. Among 109,173 live singleton deliveries, 530 women had visited ED due to asthma during pregnancy. While having an ED visit for asthma was positively associated with teenage pregnancy, low income, and presence of pregnancy-induced hypertension, it was not associated with fetal gender (OR 1.01, 95% CI 0.85-1.19). Fetal gender was not a significant predictor of adverse pregnancy outcomes among women who had an asthma ED visit during pregnancy.\n\nQuestion: Is fetal gender associated with emergency department visits for asthma during pregnancy?", "question_only": "Is fetal gender associated with emergency department visits for asthma during pregnancy?", "context": "To investigate if fetal gender (1) affects the risk of having an emergency department (ED) visit for asthma; and (2) is associated with adverse pregnancy outcomes among women who had at least one visit to the ED for asthma during pregnancy. We linked two provincial administrative databases containing records on in-patient deliveries and ED visits. The study sample included women who delivered a live singleton baby between April 2003 and March 2004. Pregnant women who made at least one ED visit for asthma were counted as cases and the rest of the women as control subjects. We performed a multivariable analysis using logistic regression to model the risk of having an ED visit for asthma, with fetal gender being one of the predictors. In addition, a series of multivariable logistic regressions were also constructed separately for cases and controls for the following adverse delivery outcomes: low birth weight baby, preterm delivery, and delivery via Caesarian section. Among 109,173 live singleton deliveries, 530 women had visited ED due to asthma during pregnancy. While having an ED visit for asthma was positively associated with teenage pregnancy, low income, and presence of pregnancy-induced hypertension, it was not associated with fetal gender (OR 1.01, 95% CI 0.85-1.19). Fetal gender was not a significant predictor of adverse pregnancy outcomes among women who had an asthma ED visit during pregnancy.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Fetal gender does not affect the risk of having an ED visit for asthma during pregnancy, and it is not associated with adverse pregnancy outcomes among women who had an asthma-related ED during pregnancy.", "meshes": ["Adolescent", "Adult", "Ambulatory Care", "Asthma", "Case-Control Studies", "Chi-Square Distribution", "Emergency Service, Hospital", "Female", "Gestational Age", "Humans", "Incidence", "Male", "Maternal Age", "Parity", "Pregnancy", "Pregnancy Complications", "Pregnancy Outcome", "Pregnancy, High-Risk", "Prenatal Care", "Probability", "Registries", "Retrospective Studies", "Risk Assessment", "Sex Determination Analysis", "Sex Distribution", "Sex Factors"], "year": "2006"}
{"id": "pubmedqa_25747932", "dataset": "pubmedqa", "question": "Context: This paper uses a life-course approach to explore whether the timing and/or duration of urban (vs rural) exposure was associated with risk factors for NCDs. A cross-sectional survey was conducted among health care workers in two hospitals in Thailand. Two measures of urbanicity were considered: early-life urban exposure and the proportion of urban life years. We explored four behavioral NCD risk factors, two physiological risk factors and four biological risk factors. Both measures of urbanicity were each independently associated with increases in all behavioral and physiological risk factors. For some biological risk factors, people spending their early life in an urban area may be more susceptible to the effect of increasing proportion of urban life years than those growing up in rural areas.\n\nQuestion: Living in an urban environment and non-communicable disease risk in Thailand: Does timing matter?", "question_only": "Living in an urban environment and non-communicable disease risk in Thailand: Does timing matter?", "context": "This paper uses a life-course approach to explore whether the timing and/or duration of urban (vs rural) exposure was associated with risk factors for NCDs. A cross-sectional survey was conducted among health care workers in two hospitals in Thailand. Two measures of urbanicity were considered: early-life urban exposure and the proportion of urban life years. We explored four behavioral NCD risk factors, two physiological risk factors and four biological risk factors. Both measures of urbanicity were each independently associated with increases in all behavioral and physiological risk factors. For some biological risk factors, people spending their early life in an urban area may be more susceptible to the effect of increasing proportion of urban life years than those growing up in rural areas.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Urbanicity was associated with increases in behavioral and physiological risk factors. However, these associations may not translate directly into increases in biological risk factors. It is likely that these biological risk factors were results of a complex interaction between both long term accumulation of exposure and early life exposures.", "meshes": ["Adult", "Child, Preschool", "Chronic Disease", "Cross-Sectional Studies", "Humans", "Middle Aged", "Prevalence", "Risk Factors", "Thailand", "Time Factors", "Urban Health"], "year": "2015"}
{"id": "pubmedqa_10158597", "dataset": "pubmedqa", "question": "Context: To evaluate the effectiveness of the role of a discharge coordinator whose sole responsibility was to plan and coordinate the discharge of patients from medical wards. An intervention study in which the quality of discharge planning was assessed before and after the introduction of a discharge coordinator. Patients were interviewed on the ward before discharge and seven to 10 days after being discharged home. The three medical wards at the Homerton Hospital in Hackney, East London. 600 randomly sampled adult patients admitted to the medical wards of the study hospital, who were resident in the district (but not in institutions), were under the care of physicians (excluding psychiatry), and were discharged home from one of the medical wards. The sampling was conducted in three study phases, over 18 months. Phase I comprised base line data collection; in phase II data were collected after the introduction of the district discharge planning policy and a discharge form (checklist) for all patients; in phase III data were collected after the introduction of the discharge coordinator. The quality and out come of discharge planning. Readmission rates, duration of stay, appropriateness of days of care, patients' health and satisfaction, problems after discharge, and receipt of services. The discharge coordinator resulted in an improved discharge planning process, and there was a reduction in problems experienced by patients after discharge, and in perceived need for medical and healthcare services. There was no evidence that the discharge coordinator resulted in a more timely or effective provision of community services after discharge, or that the appropriateness or efficiency of bed use was improved.\n\nQuestion: Does a dedicated discharge coordinator improve the quality of hospital discharge?", "question_only": "Does a dedicated discharge coordinator improve the quality of hospital discharge?", "context": "To evaluate the effectiveness of the role of a discharge coordinator whose sole responsibility was to plan and coordinate the discharge of patients from medical wards. An intervention study in which the quality of discharge planning was assessed before and after the introduction of a discharge coordinator. Patients were interviewed on the ward before discharge and seven to 10 days after being discharged home. The three medical wards at the Homerton Hospital in Hackney, East London. 600 randomly sampled adult patients admitted to the medical wards of the study hospital, who were resident in the district (but not in institutions), were under the care of physicians (excluding psychiatry), and were discharged home from one of the medical wards. The sampling was conducted in three study phases, over 18 months. Phase I comprised base line data collection; in phase II data were collected after the introduction of the district discharge planning policy and a discharge form (checklist) for all patients; in phase III data were collected after the introduction of the discharge coordinator. The quality and out come of discharge planning. Readmission rates, duration of stay, appropriateness of days of care, patients' health and satisfaction, problems after discharge, and receipt of services. The discharge coordinator resulted in an improved discharge planning process, and there was a reduction in problems experienced by patients after discharge, and in perceived need for medical and healthcare services. There was no evidence that the discharge coordinator resulted in a more timely or effective provision of community services after discharge, or that the appropriateness or efficiency of bed use was improved.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The introduction of a discharge coordinator improved the quality of discharge planning, but at additional cost.", "meshes": ["Activities of Daily Living", "Health Status Indicators", "Humans", "Length of Stay", "London", "Patient Discharge", "Patient Readmission", "Patient Satisfaction", "Program Evaluation", "Quality Assurance, Health Care", "Social Work", "State Medicine"], "year": "1996"}
{"id": "pubmedqa_25501465", "dataset": "pubmedqa", "question": "Context: There is heterogeneity in how pediatric voiding cystourethrography (VCUG) is performed. Some institutions, including our own, obtain a radiographic scout image prior to contrast agent instillation. To demonstrate that the radiographic scout image does not augment VCUG interpretation or contribute management-changing information but nonetheless carries a non-negligible effective dose. We evaluated 181 children who underwent VCUG in 2012, with an age breakdown of less than 1 year (56 children), 1-5 years (66 children), 6-10 years (43 children) and 11-18 years (16 children), with a mean age of 4.0 years. We investigated patient demographics, clinical indication for the examination, scout image findings and estimated effective radiation dose, as well as overall exam findings and impression. No clinically significant or management-changing findings were present on scout images, and no radiopaque urinary tract calculi or concerning incidental finding was identified. Scout image estimated effective radiation dose averaged 0.09 mSv in children younger than 1 y, 0.09 mSv in children age 1-5, 0.13 mSv in children age 6-10 and 0.18 mSv in children age 11-18. Total fluoroscopy time per examination averaged 36.7 s (range 34.8-39.6 s for all age group averages). Evaluation of known or suspected vesicoureteral reflux (VUR) and urinary tract infection (UTI) were the most common clinical indications, stated in 40.9% and 37.0% of exams, respectively.\n\nQuestion: Evaluation of pediatric VCUG at an academic children's hospital: is the radiographic scout image necessary?", "question_only": "Evaluation of pediatric VCUG at an academic children's hospital: is the radiographic scout image necessary?", "context": "There is heterogeneity in how pediatric voiding cystourethrography (VCUG) is performed. Some institutions, including our own, obtain a radiographic scout image prior to contrast agent instillation. To demonstrate that the radiographic scout image does not augment VCUG interpretation or contribute management-changing information but nonetheless carries a non-negligible effective dose. We evaluated 181 children who underwent VCUG in 2012, with an age breakdown of less than 1 year (56 children), 1-5 years (66 children), 6-10 years (43 children) and 11-18 years (16 children), with a mean age of 4.0 years. We investigated patient demographics, clinical indication for the examination, scout image findings and estimated effective radiation dose, as well as overall exam findings and impression. No clinically significant or management-changing findings were present on scout images, and no radiopaque urinary tract calculi or concerning incidental finding was identified. Scout image estimated effective radiation dose averaged 0.09 mSv in children younger than 1 y, 0.09 mSv in children age 1-5, 0.13 mSv in children age 6-10 and 0.18 mSv in children age 11-18. Total fluoroscopy time per examination averaged 36.7 s (range 34.8-39.6 s for all age group averages). Evaluation of known or suspected vesicoureteral reflux (VUR) and urinary tract infection (UTI) were the most common clinical indications, stated in 40.9% and 37.0% of exams, respectively.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Although the estimated effective dose is low for VCUG radiographic scout images, this step did not augment VCUG interpretation or contribute management-changing information. This step should be omitted or substituted to further reduce dose in pediatric VCUG.", "meshes": ["Academic Medical Centers", "Adolescent", "Child", "Child, Preschool", "Female", "Fluoroscopy", "Hospitals, Pediatric", "Humans", "Infant", "Male", "Retrospective Studies", "Vesico-Ureteral Reflux"], "year": "2015"}
{"id": "pubmedqa_8985020", "dataset": "pubmedqa", "question": "Context: To assess the outcomes of patients with nasopharyngeal carcinoma (NPC) whose treatment was determined by computerized tomography (CT) and/or magnetic resonance imaging staging and to analyze the impact of induction chemotherapy and accelerated fractionated radiotherapy. The analysis is based on 122 of 143 previously untreated patients with NPC treated with radiation therapy at The University of Texas M. D. Anderson Cancer Center between 1983 and 1992. Excluded were 4 patients treated with palliative intent, 4 children, 12 patients not staged with CT, and 1 patient who died of a cerebrovascular accident prior to completion of treatment. The stage distribution was as follows: AJCC Stage I-2, Stage II-7, Stage III-12, Stage IV-101; Tl-15, T2-33, T3-22, T4-52; N0-32, N1-10, N2-47, N3-32, Nx-1. Fifty-nine (48%) patients had squamous cell carcinoma; 63 (52%) had lymphoepitheliomas, undifferentiated NPC or poorly differentiated carcinoma, NOS (UNPC). Sixty-seven patients (65 with Stage IV disease) received induction chemotherapy. Fifty-eight patients (24 of whom had induction chemotherapy) were treated with the concomitant boost fractionation schedule. The median follow-up for surviving patients was 57 months. The overall actuarial 2- and 5-year survival rates were 78 and 68%, respectively. Forty-nine patients (40%) had disease recurrence. Thirty-three (27%) had local regional failures; 19 at the primary site only, 8 in the neck and 6 in both. Local failure occurred in 31% of patients staged T4 compared to 13% of T1-T3 (p = 0.007). Sixteen patients failed at distant sites alone. Among Stage IV patients the 5-year actuarial rates for patients who did and did not receive induction chemotherapy were as follows: overall survival: 68 vs. 56% (p = 0.02), freedom from relapse: 64 vs. 37% (p = 0.01), and local control: 86 vs. 56% (p = 0.009). The actuarial 5-year distant failure rate in patients with UNPC who were treated with induction chemotherapy and controlled in the primary and neck was 13%. In patients who did not receive chemotherapy, the actuarial 5-year local control rates for patients treated with concomitant boost or conventional fractionation were 66 and 67%, respectively.\n\nQuestion: Does induction chemotherapy have a role in the management of nasopharyngeal carcinoma?", "question_only": "Does induction chemotherapy have a role in the management of nasopharyngeal carcinoma?", "context": "To assess the outcomes of patients with nasopharyngeal carcinoma (NPC) whose treatment was determined by computerized tomography (CT) and/or magnetic resonance imaging staging and to analyze the impact of induction chemotherapy and accelerated fractionated radiotherapy. The analysis is based on 122 of 143 previously untreated patients with NPC treated with radiation therapy at The University of Texas M. D. Anderson Cancer Center between 1983 and 1992. Excluded were 4 patients treated with palliative intent, 4 children, 12 patients not staged with CT, and 1 patient who died of a cerebrovascular accident prior to completion of treatment. The stage distribution was as follows: AJCC Stage I-2, Stage II-7, Stage III-12, Stage IV-101; Tl-15, T2-33, T3-22, T4-52; N0-32, N1-10, N2-47, N3-32, Nx-1. Fifty-nine (48%) patients had squamous cell carcinoma; 63 (52%) had lymphoepitheliomas, undifferentiated NPC or poorly differentiated carcinoma, NOS (UNPC). Sixty-seven patients (65 with Stage IV disease) received induction chemotherapy. Fifty-eight patients (24 of whom had induction chemotherapy) were treated with the concomitant boost fractionation schedule. The median follow-up for surviving patients was 57 months. The overall actuarial 2- and 5-year survival rates were 78 and 68%, respectively. Forty-nine patients (40%) had disease recurrence. Thirty-three (27%) had local regional failures; 19 at the primary site only, 8 in the neck and 6 in both. Local failure occurred in 31% of patients staged T4 compared to 13% of T1-T3 (p = 0.007). Sixteen patients failed at distant sites alone. Among Stage IV patients the 5-year actuarial rates for patients who did and did not receive induction chemotherapy were as follows: overall survival: 68 vs. 56% (p = 0.02), freedom from relapse: 64 vs. 37% (p = 0.01), and local control: 86 vs. 56% (p = 0.009). The actuarial 5-year distant failure rate in patients with UNPC who were treated with induction chemotherapy and controlled in the primary and neck was 13%. In patients who did not receive chemotherapy, the actuarial 5-year local control rates for patients treated with concomitant boost or conventional fractionation were 66 and 67%, respectively.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "While not providing conclusive evidence, this single institution experience suggests that neoadjuvant chemotherapy for Stage IV NPC patients improves both survival and disease control. Recurrence within the irradiated volume was the most prevalent mode of failure and future studies will evaluate regimens to enhance local regional control.", "meshes": ["Adolescent", "Adult", "Aged", "Antineoplastic Combined Chemotherapy Protocols", "Combined Modality Therapy", "Humans", "Middle Aged", "Multivariate Analysis", "Nasopharyngeal Neoplasms", "Neoplasm Staging", "Radiotherapy Dosage", "Survival Rate", "Tomography, X-Ray Computed"], "year": "1996"}
{"id": "pubmedqa_11035130", "dataset": "pubmedqa", "question": "Context: It is postulated that some aspects of methotrexate toxicity may be related to its action as an anti-folate. Folic acid (FA) is often given as an adjunct to methotrexate therapy, but there is no conclusive proof that it decreases the toxicity of methotrexate and there is a theoretical risk that it may decrease the efficacy of methotrexate. To look at the effect of stopping FA supplementation in UK rheumatoid arthritis (RA) patients established on methotrexate<20 mg weekly and FA 5 mg daily, to report all toxicity (including absolute changes in haematological and liver enzyme indices) and to report changes in the efficacy of methotrexate. In a prospective, randomized, double-blind, placebo-controlled study, 75 patients who were established on methotrexate<20 mg weekly and FA 5 mg daily were asked to stop their FA and were randomized to one of two groups: placebo or FA 5 mg daily. Patients were evaluated for treatment toxicity and efficacy before entry and then at intervals of 3 months for 1 yr. Overall, 25 (33%) patients concluded the study early, eight (21%) in the group remaining on FA and 17 (46%) in the placebo group (P = 0.02). Two patients in the placebo group discontinued because of neutropenia. At 9 months there was an increased incidence of nausea in the placebo group (45 vs. 7%, P = 0.001). The placebo group had significantly lower disease activity on a few of the variables measured, but these were probably not of clinical significance.\n\nQuestion: Do patients with rheumatoid arthritis established on methotrexate and folic acid 5 mg daily need to continue folic acid supplements long term?", "question_only": "Do patients with rheumatoid arthritis established on methotrexate and folic acid 5 mg daily need to continue folic acid supplements long term?", "context": "It is postulated that some aspects of methotrexate toxicity may be related to its action as an anti-folate. Folic acid (FA) is often given as an adjunct to methotrexate therapy, but there is no conclusive proof that it decreases the toxicity of methotrexate and there is a theoretical risk that it may decrease the efficacy of methotrexate. To look at the effect of stopping FA supplementation in UK rheumatoid arthritis (RA) patients established on methotrexate<20 mg weekly and FA 5 mg daily, to report all toxicity (including absolute changes in haematological and liver enzyme indices) and to report changes in the efficacy of methotrexate. In a prospective, randomized, double-blind, placebo-controlled study, 75 patients who were established on methotrexate<20 mg weekly and FA 5 mg daily were asked to stop their FA and were randomized to one of two groups: placebo or FA 5 mg daily. Patients were evaluated for treatment toxicity and efficacy before entry and then at intervals of 3 months for 1 yr. Overall, 25 (33%) patients concluded the study early, eight (21%) in the group remaining on FA and 17 (46%) in the placebo group (P = 0.02). Two patients in the placebo group discontinued because of neutropenia. At 9 months there was an increased incidence of nausea in the placebo group (45 vs. 7%, P = 0.001). The placebo group had significantly lower disease activity on a few of the variables measured, but these were probably not of clinical significance.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "It is important to continue FA supplementation over the long term in patients on methotrexate and FA in order to prevent them discontinuing treatment because of mouth ulcers or nausea and vomiting. Our data suggest that FA supplementation is also helpful in preventing neutropenia, with very little loss of efficacy of methotrexate.", "meshes": ["Aged", "Antirheumatic Agents", "Arthritis, Rheumatoid", "Double-Blind Method", "Drug Administration Schedule", "Female", "Folic Acid", "Humans", "Male", "Methotrexate", "Middle Aged", "Prospective Studies", "Treatment Outcome"], "year": "2000"}
{"id": "pubmedqa_19520213", "dataset": "pubmedqa", "question": "Context: A list of telephone numbers of UK hospitals with a radiology department was obtained from the Royal College of Radiologists. One hundred hospitals were then randomly selected for inclusion in the survey. An 18-item questionnaire was successfully administered to consultant radiologists from 84 departments. Sixty-one percent of departments had a named radiologist to report their skeletal surveys, 16% assigned surveys to a random radiologist, and 23% referred them elsewhere. Only 52% of departments had a dedicated paediatric radiologist, thus in a significant proportion of departments (25%) initial reports on skeletal surveys for physical abuse were provided by non-paediatric radiologists. Fifteen percent did not have ready access to a paediatric radiology opinion. Sixty-one percent thought that the service could be improved. Expert evidence was provided by 5% of respondents. Seventy-three percent would never consider providing expert evidence, even if given adequate radiology and/or legal training.\n\nQuestion: Are UK radiologists satisfied with the training and support received in suspected child abuse?", "question_only": "Are UK radiologists satisfied with the training and support received in suspected child abuse?", "context": "A list of telephone numbers of UK hospitals with a radiology department was obtained from the Royal College of Radiologists. One hundred hospitals were then randomly selected for inclusion in the survey. An 18-item questionnaire was successfully administered to consultant radiologists from 84 departments. Sixty-one percent of departments had a named radiologist to report their skeletal surveys, 16% assigned surveys to a random radiologist, and 23% referred them elsewhere. Only 52% of departments had a dedicated paediatric radiologist, thus in a significant proportion of departments (25%) initial reports on skeletal surveys for physical abuse were provided by non-paediatric radiologists. Fifteen percent did not have ready access to a paediatric radiology opinion. Sixty-one percent thought that the service could be improved. Expert evidence was provided by 5% of respondents. Seventy-three percent would never consider providing expert evidence, even if given adequate radiology and/or legal training.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The survey shows significant dissatisfaction amongst consultant radiologists with the current service, confirms a low number of paediatric radiologists taking on this work, and suggests the potential to increase numbers of radiology child abuse experts by 27% if given improved training and support. Appropriate service and education strategies should be implemented.", "meshes": ["Attitude of Health Personnel", "Bone and Bones", "Child", "Child Abuse", "Clinical Competence", "Education, Medical, Continuing", "Humans", "Medical Staff, Hospital", "Radiography", "Radiology", "United Kingdom"], "year": "2009"}
{"id": "pubmedqa_21865668", "dataset": "pubmedqa", "question": "Context: Most older drivers continue to drive as they age. To maintain safe and independent transport, mobility is important for all individuals, but especially for older drivers. The objective of this study was to investigate whether automatic transmission, compared with manual transmission, may improve the driving behavior of older drivers. In total, 31 older drivers (mean age 75.2 years) and 32 younger drivers - used as a control group (mean age 39.2 years) - were assessed twice on the same fixed route; once in a car with manual transmission and once in a car with automatic transmission. The cars were otherwise identical. The driving behavior was assessed with the Ryd On-Road Assessment driving protocol. Time to completion of left turns (right-hand side driving) and the impact of a distraction task were measured. The older group had more driving errors than the younger group, in both the manual and the automatic transmission car. However, and contrary to the younger drivers, automatic transmission improved the older participants' driving behavior as demonstrated by safer speed adjustment in urban areas, greater maneuvering skills, safer lane position and driving in accordance with the speed regulations.\n\nQuestion: Does automatic transmission improve driving behavior in older drivers?", "question_only": "Does automatic transmission improve driving behavior in older drivers?", "context": "Most older drivers continue to drive as they age. To maintain safe and independent transport, mobility is important for all individuals, but especially for older drivers. The objective of this study was to investigate whether automatic transmission, compared with manual transmission, may improve the driving behavior of older drivers. In total, 31 older drivers (mean age 75.2 years) and 32 younger drivers - used as a control group (mean age 39.2 years) - were assessed twice on the same fixed route; once in a car with manual transmission and once in a car with automatic transmission. The cars were otherwise identical. The driving behavior was assessed with the Ryd On-Road Assessment driving protocol. Time to completion of left turns (right-hand side driving) and the impact of a distraction task were measured. The older group had more driving errors than the younger group, in both the manual and the automatic transmission car. However, and contrary to the younger drivers, automatic transmission improved the older participants' driving behavior as demonstrated by safer speed adjustment in urban areas, greater maneuvering skills, safer lane position and driving in accordance with the speed regulations.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Switching to automatic transmission may be recommended for older drivers as a means to maintain safe driving and thereby the quality of their transport mobility.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Aging", "Automobile Driving", "Automobiles", "Humans", "Middle Aged", "Motor Skills", "Task Performance and Analysis"], "year": "2012"}
{"id": "pubmedqa_27615402", "dataset": "pubmedqa", "question": "Context: Parental drinking has been shown to be associated with offspring drinking. However, the relationship appears to be more complex than often assumed and few studies have tracked it over longer time periods. To explore the long-term (10-year) transmission of familial drinking during adolescence to offspring drinking patterns in young adulthood. Swedish longitudinal study, assessing the relationship between familial drinking in 2000 and offspring drinking in 2010 using simultaneous quantile regression analysis (n=744).DATA: Data on familial drinking was gathered from the Swedish level-of-living surveys (LNU) and from partner LNU in 2000 while data on offspring drinking in young adulthood was gathered from LNU 2010. Drinking among offspring, parents and potential stepparents was measured through identical quantity-frequency indices referring to the past 12 months in 2010 and 2000 respectively. Young adults whose families were abstainers in 2000 drank substantially less across quintiles in 2010 than offspring of non-abstaining families. The difference, however, was not statistically significant between quintiles of the conditional distribution. Actual drinking levels in drinking families were not at all or weakly associated with drinking in offspring. Supplementary analyses confirmed these patterns.\n\nQuestion: Does the familial transmission of drinking patterns persist into young adulthood?", "question_only": "Does the familial transmission of drinking patterns persist into young adulthood?", "context": "Parental drinking has been shown to be associated with offspring drinking. However, the relationship appears to be more complex than often assumed and few studies have tracked it over longer time periods. To explore the long-term (10-year) transmission of familial drinking during adolescence to offspring drinking patterns in young adulthood. Swedish longitudinal study, assessing the relationship between familial drinking in 2000 and offspring drinking in 2010 using simultaneous quantile regression analysis (n=744).DATA: Data on familial drinking was gathered from the Swedish level-of-living surveys (LNU) and from partner LNU in 2000 while data on offspring drinking in young adulthood was gathered from LNU 2010. Drinking among offspring, parents and potential stepparents was measured through identical quantity-frequency indices referring to the past 12 months in 2010 and 2000 respectively. Young adults whose families were abstainers in 2000 drank substantially less across quintiles in 2010 than offspring of non-abstaining families. The difference, however, was not statistically significant between quintiles of the conditional distribution. Actual drinking levels in drinking families were not at all or weakly associated with drinking in offspring. Supplementary analyses confirmed these patterns.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "The association between familial drinking and offspring drinking in young adulthood exhibits clear non-linear trends. Changes in the lower part of the familial drinking distribution are strongly related to drinking in young adults, but the actual levels of drinking in drinking families appear less important in shaping the drinking patterns of the offspring in young adulthood.", "meshes": ["Adolescent", "Adult", "Alcohol Drinking", "Family", "Female", "Follow-Up Studies", "Humans", "Longitudinal Studies", "Male", "Parent-Child Relations", "Parents", "Surveys and Questionnaires", "Sweden", "Young Adult"], "year": "2016"}
{"id": "pubmedqa_19593710", "dataset": "pubmedqa", "question": "Context: ESC (Electronic Stability Control) is a crash avoidance technology that reduces the likelihood of collisions involving loss of control. Although past and emerging research indicates that ESC is effective in reducing collision rates and saving lives, and its inclusion in all vehicle platforms is encouraged, drivers may demonstrate behavioral adaptation or an overreliance on ESC that could offset or reduce its overall effectiveness. The main objective of the present study was to determine whether behavioral adaptation to ESC is likely to occur upon the widespread introduction of ESC into the Canadian vehicle fleet. Secondary objectives were to confirm the results of a previous ESC public survey and to generate a baseline measure for the future assessment of planned and ongoing ESC promotional activities in Canada. Two separate telephone surveys evaluated drivers' perceptions and awareness of ESC. The first surveyed 500 randomly selected owners/drivers of passenger vehicles. The second surveyed 1017 owners/drivers of 2006-2008 ESC-equipped passenger vehicles from the provinces of Quebec and British Columbia, Canada. Though ESC drivers were much more likely than drivers of other vehicles to be aware of ESC (77% vs. 39%) and that their own vehicle was equipped with it (63% vs. 8%), 23 percent had never heard of it. Ninety percent of drivers who knew that their vehicle was equipped with ESC believed that ESC had made it safer to drive and reported being confident that ESC would work in an emergency. Twenty-three percent of ESC owners who knew their vehicle had ESC reported noticing long-lasting changes in their driving behavior since they began driving the vehicle.\n\nQuestion: Could ESC (Electronic Stability Control) change the way we drive?", "question_only": "Could ESC (Electronic Stability Control) change the way we drive?", "context": "ESC (Electronic Stability Control) is a crash avoidance technology that reduces the likelihood of collisions involving loss of control. Although past and emerging research indicates that ESC is effective in reducing collision rates and saving lives, and its inclusion in all vehicle platforms is encouraged, drivers may demonstrate behavioral adaptation or an overreliance on ESC that could offset or reduce its overall effectiveness. The main objective of the present study was to determine whether behavioral adaptation to ESC is likely to occur upon the widespread introduction of ESC into the Canadian vehicle fleet. Secondary objectives were to confirm the results of a previous ESC public survey and to generate a baseline measure for the future assessment of planned and ongoing ESC promotional activities in Canada. Two separate telephone surveys evaluated drivers' perceptions and awareness of ESC. The first surveyed 500 randomly selected owners/drivers of passenger vehicles. The second surveyed 1017 owners/drivers of 2006-2008 ESC-equipped passenger vehicles from the provinces of Quebec and British Columbia, Canada. Though ESC drivers were much more likely than drivers of other vehicles to be aware of ESC (77% vs. 39%) and that their own vehicle was equipped with it (63% vs. 8%), 23 percent had never heard of it. Ninety percent of drivers who knew that their vehicle was equipped with ESC believed that ESC had made it safer to drive and reported being confident that ESC would work in an emergency. Twenty-three percent of ESC owners who knew their vehicle had ESC reported noticing long-lasting changes in their driving behavior since they began driving the vehicle.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Collectively, results suggest that behavioral adaptation to ESC is likely in certain drivers; however, its proven effectiveness in reducing the likelihood of being involved in a serious crash probably outweighs any potential increases in unsafe driving. To fully benefit from ESC, vehicle manufacturers are encouraged to market ESC-equipped vehicles in a realistic, safe manner. Driver training and safety organizations are also encouraged to provide balanced educational information about ESC to their members.", "meshes": ["Accidents, Traffic", "Adolescent", "Age Factors", "Automobile Driving", "Automobiles", "Awareness", "Behavior", "Canada", "Data Collection", "Educational Status", "Female", "Humans", "Interviews as Topic", "Logistic Models", "Male", "Protective Devices", "Public Opinion", "Risk-Taking", "Sex Factors"], "year": "2009"}
{"id": "pubmedqa_24747511", "dataset": "pubmedqa", "question": "Context: Interference from irrelevant negative material might be a key mechanism underlying intrusive ruminative thoughts in depression. Considering commonalities between depression and social anxiety and the presence of similar intrusive thoughts in social anxiety, the current study was designed to assess whether interference from irrelevant material in working memory is specific to depression or is also present in social anxiety disorder. To examine the effects of irrelevant emotional material on working memory performance, participants memorized two lists of words on each trial and were subsequently instructed to ignore one of the lists. Participants were then asked to indicate whether a probe word belonged to the relevant list or not. Compared to control and social anxiety groups, the depression groups (both pure and comorbid with social anxiety disorder) exhibited greater difficulties removing irrelevant emotional material from working memory (i.e., greater intrusion effects). Greater intrusion effects were also associated with increased rumination. Although we included three clinical groups (depression, social anxiety, and the comorbid groups), the results are based on a relatively small number of participants.\n\nQuestion: Updating emotional content in working memory: a depression-specific deficit?", "question_only": "Updating emotional content in working memory: a depression-specific deficit?", "context": "Interference from irrelevant negative material might be a key mechanism underlying intrusive ruminative thoughts in depression. Considering commonalities between depression and social anxiety and the presence of similar intrusive thoughts in social anxiety, the current study was designed to assess whether interference from irrelevant material in working memory is specific to depression or is also present in social anxiety disorder. To examine the effects of irrelevant emotional material on working memory performance, participants memorized two lists of words on each trial and were subsequently instructed to ignore one of the lists. Participants were then asked to indicate whether a probe word belonged to the relevant list or not. Compared to control and social anxiety groups, the depression groups (both pure and comorbid with social anxiety disorder) exhibited greater difficulties removing irrelevant emotional material from working memory (i.e., greater intrusion effects). Greater intrusion effects were also associated with increased rumination. Although we included three clinical groups (depression, social anxiety, and the comorbid groups), the results are based on a relatively small number of participants.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The results indicate that difficulties removing irrelevant material from working memory might be unique to depression, and the ability to inhibit irrelevant information is relatively preserved in social anxiety disorder.", "meshes": ["Adult", "Depressive Disorder, Major", "Emotions", "Female", "Humans", "Male", "Memory, Short-Term", "Middle Aged", "Neuropsychological Tests", "Reaction Time"], "year": "2014"}
{"id": "pubmedqa_17224424", "dataset": "pubmedqa", "question": "Context: The aim of the present study was to assess the effects of exercise training on heart rate, QT interval, and on the relation between ventricular repolarization and heart rate in men and women. A 24 h Holter recording was obtained in 80 healthy subjects (40 males) who differed for the degree of physical activity. Trained individuals showed a lower heart rate and a higher heart rate variability than sedentary subjects, independent of the gender difference in basal heart rate. Mean 24 h QTc was similar in trained and non-trained men, while a significant difference was observed between trained and non-trained women. Exercise training reduced the QT/RR slope in both genders. This effect on the QT/RR relation was more marked in women; in fact, the gender difference in the ventricular repolarization duration at low heart rate observed in sedentary subjects was no longer present among trained individuals.\n\nQuestion: Effects of exercise training on heart rate and QT interval in healthy young individuals: are there gender differences?", "question_only": "Effects of exercise training on heart rate and QT interval in healthy young individuals: are there gender differences?", "context": "The aim of the present study was to assess the effects of exercise training on heart rate, QT interval, and on the relation between ventricular repolarization and heart rate in men and women. A 24 h Holter recording was obtained in 80 healthy subjects (40 males) who differed for the degree of physical activity. Trained individuals showed a lower heart rate and a higher heart rate variability than sedentary subjects, independent of the gender difference in basal heart rate. Mean 24 h QTc was similar in trained and non-trained men, while a significant difference was observed between trained and non-trained women. Exercise training reduced the QT/RR slope in both genders. This effect on the QT/RR relation was more marked in women; in fact, the gender difference in the ventricular repolarization duration at low heart rate observed in sedentary subjects was no longer present among trained individuals.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The results of this study suggest that the cardiovascular response to exercise training may be different in men and women. Women may benefit more from interventions aimed to increase physical activity as a tool for prevention of cardiovascular morbidity and mortality.", "meshes": ["Electrocardiography", "Exercise", "Female", "Heart Rate", "Humans", "Male", "Rest", "Sex Characteristics", "Ventricular Function"], "year": "2007"}
{"id": "pubmedqa_22497340", "dataset": "pubmedqa", "question": "Context: To clarify whether horizontal canal ocular reflex is influenced by otolith organs input. The subjects were seven healthy humans. The right ear was stimulated using ice-water. Each subject was kept in a left-ear-down position for 20 s and then repositioned to a prone position, a right-ear-down position and a supine position with 20 s intervals. Nystagmus was analysed using three-dimensional video-oculography. Eye movements in the supine position and the prone position were not in a symmetric fashion. Nystagmus in the left-ear-down position and the right-ear-down position were not symmetric either. These phenomena indicate that the axis of the eyeball rotation was affected by the shift of the direction of gravity exerted on the head.\n\nQuestion: Is horizontal semicircular canal ocular reflex influenced by otolith organs input?", "question_only": "Is horizontal semicircular canal ocular reflex influenced by otolith organs input?", "context": "To clarify whether horizontal canal ocular reflex is influenced by otolith organs input. The subjects were seven healthy humans. The right ear was stimulated using ice-water. Each subject was kept in a left-ear-down position for 20 s and then repositioned to a prone position, a right-ear-down position and a supine position with 20 s intervals. Nystagmus was analysed using three-dimensional video-oculography. Eye movements in the supine position and the prone position were not in a symmetric fashion. Nystagmus in the left-ear-down position and the right-ear-down position were not symmetric either. These phenomena indicate that the axis of the eyeball rotation was affected by the shift of the direction of gravity exerted on the head.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Otolith organs input influences the axis of horizontal semicircular canal ocular reflex; therefore, the plane of compensatory eye movements induced by the horizontal canal stimulation is not always parallel to the canal.", "meshes": ["Adult", "Head Movements", "Humans", "Male", "Nystagmus, Physiologic", "Otolithic Membrane", "Physical Stimulation", "Prone Position", "Reference Values", "Reflex, Vestibulo-Ocular", "Semicircular Canals", "Supine Position", "Young Adult"], "year": "2012"}
{"id": "pubmedqa_15488260", "dataset": "pubmedqa", "question": "Context: Rates of relapse and predictive relapse factors were studied over more than 4 years in a sample of Spanish outpatients with DSM-III-R criteria for unipolar major depressive episode. A final sample of 139 outpatient was followed monthly in a naturalistic study. The Structured Clinical Interview for DSM-III-R was used. Phases of evolution were recorded using the Hamilton Depression Rating Scale, applying the Frank criteria. Survival analysis, Kaplan-Meier product limit and proportional hazards models were used. A higher rate of relapses was observed in the partial remission group (91.4%) compared to the complete remission one (51.3%). The four factors with predictive relapse value were: \"partial remission versus complete remission\", \"the intensity of clinical symptoms\", \"the age\" and \"the number of previous depressive episodes\". The existence of partial remission was the most powerful predictive factor. The decreasing sample size during the follow-up and the difficulty in warranting the treatment compliance.\n\nQuestion: Is the type of remission after a major depressive episode an important risk factor to relapses in a 4-year follow up?", "question_only": "Is the type of remission after a major depressive episode an important risk factor to relapses in a 4-year follow up?", "context": "Rates of relapse and predictive relapse factors were studied over more than 4 years in a sample of Spanish outpatients with DSM-III-R criteria for unipolar major depressive episode. A final sample of 139 outpatient was followed monthly in a naturalistic study. The Structured Clinical Interview for DSM-III-R was used. Phases of evolution were recorded using the Hamilton Depression Rating Scale, applying the Frank criteria. Survival analysis, Kaplan-Meier product limit and proportional hazards models were used. A higher rate of relapses was observed in the partial remission group (91.4%) compared to the complete remission one (51.3%). The four factors with predictive relapse value were: \"partial remission versus complete remission\", \"the intensity of clinical symptoms\", \"the age\" and \"the number of previous depressive episodes\". The existence of partial remission was the most powerful predictive factor. The decreasing sample size during the follow-up and the difficulty in warranting the treatment compliance.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "At medium term, relapse rates for a major depressive episode are high. Partial remission after a depressive episode seems to be an important predictive factor for relapses in a 4-year follow-up.", "meshes": ["Adult", "Aged", "Antidepressive Agents", "Depressive Disorder, Major", "Diagnostic and Statistical Manual of Mental Disorders", "Drug Therapy, Combination", "Female", "Follow-Up Studies", "Humans", "Male", "Middle Aged", "Personality Inventory", "Prospective Studies", "Recurrence", "Risk Factors", "Serotonin Uptake Inhibitors", "Spain", "Survival Analysis"], "year": "2004"}
{"id": "pubmedqa_15041506", "dataset": "pubmedqa", "question": "Context: Sources of reports about laparoscopic and percutaneous treatment of liver hydatid cysts are limited to just a few countries. To address the reason behind this, we carried out a survey of 30 surgeons in northern Jordan. A questionnaire was distributed to collect data regarding the surgical technique preferred by each surgeon. Further information was collected from those not adopting minimal-access techniques to determine their reasons for not doing so. Only 3 surgeons (10%) considered laparoscopy as the first line of treatment. Of the 27 surgeons who did not consider percutaneous or laparoscopic treatment, fear of anaphylaxis and/or dissemination was the main reason given by 21 surgeons (78%) for not using minimal access techniques.\n\nQuestion: Is fear of anaphylactic shock discouraging surgeons from more widely adopting percutaneous and laparoscopic techniques in the treatment of liver hydatid cyst?", "question_only": "Is fear of anaphylactic shock discouraging surgeons from more widely adopting percutaneous and laparoscopic techniques in the treatment of liver hydatid cyst?", "context": "Sources of reports about laparoscopic and percutaneous treatment of liver hydatid cysts are limited to just a few countries. To address the reason behind this, we carried out a survey of 30 surgeons in northern Jordan. A questionnaire was distributed to collect data regarding the surgical technique preferred by each surgeon. Further information was collected from those not adopting minimal-access techniques to determine their reasons for not doing so. Only 3 surgeons (10%) considered laparoscopy as the first line of treatment. Of the 27 surgeons who did not consider percutaneous or laparoscopic treatment, fear of anaphylaxis and/or dissemination was the main reason given by 21 surgeons (78%) for not using minimal access techniques.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The seemingly exaggerated traditional fear of anaphylaxis seems to discourage surgeons from more widely adopting minimal access techniques for the treatment of hydatid cyst.", "meshes": ["Adult", "Anaphylaxis", "Echinococcosis, Hepatic", "General Surgery", "Humans", "Laparoscopy", "Middle Aged", "Postoperative Complications", "Practice Patterns, Physicians'", "Surveys and Questionnaires"], "year": "2004"}
{"id": "pubmedqa_16266387", "dataset": "pubmedqa", "question": "Context: Lifestyle changes over the last 30 years are the most likely explanation for the increase in allergic disease over this period.AIM: This study tests the hypothesis that the consumption of fast food is related to the prevalence of asthma and allergy. As part of the International Study of Asthma and Allergies in Childhood (ISAAC) a cross-sectional prevalence study of 1321 children (mean age = 11.4 years, range: 10.1-12.5) was conducted in Hastings, New Zealand. Using standard questions we collected data on the prevalence of asthma and asthma symptoms, as well as food frequency data. Skin prick tests were performed to common environmental allergens and exercise-induced bronchial hyperresponsiveness (BHR) was assessed according to a standard protocol. Body mass index (BMI) was calculated as weight/height2 (kg/m2) and classified into overweight and obese according to a standard international definition. After adjusting for lifestyle factors, including other diet and BMI variables, compared with children who never ate hamburgers, we found an independent risk of hamburger consumption on having a history of wheeze [consumption less than once a week (OR = 1.44, 95% CI: 1.06-1.96) and 1+ times a week (OR = 1.65, 95% CI: 1.07-2.52)] and on current wheeze [consumption less than once a week (OR = 1.17, 95% CI: 0.80-1.70) and 1+ times a week (OR = 1.81, 95% CI: 1.10-2.98)]. Takeaway consumption 1+ times a week was marginally significantly related to BHR (OR = 2.41, 95% CI: 0.99-5.91). There was no effect on atopy.\n\nQuestion: Fast foods - are they a risk factor for asthma?", "question_only": "Fast foods - are they a risk factor for asthma?", "context": "Lifestyle changes over the last 30 years are the most likely explanation for the increase in allergic disease over this period.AIM: This study tests the hypothesis that the consumption of fast food is related to the prevalence of asthma and allergy. As part of the International Study of Asthma and Allergies in Childhood (ISAAC) a cross-sectional prevalence study of 1321 children (mean age = 11.4 years, range: 10.1-12.5) was conducted in Hastings, New Zealand. Using standard questions we collected data on the prevalence of asthma and asthma symptoms, as well as food frequency data. Skin prick tests were performed to common environmental allergens and exercise-induced bronchial hyperresponsiveness (BHR) was assessed according to a standard protocol. Body mass index (BMI) was calculated as weight/height2 (kg/m2) and classified into overweight and obese according to a standard international definition. After adjusting for lifestyle factors, including other diet and BMI variables, compared with children who never ate hamburgers, we found an independent risk of hamburger consumption on having a history of wheeze [consumption less than once a week (OR = 1.44, 95% CI: 1.06-1.96) and 1+ times a week (OR = 1.65, 95% CI: 1.07-2.52)] and on current wheeze [consumption less than once a week (OR = 1.17, 95% CI: 0.80-1.70) and 1+ times a week (OR = 1.81, 95% CI: 1.10-2.98)]. Takeaway consumption 1+ times a week was marginally significantly related to BHR (OR = 2.41, 95% CI: 0.99-5.91). There was no effect on atopy.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Frequent consumption of hamburgers showed a dose-dependent association with asthma symptoms, and frequent takeaway consumption showed a similar association with BHR.", "meshes": ["Adult", "Animals", "Asthma", "Beverages", "Bronchial Hyperreactivity", "Cattle", "Child", "Cross-Sectional Studies", "Diet", "Female", "Humans", "Male", "Meat Products", "Prevalence", "Respiratory Sounds", "Risk Factors", "Skin Tests"], "year": "2005"}
{"id": "pubmedqa_15222284", "dataset": "pubmedqa", "question": "Context: The aim of this prospective, randomized study was to compare the hemodynamic performance of the Medtronic Mosaic and Edwards Perimount bioprostheses in the aortic position, and to evaluate prosthesis-specific differences in valve sizing and valve-size labeling. Between August 2000 and September 2002, 139 patients underwent isolated aortic valve replacement (AVR) with the Mosaic (n = 67) or Perimount (n = 72) bioprosthesis. Intraoperatively, the internal aortic annulus diameter was measured by insertion of a gauge (Hegar dilator), while prosthesis size was determined by using the original sizers. Transthoracic echocardiography was performed to determine hemodynamic and dimensional data. As the aim of AVR is to achieve a maximal effective orifice area (EOA) within a given aortic annulus, the ratio of EOA to patient aortic annulus area was calculated, the latter being based on annulus diameter measured intraoperatively. Operative mortality was 2.2% (Mosaic 3.0%; Perimount 1.4%; p = NS). Upsizing (using a prosthesis larger in labeled valve size than the patient's measured internal aortic annulus diameter) was possible in 28.4% of Mosaic patients and 8.3% of Perimount patients. The postoperative mean systolic pressure gradient ranged from 10.5 to 22.2 mmHg in the Mosaic group, and from 9.4 to 12.6 mmHg in the Perimount group; it was significantly lower for 21 and 23 Perimount valves than for 21 and 23 Mosaic valves. The EOA ranged from 0.78 to 2.37 cm2 in Mosaic patients, and from 0.95 to 2.12 cm2 in Perimount patients. When indexing EOA by calculating the ratio of EOA to patient aortic annulus area to adjust for variables such as patient anatomy and valve dimensions, there was no significant difference between the two bioprostheses.\n\nQuestion: The effective orifice area/patient aortic annulus area ratio: a better way to compare different bioprostheses?", "question_only": "The effective orifice area/patient aortic annulus area ratio: a better way to compare different bioprostheses?", "context": "The aim of this prospective, randomized study was to compare the hemodynamic performance of the Medtronic Mosaic and Edwards Perimount bioprostheses in the aortic position, and to evaluate prosthesis-specific differences in valve sizing and valve-size labeling. Between August 2000 and September 2002, 139 patients underwent isolated aortic valve replacement (AVR) with the Mosaic (n = 67) or Perimount (n = 72) bioprosthesis. Intraoperatively, the internal aortic annulus diameter was measured by insertion of a gauge (Hegar dilator), while prosthesis size was determined by using the original sizers. Transthoracic echocardiography was performed to determine hemodynamic and dimensional data. As the aim of AVR is to achieve a maximal effective orifice area (EOA) within a given aortic annulus, the ratio of EOA to patient aortic annulus area was calculated, the latter being based on annulus diameter measured intraoperatively. Operative mortality was 2.2% (Mosaic 3.0%; Perimount 1.4%; p = NS). Upsizing (using a prosthesis larger in labeled valve size than the patient's measured internal aortic annulus diameter) was possible in 28.4% of Mosaic patients and 8.3% of Perimount patients. The postoperative mean systolic pressure gradient ranged from 10.5 to 22.2 mmHg in the Mosaic group, and from 9.4 to 12.6 mmHg in the Perimount group; it was significantly lower for 21 and 23 Perimount valves than for 21 and 23 Mosaic valves. The EOA ranged from 0.78 to 2.37 cm2 in Mosaic patients, and from 0.95 to 2.12 cm2 in Perimount patients. When indexing EOA by calculating the ratio of EOA to patient aortic annulus area to adjust for variables such as patient anatomy and valve dimensions, there was no significant difference between the two bioprostheses.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Comparisons of absolute EOA values grouped by the manufacturers' valve sizes are misleading because of specific differences in geometric dimensions. The EOA:patient aortic annulus area ratio provides a new hemodynamic index which may facilitate objective comparisons between different valve types.", "meshes": ["Aged", "Aged, 80 and over", "Aortic Valve", "Aortic Valve Insufficiency", "Aortic Valve Stenosis", "Bioprosthesis", "Blood Pressure", "Female", "Heart Valve Prosthesis", "Heart Valve Prosthesis Implantation", "Humans", "Male", "Middle Aged", "Product Labeling", "Prospective Studies", "Prosthesis Design", "Prosthesis Fitting"], "year": "2004"}
{"id": "pubmedqa_23379759", "dataset": "pubmedqa", "question": "Context: The aims of the study were to report the rates of recurrent and residual cholesteatoma following primary CAT surgery and to report the rate of conversion to a modified radical mastoidectomy. This was a retrospective review of a single surgeon series between 2006 and 2012. In total 132 second-look operations were undertaken, with a mean interval between primary surgery and second-look procedures of 6 months. The rate of cholesteatoma at second-look surgery was 19.7%, which was split into residual disease (10.6%) and recurrent disease (9.09%). New tympanic membrane defects with cholesteatoma were considered as recurrent disease. Residual disease was defined as cholesteatoma present behind an intact tympanic membrane. The majority of recurrent and residual disease was easily removed at second look (73.1%). Only four cases were converted to a modified radical mastoidectomy (3%) and three cases required a third-look procedure.\n\nQuestion: Can early second-look tympanoplasty reduce the rate of conversion to modified radical mastoidectomy?", "question_only": "Can early second-look tympanoplasty reduce the rate of conversion to modified radical mastoidectomy?", "context": "The aims of the study were to report the rates of recurrent and residual cholesteatoma following primary CAT surgery and to report the rate of conversion to a modified radical mastoidectomy. This was a retrospective review of a single surgeon series between 2006 and 2012. In total 132 second-look operations were undertaken, with a mean interval between primary surgery and second-look procedures of 6 months. The rate of cholesteatoma at second-look surgery was 19.7%, which was split into residual disease (10.6%) and recurrent disease (9.09%). New tympanic membrane defects with cholesteatoma were considered as recurrent disease. Residual disease was defined as cholesteatoma present behind an intact tympanic membrane. The majority of recurrent and residual disease was easily removed at second look (73.1%). Only four cases were converted to a modified radical mastoidectomy (3%) and three cases required a third-look procedure.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Combined approach tympanoplasty (CAT) allows for successful treatment of cholesteatoma with rates of recurrent and residual disease comparable to open mastoid surgery. Early timing of second-look procedures allows easier removal of any recurrent or residual disease, which reduces the conversion rate to open mastoidectomy.", "meshes": ["Adolescent", "Adult", "Child", "Child, Preschool", "Cholesteatoma, Middle Ear", "Humans", "Middle Aged", "Recurrence", "Retrospective Studies", "Second-Look Surgery", "Treatment Outcome", "Tympanoplasty", "Young Adult"], "year": "2013"}
{"id": "pubmedqa_11862129", "dataset": "pubmedqa", "question": "Context: To determine if clinical variables assessed in relation to Albuterol aerosol treatments accurately identify children with pathologic radiographs during their initial episode of bronchospasm. A prospective convenience sample of children with a first episode of wheezing. Data collected included demographics, baseline and post-treatment clinical score and physical examination, number of aerosols, requirement for supplemental oxygen, and disposition. Chest radiographs were obtained and interpreted, and patients were divided into 2 groups based on a pathologic versus nonpathologic radiograph interpretation. Chi2 testing was performed for categoric variables, and the student t test was performed for continuous variables. A discriminant analysis was used to develop a model. Pathologic radiographs were identified in 61 patients (9%). Between groups, a significant difference was noted for pretreatment oxygen saturation only. Clinical score, respiratory rate, and presence of rales both pretreatment and posttreatment were not significantly different between groups. The discriminant analysis correctly predicted 90% of nonpathologic radiographs but only 15% of pathologic radiographs.\n\nQuestion: Do clinical variables predict pathologic radiographs in the first episode of wheezing?", "question_only": "Do clinical variables predict pathologic radiographs in the first episode of wheezing?", "context": "To determine if clinical variables assessed in relation to Albuterol aerosol treatments accurately identify children with pathologic radiographs during their initial episode of bronchospasm. A prospective convenience sample of children with a first episode of wheezing. Data collected included demographics, baseline and post-treatment clinical score and physical examination, number of aerosols, requirement for supplemental oxygen, and disposition. Chest radiographs were obtained and interpreted, and patients were divided into 2 groups based on a pathologic versus nonpathologic radiograph interpretation. Chi2 testing was performed for categoric variables, and the student t test was performed for continuous variables. A discriminant analysis was used to develop a model. Pathologic radiographs were identified in 61 patients (9%). Between groups, a significant difference was noted for pretreatment oxygen saturation only. Clinical score, respiratory rate, and presence of rales both pretreatment and posttreatment were not significantly different between groups. The discriminant analysis correctly predicted 90% of nonpathologic radiographs but only 15% of pathologic radiographs.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "Clinical variables, either isolated or as components of a model, could not identify all children with pathologic radiographs.", "meshes": ["Adolescent", "Albuterol", "Bronchodilator Agents", "Child", "Child, Preschool", "Discriminant Analysis", "Female", "Humans", "Infant", "Male", "Multivariate Analysis", "Physical Examination", "Prospective Studies", "Radiography", "Respiratory Sounds", "Respiratory Tract Diseases", "Sensitivity and Specificity", "Statistics, Nonparametric"], "year": "2002"}
{"id": "pubmedqa_25986020", "dataset": "pubmedqa", "question": "Context: Adoption and implementation of evidence-based measures for catheter care leads to reductions in central line-associated bloodstream infection (CLABSI) rates in the NICU. The purpose of this study is to evaluate whether this rate reduction is sustainable for at least 1 year and to identify key determinants of this sustainability at the NICU of the Floating Hospital for Children at Tufts Medical Center. We reviewed the incidence of CLABSIs in the NICU temporally to the implementation of new practice policies and procedures, from July 2008 to December 2013. Adoption of standardized care practices, including bundles and checklists, was associated with a significant reduction of the CLABSI rate to zero for>370 consecutive days in our NICU in 2012. Overall, our CLABSI rates decreased from 4.1 per 1000 line days in 2009 (13 infections; 3163 line days) to 0.94 in 2013 (2 infections; 2115 line days), which represents a 77% reduction over a 5-year period. In the first quarter of 2013, there was a brief increase in CLABSI rate to 3.3 per 1000 line days; after a series of interventions, the CLABSI rate was maintained at zero for>600 days. Ongoing training, surveillance, and vigilance with catheter insertion and maintenance practices and improved documentation were identified as key drivers for success.\n\nQuestion: Is zero central line-associated bloodstream infection rate sustainable?", "question_only": "Is zero central line-associated bloodstream infection rate sustainable?", "context": "Adoption and implementation of evidence-based measures for catheter care leads to reductions in central line-associated bloodstream infection (CLABSI) rates in the NICU. The purpose of this study is to evaluate whether this rate reduction is sustainable for at least 1 year and to identify key determinants of this sustainability at the NICU of the Floating Hospital for Children at Tufts Medical Center. We reviewed the incidence of CLABSIs in the NICU temporally to the implementation of new practice policies and procedures, from July 2008 to December 2013. Adoption of standardized care practices, including bundles and checklists, was associated with a significant reduction of the CLABSI rate to zero for>370 consecutive days in our NICU in 2012. Overall, our CLABSI rates decreased from 4.1 per 1000 line days in 2009 (13 infections; 3163 line days) to 0.94 in 2013 (2 infections; 2115 line days), which represents a 77% reduction over a 5-year period. In the first quarter of 2013, there was a brief increase in CLABSI rate to 3.3 per 1000 line days; after a series of interventions, the CLABSI rate was maintained at zero for>600 days. Ongoing training, surveillance, and vigilance with catheter insertion and maintenance practices and improved documentation were identified as key drivers for success.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "High-quality training, strict compliance with evidence-based guidelines, and thorough documentation is associated with significant reductions in CLABSIs. Mindful organizing may lead to a better understanding of what goes into a unit's ability to handle peak demands and sustain extraordinary performance in the long-term.", "meshes": ["Bacteremia", "Catheter-Related Infections", "Catheterization, Central Venous", "Guideline Adherence", "Humans", "Infant, Newborn", "Time Factors"], "year": "2015"}
{"id": "pubmedqa_26460153", "dataset": "pubmedqa", "question": "Context: We retrospectively identified 84 consecutive patients aged ≥80 years, who underwent a cardiac reoperation at the department for Cardiothoracic Surgery in the Heart&Vessel Center Bad Bevensen between January 2007 and 2013. Demographic profiles as well as operative data were analyzed, and the patients were prospectively followed. Patient's functional status and quality of life were assessed with the Barthel Index, New York Heart Association class and the short form-12 questionnaire. The mean age of the study group (61 men, 23 women) was 81.9 ± 1.9 years. Most redo-procedures were carried out after primary coronary artery bypass grafting (65%), primary aortic valve replacement (21%) and primary mitral valve replacement (6%). The most frequent actual surgical procedures were combined coronary artery bypass grafting and aortic valve replacement (26%), isolated coronary artery bypass grafting (19%), and isolated aortic valve replacement (19%). The mean length of hospital stay was 17 ± 15 days. In-hospital mortality counted for 32.1%. During follow up (29 ± 20 months) a further 19.0% of the patients died. The Barthel Index of the survivors was 89 ± 17 and their mean New York Heart Association class was 2 ± 1. A total of 93% of the patients were living at home. Summary scores of physical and mental health of the short form-12 questionnaire equalled those of an age- and sex-matched normative population.\n\nQuestion: Cardiac reoperations in octogenarians: Do they really benefit?", "question_only": "Cardiac reoperations in octogenarians: Do they really benefit?", "context": "We retrospectively identified 84 consecutive patients aged ≥80 years, who underwent a cardiac reoperation at the department for Cardiothoracic Surgery in the Heart&Vessel Center Bad Bevensen between January 2007 and 2013. Demographic profiles as well as operative data were analyzed, and the patients were prospectively followed. Patient's functional status and quality of life were assessed with the Barthel Index, New York Heart Association class and the short form-12 questionnaire. The mean age of the study group (61 men, 23 women) was 81.9 ± 1.9 years. Most redo-procedures were carried out after primary coronary artery bypass grafting (65%), primary aortic valve replacement (21%) and primary mitral valve replacement (6%). The most frequent actual surgical procedures were combined coronary artery bypass grafting and aortic valve replacement (26%), isolated coronary artery bypass grafting (19%), and isolated aortic valve replacement (19%). The mean length of hospital stay was 17 ± 15 days. In-hospital mortality counted for 32.1%. During follow up (29 ± 20 months) a further 19.0% of the patients died. The Barthel Index of the survivors was 89 ± 17 and their mean New York Heart Association class was 2 ± 1. A total of 93% of the patients were living at home. Summary scores of physical and mental health of the short form-12 questionnaire equalled those of an age- and sex-matched normative population.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Despite high perioperative mortality, results document a sustainable recovery of the survivors offering the prospect of a highly independent and satisfying life. Therefore, advanced age alone should not be a contraindication for redo cardiac interventions. Geriatr Gerontol Int 2016; 16: 1138-1144.", "meshes": ["Age Factors", "Aged, 80 and over", "Cardiovascular Surgical Procedures", "Cause of Death", "Cohort Studies", "Female", "Frail Elderly", "Geriatric Assessment", "Hospital Mortality", "Humans", "Kaplan-Meier Estimate", "Male", "Prognosis", "Quality of Life", "Reoperation", "Retrospective Studies", "Risk Assessment", "Sex Factors", "Survival Analysis", "Treatment Outcome"], "year": "2016"}
{"id": "pubmedqa_24098953", "dataset": "pubmedqa", "question": "Context: All VLBW infants from January 2008 to December 2012 with positive blood culture beyond 72 hours of life were enrolled in a retrospective cohort study. Newborns born after June 2010 were treated with IgM-eIVIG, 250 mg/kg/day iv for three days in addition to standard antibiotic regimen and compared to an historical cohort born before June 2010, receiving antimicrobial regimen alone. Short-term mortality (i.e. death within 7 and 21 days from treatment) was the primary outcome. Secondary outcomes were: total mortality, intraventricular hemorrhage, necrotizing enterocolitis, periventricular leukomalacia, bronchopulmonary dysplasia at discharge. 79 neonates (40 cases) were enrolled. No difference in birth weight, gestational age or SNAP II score (disease severity score) were found. Significantly reduced short-term mortality was found in treated infants (22% vs 46%; p = 0.005) considering all microbial aetiologies and the subgroup affected by Candida spp. Secondary outcomes were not different between groups.\n\nQuestion: Are IgM-enriched immunoglobulins an effective adjuvant in septic VLBW infants?", "question_only": "Are IgM-enriched immunoglobulins an effective adjuvant in septic VLBW infants?", "context": "All VLBW infants from January 2008 to December 2012 with positive blood culture beyond 72 hours of life were enrolled in a retrospective cohort study. Newborns born after June 2010 were treated with IgM-eIVIG, 250 mg/kg/day iv for three days in addition to standard antibiotic regimen and compared to an historical cohort born before June 2010, receiving antimicrobial regimen alone. Short-term mortality (i.e. death within 7 and 21 days from treatment) was the primary outcome. Secondary outcomes were: total mortality, intraventricular hemorrhage, necrotizing enterocolitis, periventricular leukomalacia, bronchopulmonary dysplasia at discharge. 79 neonates (40 cases) were enrolled. No difference in birth weight, gestational age or SNAP II score (disease severity score) were found. Significantly reduced short-term mortality was found in treated infants (22% vs 46%; p = 0.005) considering all microbial aetiologies and the subgroup affected by Candida spp. Secondary outcomes were not different between groups.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "This hypothesis-generator study shows that IgM-eIVIG is an effective adjuvant therapy in VLBW infants with proven sepsis. Randomized controlled trials are warranted to confirm this pilot observation.", "meshes": ["Adjuvants, Immunologic", "Analysis of Variance", "Cohort Studies", "Confidence Intervals", "Dose-Response Relationship, Drug", "Drug Administration Schedule", "Drug Combinations", "Female", "Hospital Mortality", "Humans", "Immunoglobulin A", "Immunoglobulin M", "Immunoglobulins, Intravenous", "Infant, Newborn", "Infant, Very Low Birth Weight", "Infusions, Intravenous", "Intensive Care Units, Neonatal", "Italy", "Length of Stay", "Male", "Odds Ratio", "Retrospective Studies", "Risk Assessment", "Sepsis", "Severity of Illness Index", "Survival Rate", "Treatment Outcome"], "year": "2013"}
{"id": "pubmedqa_16816043", "dataset": "pubmedqa", "question": "Context: To determine under what conditions lay people and health professionals find it acceptable for a physician to breach confidentiality to protect the wife of a patient with a sexually transmitted disease (STD). In a study in France, breaching confidentiality in 48 scenarios were accepted by 144 lay people, 10 psychologists and 7 physicians. The scenarios were all possible combinations of five factors: severity of the disease (severe, lethal); time taken to discuss this with (little time, much time); intent to inform the spouse about the disease (none, one of these days, immediately); intent to adopt protective behaviours (no intent, intent); and decision to consult an expert in STDs (yes, no), 2 x 2 x 3 x 2 x 2. The importance and interactions of each factor were determined, at the group level, by performing analyses of variance and constructing graphs. The concept of breaching confidentiality to protect a wife from her husband's STD was favoured much more by lay people and psychologists than by physicians (mean ratings 11.76, 9.28 and 2.90, respectively, on a scale of 0-22). The patient's stated intentions to protect his wife and to inform her of the disease had the greatest impact on acceptability. A cluster analysis showed groups of lay participants who found breaching confidentiality \"always acceptable\" (n = 14), \"depending on the many circumstances\" (n = 87), requiring \"consultation with an expert\" (n = 30) and \"never acceptable (n = 13)\".\n\nQuestion: Do French lay people and health professionals find it acceptable to breach confidentiality to protect a patient's wife from a sexually transmitted disease?", "question_only": "Do French lay people and health professionals find it acceptable to breach confidentiality to protect a patient's wife from a sexually transmitted disease?", "context": "To determine under what conditions lay people and health professionals find it acceptable for a physician to breach confidentiality to protect the wife of a patient with a sexually transmitted disease (STD). In a study in France, breaching confidentiality in 48 scenarios were accepted by 144 lay people, 10 psychologists and 7 physicians. The scenarios were all possible combinations of five factors: severity of the disease (severe, lethal); time taken to discuss this with (little time, much time); intent to inform the spouse about the disease (none, one of these days, immediately); intent to adopt protective behaviours (no intent, intent); and decision to consult an expert in STDs (yes, no), 2 x 2 x 3 x 2 x 2. The importance and interactions of each factor were determined, at the group level, by performing analyses of variance and constructing graphs. The concept of breaching confidentiality to protect a wife from her husband's STD was favoured much more by lay people and psychologists than by physicians (mean ratings 11.76, 9.28 and 2.90, respectively, on a scale of 0-22). The patient's stated intentions to protect his wife and to inform her of the disease had the greatest impact on acceptability. A cluster analysis showed groups of lay participants who found breaching confidentiality \"always acceptable\" (n = 14), \"depending on the many circumstances\" (n = 87), requiring \"consultation with an expert\" (n = 30) and \"never acceptable (n = 13)\".", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "Most people in France are influenced by situational factors when deciding if a physician should breach confidentiality to protect the spouse of a patient infected with STD.", "meshes": ["Adolescent", "Adult", "Attitude of Health Personnel", "Attitude to Health", "Cluster Analysis", "Confidentiality", "Female", "France", "Humans", "Intention", "Interpersonal Relations", "Male", "Middle Aged", "Severity of Illness Index", "Sexual Behavior", "Sexually Transmitted Diseases", "Spouses", "Time Factors"], "year": "2006"}
{"id": "pubmedqa_8111516", "dataset": "pubmedqa", "question": "Context: To determine whether volunteer family physician reports of the frequency of influenza-like illness (ILI) usefully supplement information from other influenza surveillance systems conducted by the Centers for Disease Control and Prevention. Evaluation of physician reports from five influenza surveillance seasons (1987-88 through 1991-92). Family physician office practices in all regions of the United States. An average of 140 physicians during each of five influenza seasons. None. An office visit or hospitalization of a patient for ILI, defined as presence of fever (temperature>or = 37.8 degrees C) and cough, sore throat, or myalgia, along with the physician's clinical judgment of influenza. A subset of physicians collected specimens for confirmation of influenza virus by culture. Physicians attributed 81,408 (5%) of 1,672,542 office visits to ILI; 2754 (3%) patients with ILI were hospitalized. Persons 65 years of age and older accounted for 11% of visits for ILI and 43% of hospitalizations for ILI. In three of five seasons, physicians obtained influenza virus isolates from a greater proportion of specimens compared with those processed by World Health Organization laboratories (36% vs 12%). Influenza virus isolates from sentinel physicians peaked from 1 to 4 weeks earlier than those reported by World Health Organization laboratories. Physicians reported peak morbidity 1 to 4 weeks earlier than state and territorial health departments in four of five seasons and 2 to 5 weeks earlier than peak mortality reported by 121 cities during seasons with excess mortality associated with pneumonia and influenza.\n\nQuestion: Do family physicians make good sentinels for influenza?", "question_only": "Do family physicians make good sentinels for influenza?", "context": "To determine whether volunteer family physician reports of the frequency of influenza-like illness (ILI) usefully supplement information from other influenza surveillance systems conducted by the Centers for Disease Control and Prevention. Evaluation of physician reports from five influenza surveillance seasons (1987-88 through 1991-92). Family physician office practices in all regions of the United States. An average of 140 physicians during each of five influenza seasons. None. An office visit or hospitalization of a patient for ILI, defined as presence of fever (temperature>or = 37.8 degrees C) and cough, sore throat, or myalgia, along with the physician's clinical judgment of influenza. A subset of physicians collected specimens for confirmation of influenza virus by culture. Physicians attributed 81,408 (5%) of 1,672,542 office visits to ILI; 2754 (3%) patients with ILI were hospitalized. Persons 65 years of age and older accounted for 11% of visits for ILI and 43% of hospitalizations for ILI. In three of five seasons, physicians obtained influenza virus isolates from a greater proportion of specimens compared with those processed by World Health Organization laboratories (36% vs 12%). Influenza virus isolates from sentinel physicians peaked from 1 to 4 weeks earlier than those reported by World Health Organization laboratories. Physicians reported peak morbidity 1 to 4 weeks earlier than state and territorial health departments in four of five seasons and 2 to 5 weeks earlier than peak mortality reported by 121 cities during seasons with excess mortality associated with pneumonia and influenza.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Family physicians provide sensitive, timely, and accurate community influenza morbidity data that complement data from other surveillance systems. This information enables monitoring of the type, timing, and intensity of influenza activity and can help health care workers implement prevention or control measures.", "meshes": ["Adolescent", "Adult", "Aged", "Centers for Disease Control and Prevention (U.S.)", "Child", "Child, Preschool", "Family Practice", "Humans", "Infant", "Influenza, Human", "Middle Aged", "Population Surveillance", "United States"], "year": "1993"}
{"id": "pubmedqa_25604390", "dataset": "pubmedqa", "question": "Context: Dickkopf-3 (DKK3) may act as a tumor suppressor as it is down-regulated in various types of cancer. This study assessed the DKK3 protein expression in gastric cancer and its potential value as a prognostic marker. DKK3 expression was evaluated by immunohistochemistry in 158 gastric cancer samples from patients who underwent gastrectomy from 2002 to 2008. Clinicopathological parameters and survival data were analyzed. Loss of DKK3 expression was found in 64 of 158 (40.5%) samples, and it was associated with advanced T stage (p<0.001), lymph node metastasis (p<0.001), UICC TNM stage (p<0.001), tumor location (p = 0.029), lymphovascular invasion (p = 0.035), and perineural invasion (p = 0.032). Patients without DKK3 expression in tumor cells had a significantly worse disease-free and overall survival than those with DKK3 expression (p<0.001, and p = 0.001, respectively). TNM stage (p = 0.028 and p<0.001, respectively) and residual tumor (p<0.001 and p = 0.003, respectively) were independent predictors of disease-free and overall survival. Based on the preoperative clinical stage assessed by computed tomography (CT), loss of DKK3 expression was predominantly associated with worse prognosis in patients with clinically node-negative advanced gastric cancer (AGC). The combination of DKK3 expression status and CT increased the accuracy of CT staging for predicting lymph node involvement from 71.5 to 80.0% in AGC patients.\n\nQuestion: Aberrant loss of dickkopf-3 in gastric cancer: can it predict lymph node metastasis preoperatively?", "question_only": "Aberrant loss of dickkopf-3 in gastric cancer: can it predict lymph node metastasis preoperatively?", "context": "Dickkopf-3 (DKK3) may act as a tumor suppressor as it is down-regulated in various types of cancer. This study assessed the DKK3 protein expression in gastric cancer and its potential value as a prognostic marker. DKK3 expression was evaluated by immunohistochemistry in 158 gastric cancer samples from patients who underwent gastrectomy from 2002 to 2008. Clinicopathological parameters and survival data were analyzed. Loss of DKK3 expression was found in 64 of 158 (40.5%) samples, and it was associated with advanced T stage (p<0.001), lymph node metastasis (p<0.001), UICC TNM stage (p<0.001), tumor location (p = 0.029), lymphovascular invasion (p = 0.035), and perineural invasion (p = 0.032). Patients without DKK3 expression in tumor cells had a significantly worse disease-free and overall survival than those with DKK3 expression (p<0.001, and p = 0.001, respectively). TNM stage (p = 0.028 and p<0.001, respectively) and residual tumor (p<0.001 and p = 0.003, respectively) were independent predictors of disease-free and overall survival. Based on the preoperative clinical stage assessed by computed tomography (CT), loss of DKK3 expression was predominantly associated with worse prognosis in patients with clinically node-negative advanced gastric cancer (AGC). The combination of DKK3 expression status and CT increased the accuracy of CT staging for predicting lymph node involvement from 71.5 to 80.0% in AGC patients.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "Loss of DKK3 protein expression was significantly associated with poor survival in patients with gastric cancer and was strongly correlated with the TNM stage. DKK3 might be a potential biomarker of lymph node involvement that can improve the predictive power of CT.", "meshes": ["Adenocarcinoma", "Aged", "Disease-Free Survival", "Female", "Gastrectomy", "Humans", "Immunohistochemistry", "Intercellular Signaling Peptides and Proteins", "Lymphatic Metastasis", "Male", "Middle Aged", "Neoplasm Invasiveness", "Neoplasm Staging", "Neoplasm, Residual", "Preoperative Period", "Retrospective Studies", "Stomach Neoplasms", "Survival Rate", "Tomography, X-Ray Computed"], "year": "2015"}
{"id": "pubmedqa_22266735", "dataset": "pubmedqa", "question": "Context: The International Association of the Diabetes and Pregnancy Study Groups (IADPSG) recently recommended new criteria for diagnosing gestational diabetes mellitus (GDM). This study was undertaken to determine whether adopting the IADPSG criteria would be cost-effective, compared with the current standard of care. We developed a decision analysis model comparing the cost-utility of three strategies to identify GDM: 1) no screening, 2) current screening practice (1-h 50-g glucose challenge test between 24 and 28 weeks followed by 3-h 100-g glucose tolerance test when indicated), or 3) screening practice proposed by the IADPSG. Assumptions included that 1) women diagnosed with GDM received additional prenatal monitoring, mitigating the risks of preeclampsia, shoulder dystocia, and birth injury; and 2) GDM women had opportunity for intensive postdelivery counseling and behavior modification to reduce future diabetes risks. The primary outcome measure was the incremental cost-effectiveness ratio (ICER). Our model demonstrates that the IADPSG recommendations are cost-effective only when postdelivery care reduces diabetes incidence. For every 100,000 women screened, 6,178 quality-adjusted life-years (QALYs) are gained, at a cost of $125,633,826. The ICER for the IADPSG strategy compared with the current standard was $20,336 per QALY gained. When postdelivery care was not accomplished, the IADPSG strategy was no longer cost-effective. These results were robust in sensitivity analyses.\n\nQuestion: Screening for gestational diabetes mellitus: are the criteria proposed by the international association of the Diabetes and Pregnancy Study Groups cost-effective?", "question_only": "Screening for gestational diabetes mellitus: are the criteria proposed by the international association of the Diabetes and Pregnancy Study Groups cost-effective?", "context": "The International Association of the Diabetes and Pregnancy Study Groups (IADPSG) recently recommended new criteria for diagnosing gestational diabetes mellitus (GDM). This study was undertaken to determine whether adopting the IADPSG criteria would be cost-effective, compared with the current standard of care. We developed a decision analysis model comparing the cost-utility of three strategies to identify GDM: 1) no screening, 2) current screening practice (1-h 50-g glucose challenge test between 24 and 28 weeks followed by 3-h 100-g glucose tolerance test when indicated), or 3) screening practice proposed by the IADPSG. Assumptions included that 1) women diagnosed with GDM received additional prenatal monitoring, mitigating the risks of preeclampsia, shoulder dystocia, and birth injury; and 2) GDM women had opportunity for intensive postdelivery counseling and behavior modification to reduce future diabetes risks. The primary outcome measure was the incremental cost-effectiveness ratio (ICER). Our model demonstrates that the IADPSG recommendations are cost-effective only when postdelivery care reduces diabetes incidence. For every 100,000 women screened, 6,178 quality-adjusted life-years (QALYs) are gained, at a cost of $125,633,826. The ICER for the IADPSG strategy compared with the current standard was $20,336 per QALY gained. When postdelivery care was not accomplished, the IADPSG strategy was no longer cost-effective. These results were robust in sensitivity analyses.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The IADPSG recommendation for glucose screening in pregnancy is cost-effective. The model is most sensitive to the likelihood of preventing future diabetes in patients identified with GDM using postdelivery counseling and intervention.", "meshes": ["Cost-Benefit Analysis", "Diabetes, Gestational", "Female", "Glucose Tolerance Test", "Humans", "Mass Screening", "Pregnancy", "Quality-Adjusted Life Years"], "year": "2012"}
{"id": "pubmedqa_21550158", "dataset": "pubmedqa", "question": "Context: This investigation assesses the effect of platelet-rich plasma (PRP) gel on postoperative pain, swelling, and trismus as well as healing and bone regeneration potential on mandibular third molar extraction sockets. A prospective randomized comparative clinical study was undertaken over a 2-year period. Patients requiring surgical extraction of a single impacted third molar and who fell within the inclusion criteria and indicated willingness to return for recall visits were recruited. The predictor variable was application of PRP gel to the socket of the third molar in the test group, whereas the control group had no PRP. The outcome variables were pain, swelling, and maximum mouth opening, which were measured using a 10-point visual analog scale, tape, and millimeter caliper, respectively. Socket healing was assessed radiographically by allocating scores for lamina dura, overall density, and trabecular pattern. Quantitative data were presented as mean. Mann-Whitney test was used to compare means between groups for continuous variables, whereas Fischer exact test was used for categorical variables. Statistical significance was inferred at P<.05. Sixty patients aged 19 to 35 years (mean: 24.7 ± 3.6 years) were divided into both test and control groups of 30 patients each. The mean postoperative pain score (visual analog scale) was lower for the PRP group at all time points and this was statistically significant (P<.05). Although the figures for swelling and interincisal mouth opening were lower in the test group, this difference was not statistically significant. Similarly, the scores for lamina dura, trabecular pattern, and bone density were better among patients in the PRP group. This difference was also not statistically significant.\n\nQuestion: Can autologous platelet-rich plasma gel enhance healing after surgical extraction of mandibular third molars?", "question_only": "Can autologous platelet-rich plasma gel enhance healing after surgical extraction of mandibular third molars?", "context": "This investigation assesses the effect of platelet-rich plasma (PRP) gel on postoperative pain, swelling, and trismus as well as healing and bone regeneration potential on mandibular third molar extraction sockets. A prospective randomized comparative clinical study was undertaken over a 2-year period. Patients requiring surgical extraction of a single impacted third molar and who fell within the inclusion criteria and indicated willingness to return for recall visits were recruited. The predictor variable was application of PRP gel to the socket of the third molar in the test group, whereas the control group had no PRP. The outcome variables were pain, swelling, and maximum mouth opening, which were measured using a 10-point visual analog scale, tape, and millimeter caliper, respectively. Socket healing was assessed radiographically by allocating scores for lamina dura, overall density, and trabecular pattern. Quantitative data were presented as mean. Mann-Whitney test was used to compare means between groups for continuous variables, whereas Fischer exact test was used for categorical variables. Statistical significance was inferred at P<.05. Sixty patients aged 19 to 35 years (mean: 24.7 ± 3.6 years) were divided into both test and control groups of 30 patients each. The mean postoperative pain score (visual analog scale) was lower for the PRP group at all time points and this was statistically significant (P<.05). Although the figures for swelling and interincisal mouth opening were lower in the test group, this difference was not statistically significant. Similarly, the scores for lamina dura, trabecular pattern, and bone density were better among patients in the PRP group. This difference was also not statistically significant.", "options": ["yes", "no", "maybe"], "gold_label": "yes", "long_answer": "The PRP group recorded reduced pain, swelling, and trismus as well as enhanced and faster bone healing compared with those in the control. Hence the study showed that topical application of PRP gel has a beneficial effect in enhancing socket healing after third molar surgery.", "meshes": ["Adult", "Bone Regeneration", "Chi-Square Distribution", "Female", "Gels", "Humans", "Male", "Mandible", "Molar, Third", "Pain Measurement", "Pain, Postoperative", "Platelet-Rich Plasma", "Prospective Studies", "Radiography", "Range of Motion, Articular", "Single-Blind Method", "Statistics, Nonparametric", "Tooth Extraction", "Tooth Socket", "Tooth, Impacted", "Wound Healing", "Young Adult"], "year": "2011"}
{"id": "pubmedqa_26859535", "dataset": "pubmedqa", "question": "Context: Updated guidelines for the screening and management of cervical cancer in the United States recommend starting Papanicolaou (Pap) testing at age 21 and screening less frequently with less aggressive management for abnormalities. We sought to examine updated Pap test screening guidelines and how they may affect the detection of invasive cervical cancer, especially among women<30 years of age. Patients diagnosed at Brigham and Women's Hospital with invasive cervical cancer between 2002 and 2012 were retrospectively identified. Prior screening history was obtained and patients were divided into two groups based on age<30 years or age ≥30 years. The two groups were then compared with respect to demographics, pathological findings, and time to diagnosis. A total of 288 patients with invasive cervical carcinoma were identified. Among these patients, 109 had adequate information on prior screening history. Invasive adenocarcinoma (IAC) was diagnosed in 37 (33.94%) patients, whereas 64 (58.72%) patients were diagnosed with invasive squamous cell carcinoma (ISCC). The remaining eight patients were diagnosed with other types of cancers of the cervix. A total of 13 patients were younger than 30 while 96 patients were 30 or older. The mean time from normal Pap to diagnosis of IAC was 15 months in patients younger than 30 years of age compared to 56 months in patients aged 30 and older (p < 0.001). The mean time from normal Pap to diagnosis of ISCC was 38 months in patients younger than 30 years of age and 82 months in patients aged 30 and older (p = 0.018).\n\nQuestion: Screening History Among Women with Invasive Cervical Cancer in an Academic Medical Center: Will We Miss Cancers Following Updated Guidelines?", "question_only": "Screening History Among Women with Invasive Cervical Cancer in an Academic Medical Center: Will We Miss Cancers Following Updated Guidelines?", "context": "Updated guidelines for the screening and management of cervical cancer in the United States recommend starting Papanicolaou (Pap) testing at age 21 and screening less frequently with less aggressive management for abnormalities. We sought to examine updated Pap test screening guidelines and how they may affect the detection of invasive cervical cancer, especially among women<30 years of age. Patients diagnosed at Brigham and Women's Hospital with invasive cervical cancer between 2002 and 2012 were retrospectively identified. Prior screening history was obtained and patients were divided into two groups based on age<30 years or age ≥30 years. The two groups were then compared with respect to demographics, pathological findings, and time to diagnosis. A total of 288 patients with invasive cervical carcinoma were identified. Among these patients, 109 had adequate information on prior screening history. Invasive adenocarcinoma (IAC) was diagnosed in 37 (33.94%) patients, whereas 64 (58.72%) patients were diagnosed with invasive squamous cell carcinoma (ISCC). The remaining eight patients were diagnosed with other types of cancers of the cervix. A total of 13 patients were younger than 30 while 96 patients were 30 or older. The mean time from normal Pap to diagnosis of IAC was 15 months in patients younger than 30 years of age compared to 56 months in patients aged 30 and older (p < 0.001). The mean time from normal Pap to diagnosis of ISCC was 38 months in patients younger than 30 years of age and 82 months in patients aged 30 and older (p = 0.018).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "In this small retrospective study, updated Pap test screening guidelines would not have missed invasive cancer on average among screened women age 30 and older. However, young patients aged 21-29 years may be at increased risk of developing IAC of the cervix between the recommended screening intervals.", "meshes": ["Academic Medical Centers", "Adenocarcinoma", "Adult", "Aged", "Female", "Humans", "Mass Screening", "Massachusetts", "Middle Aged", "Papanicolaou Test", "Practice Guidelines as Topic", "Retrospective Studies", "Risk Factors", "Uterine Cervical Neoplasms", "Vaginal Smears"], "year": "2016"}
{"id": "pubmedqa_24799031", "dataset": "pubmedqa", "question": "Context: The objective was to evaluate the efficacy of diffusion-weighted imaging (DWI) in predicting the development of vascularization in hypovascular hepatocellular lesions (HHLs). Forty-two HHLs that were diagnosed by computed tomographic (CT) arteriography were evaluated retrospectively. The lesion on DWI was classified as isointense, hypointense, or hyperintense. Follow-up studies that included intravenous dynamic CT or magnetic resonance imaging were performed. The 730-day cumulative developments of vascularization in hypointense, isointense, and hyperintense lesions were 17%, 30%, and 40%, respectively. The differences among these developments were not statistically significant.\n\nQuestion: Is diffusion-weighted imaging a significant indicator of the development of vascularization in hypovascular hepatocellular lesions?", "question_only": "Is diffusion-weighted imaging a significant indicator of the development of vascularization in hypovascular hepatocellular lesions?", "context": "The objective was to evaluate the efficacy of diffusion-weighted imaging (DWI) in predicting the development of vascularization in hypovascular hepatocellular lesions (HHLs). Forty-two HHLs that were diagnosed by computed tomographic (CT) arteriography were evaluated retrospectively. The lesion on DWI was classified as isointense, hypointense, or hyperintense. Follow-up studies that included intravenous dynamic CT or magnetic resonance imaging were performed. The 730-day cumulative developments of vascularization in hypointense, isointense, and hyperintense lesions were 17%, 30%, and 40%, respectively. The differences among these developments were not statistically significant.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The signal intensity on DWI showed no significant difference in the development of vascularization.", "meshes": ["Aged", "Aged, 80 and over", "Carcinoma, Hepatocellular", "Contrast Media", "Diffusion Magnetic Resonance Imaging", "Female", "Gadolinium DTPA", "Hepatitis B, Chronic", "Hepatitis C, Chronic", "Humans", "Liver Diseases, Alcoholic", "Liver Neoplasms", "Male", "Middle Aged", "Neovascularization, Pathologic", "Non-alcoholic Fatty Liver Disease", "Reproducibility of Results", "Retrospective Studies", "Tomography, X-Ray Computed"], "year": null}
{"id": "pubmedqa_12040336", "dataset": "pubmedqa", "question": "Context: The role of early revascularization among patients with acute myocardial infarction complicated by cardiogenic shock remains controversial. Angioplasty registries, while suggesting a benefit, are subject to selection bias, and clinical trials have been underpowered to detect early benefits. If an invasive strategy is beneficial in this population, patients admitted to hospitals with onsite coronary revascularization might be expected to have a better prognosis. We sought to determine whether access to cardiovascular resources at the admitting hospital influenced the prognosis of patients with acute myocardial infarction complicated by cardiogenic shock. By use of the Cooperative Cardiovascular Project database (a retrospective medical record review of Medicare patients discharged with acute myocardial infarction), we identified patients aged>or =65 years whose myocardial infarction was complicated by cardiogenic shock. Of the 601 patients with cardiogenic shock, 287 (47.8%) were admitted to hospitals without revascularization services and 314 (52.2%) were admitted to hospitals with coronary angioplasty and coronary artery bypass surgery facilities. Clinical characteristics were similar across the subgroups. Patients admitted to hospitals with revascularization services were more likely to undergo coronary revascularization during the index hospitalization and during the first month after acute myocardial infarction. After adjustment for demographic, clinical, hospital, and treatment strategies, the presence of onsite revascularization services was not associated with a significantly lower 30-day (odds ratio 0.83, 95% CI 0.47, 1.45) or 1-year mortality (odds ratio 0.91, 95% CI 0.49, 1.72).\n\nQuestion: Cardiogenic shock complicating acute myocardial infarction in elderly patients: does admission to a tertiary center improve survival?", "question_only": "Cardiogenic shock complicating acute myocardial infarction in elderly patients: does admission to a tertiary center improve survival?", "context": "The role of early revascularization among patients with acute myocardial infarction complicated by cardiogenic shock remains controversial. Angioplasty registries, while suggesting a benefit, are subject to selection bias, and clinical trials have been underpowered to detect early benefits. If an invasive strategy is beneficial in this population, patients admitted to hospitals with onsite coronary revascularization might be expected to have a better prognosis. We sought to determine whether access to cardiovascular resources at the admitting hospital influenced the prognosis of patients with acute myocardial infarction complicated by cardiogenic shock. By use of the Cooperative Cardiovascular Project database (a retrospective medical record review of Medicare patients discharged with acute myocardial infarction), we identified patients aged>or =65 years whose myocardial infarction was complicated by cardiogenic shock. Of the 601 patients with cardiogenic shock, 287 (47.8%) were admitted to hospitals without revascularization services and 314 (52.2%) were admitted to hospitals with coronary angioplasty and coronary artery bypass surgery facilities. Clinical characteristics were similar across the subgroups. Patients admitted to hospitals with revascularization services were more likely to undergo coronary revascularization during the index hospitalization and during the first month after acute myocardial infarction. After adjustment for demographic, clinical, hospital, and treatment strategies, the presence of onsite revascularization services was not associated with a significantly lower 30-day (odds ratio 0.83, 95% CI 0.47, 1.45) or 1-year mortality (odds ratio 0.91, 95% CI 0.49, 1.72).", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "In a community-based cohort, patients with acute myocardial infarction complicated by cardiogenic shock did not have significantly different adjusted 30-day and 1-year mortality, irrespective of the revascularization capabilities of the admitting hospital.", "meshes": ["Aged", "Angioplasty, Balloon, Coronary", "Cohort Studies", "Coronary Artery Bypass", "Coronary Care Units", "Databases, Factual", "Female", "Hospitalization", "Humans", "Male", "Myocardial Infarction", "Myocardial Revascularization", "Odds Ratio", "Prognosis", "Shock, Cardiogenic"], "year": "2002"}
{"id": "pubmedqa_24946973", "dataset": "pubmedqa", "question": "Context: To evaluate accelerated partial breast irradiation (APBI) in patients after oncoplastic surgery for early breast cancer. A retrospective analysis of 136 breasts of 134 patients, who received breast-conserving oncoplastic surgery for low-risk breast cancer between 2002 and 2010 in the Universities of Vienna and Luebeck followed by adjuvant APBI applying total doses of pulse dose rate of 50.4 Gy or high-dose rate (HDR) of 32 Gy over 4 days. Target volume definition was performed by the use of surgical-free margin data, related to intraoperatively fixed clip positions, pre- and postoperative imaging, and palpation. At the time of data acquisition, 131 of 134 patients were alive. The median follow-up time was 39 months (range, 4-106 months). After high-dose rate treatment, 3 of 89 patients showed systemic progress after a mean follow-up of 47 months (range, 19-75 months) and 2 patients had a different quadrant in-breast tumor after 27 and 35 months. One patient died 7 months after treatment of unknown causes. After pulse dose rate treatment, 1 of 45 patients had a local recurrence after 42 months and 1 patient died because of another cause after 13 months. We observed mild fibrosis in 27 breasts, telangiectasia in 6, hyperpigmentation in 14 cases, and keloid formation in 1.\n\nQuestion: Is oncoplastic surgery a contraindication for accelerated partial breast radiation using the interstitial multicatheter brachytherapy method?", "question_only": "Is oncoplastic surgery a contraindication for accelerated partial breast radiation using the interstitial multicatheter brachytherapy method?", "context": "To evaluate accelerated partial breast irradiation (APBI) in patients after oncoplastic surgery for early breast cancer. A retrospective analysis of 136 breasts of 134 patients, who received breast-conserving oncoplastic surgery for low-risk breast cancer between 2002 and 2010 in the Universities of Vienna and Luebeck followed by adjuvant APBI applying total doses of pulse dose rate of 50.4 Gy or high-dose rate (HDR) of 32 Gy over 4 days. Target volume definition was performed by the use of surgical-free margin data, related to intraoperatively fixed clip positions, pre- and postoperative imaging, and palpation. At the time of data acquisition, 131 of 134 patients were alive. The median follow-up time was 39 months (range, 4-106 months). After high-dose rate treatment, 3 of 89 patients showed systemic progress after a mean follow-up of 47 months (range, 19-75 months) and 2 patients had a different quadrant in-breast tumor after 27 and 35 months. One patient died 7 months after treatment of unknown causes. After pulse dose rate treatment, 1 of 45 patients had a local recurrence after 42 months and 1 patient died because of another cause after 13 months. We observed mild fibrosis in 27 breasts, telangiectasia in 6, hyperpigmentation in 14 cases, and keloid formation in 1.", "options": ["yes", "no", "maybe"], "gold_label": "maybe", "long_answer": "These preliminary results suggest the feasibility of multicatheter APBI after oncoplastic breast-conserving surgery in selected low-risk breast cancer patients; however, special attention to target volume definition is needed. Further prospective investigations with long follow-up are needed to define the real value of the procedure.", "meshes": ["Adult", "Aged", "Aged, 80 and over", "Brachytherapy", "Breast", "Breast Neoplasms", "Carcinoma, Ductal, Breast", "Carcinoma, Intraductal, Noninfiltrating", "Carcinoma, Lobular", "Catheters", "Feasibility Studies", "Female", "Follow-Up Studies", "Humans", "Kaplan-Meier Estimate", "Mastectomy, Segmental", "Middle Aged", "Neoplasm Recurrence, Local", "Radiotherapy Dosage", "Radiotherapy Planning, Computer-Assisted", "Radiotherapy, Adjuvant", "Retrospective Studies", "Treatment Outcome"], "year": null}
{"id": "pubmedqa_24074624", "dataset": "pubmedqa", "question": "Context: Some pediatric patients, typically those that are very young or felt to be especially sick are temporarily admitted to the intensive care unit (ICU) for observation during their first transfusion. If a significant reaction that requires ICU management does not occur, these patients are then transferred to a regular ward where future blood products are administered. The aim of this project was to determine if heightened observation such as temporary ICU admissions for the first transfusion are warranted. From the blood bank records of a tertiary care pediatric hospital, a list of patients on whom a transfusion reaction was reported between 2007 and 2012, the type of reaction and the patient's transfusion history, were extracted. The hospital location where the transfusion occurred, and whether the patient was evaluated by the ICU team or transferred to the ICU for management of the reaction was determined from the patient's electronic medical record. There were 174 acute reactions in 150 patients. Of these 150 patients, 13 (8.7%) different patients experienced a reaction during their first transfusion; all 13 patients experienced clinically mild reactions (8 febrile non-hemolytic, 4 mild allergic, and 1 patient who simultaneously had a mild allergic and a febrile non-hemolytic), and none required ICU management. Six severe reactions (6 of 174, 3.4%) involving significant hypotension and/or hypoxia that required acute and intensive management occurred during subsequent (i.e. not the first) transfusion in six patients.\n\nQuestion: Is intensive monitoring during the first transfusion in pediatric patients necessary?", "question_only": "Is intensive monitoring during the first transfusion in pediatric patients necessary?", "context": "Some pediatric patients, typically those that are very young or felt to be especially sick are temporarily admitted to the intensive care unit (ICU) for observation during their first transfusion. If a significant reaction that requires ICU management does not occur, these patients are then transferred to a regular ward where future blood products are administered. The aim of this project was to determine if heightened observation such as temporary ICU admissions for the first transfusion are warranted. From the blood bank records of a tertiary care pediatric hospital, a list of patients on whom a transfusion reaction was reported between 2007 and 2012, the type of reaction and the patient's transfusion history, were extracted. The hospital location where the transfusion occurred, and whether the patient was evaluated by the ICU team or transferred to the ICU for management of the reaction was determined from the patient's electronic medical record. There were 174 acute reactions in 150 patients. Of these 150 patients, 13 (8.7%) different patients experienced a reaction during their first transfusion; all 13 patients experienced clinically mild reactions (8 febrile non-hemolytic, 4 mild allergic, and 1 patient who simultaneously had a mild allergic and a febrile non-hemolytic), and none required ICU management. Six severe reactions (6 of 174, 3.4%) involving significant hypotension and/or hypoxia that required acute and intensive management occurred during subsequent (i.e. not the first) transfusion in six patients.", "options": ["yes", "no", "maybe"], "gold_label": "no", "long_answer": "The practice of intensive observation for the first transfusion in pediatric patients is probably unnecessary.", "meshes": ["Adolescent", "Adult", "Blood Component Transfusion", "Blood Transfusion", "Child", "Child, Preschool", "Critical Care", "Humans", "Infant", "Infant, Newborn", "Intensive Care Units", "Population Surveillance", "Transfusion Reaction", "Young Adult"], "year": "2014"}
